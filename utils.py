"""
tiny_lora_utils.py - Shared Utilities for TinyLoRA Training, Validation and Testing
TinyLoRA å…±äº«å·¥å…·æ¨¡å— - ç”¨äºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•

This module contains:
- TinyLoRAGlobalParams and TinyLoRALinear classes
- apply_tiny_lora function for injecting TinyLoRA into a model
- compile_and_run function for C++ code evaluation
- Model and tokenizer loading utilities
"""

import os
import re
import sys
import subprocess
import tempfile
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import prepare_model_for_kbit_training


# ========== TinyLoRA Classes ==========

class TinyLoRAGlobalParams(nn.Module):
    """ä¸“é—¨ç”¨äºæ³¨å†Œå…¨å±€å…±äº«å‘é‡çš„å®¹å™¨ / Container for global shared vector"""
    def __init__(self, u_dim=16, device='cpu', dtype=torch.bfloat16):
        super().__init__()
        self.global_v = nn.Parameter(torch.randn(u_dim, device=device, dtype=dtype))
    
    def forward(self):
        """ä¸å®é™…è°ƒç”¨ï¼Œä»…ç”¨äºæ³¨å†Œå‚æ•° / Not actually called, only for parameter registration"""
        return self.global_v


class TinyLoRALinear(nn.Module):
    def __init__(self, original_layer, rank=2, u=None, global_params_ref=None):
        """
        TinyLoRA Linear Layer with global parameter sharing / æ”¯æŒå…¨å±€å‚æ•°å…±äº«çš„ TinyLoRA çº¿æ€§å±‚
        
        Args:
            original_layer: Original Linear layer to be replaced / è¢«æ›¿æ¢çš„åŸå§‹çº¿æ€§å±‚
            rank: Rank for TinyLoRA (default=2) / TinyLoRA çš„ç§©ï¼ˆé»˜è®¤ä¸º 2ï¼‰
            u: Dimension of global shared vector / å…¨å±€å…±äº«å‘é‡çš„ç»´åº¦
            global_params_ref: Reference to TinyLoRAGlobalParams container / TinyLoRAGlobalParams å®¹å™¨çš„å¼•ç”¨
        """
        super().__init__()
        self.in_features = original_layer.in_features
        self.out_features = original_layer.out_features
        self.rank = rank
        
        if global_params_ref is None:
            raise ValueError("global_params_ref cannot be None / global_params_ref ä¸èƒ½ä¸º None")
        self.global_params_ref = global_params_ref
        u_dim = global_params_ref.global_v.shape[0]
        
        # Get weight from original layer / ä»åŸå§‹å±‚è·å–æƒé‡
        W = original_layer.weight.data
        device = W.device
        
        # Handle 4-bit quantized weights / å¤„ç† 4-bit é‡åŒ–æƒé‡
        if hasattr(original_layer, 'quant_state'):
            # Dequantize to FP32 on CPU for SVD / åé‡åŒ–åˆ° CPU FP32 è¿›è¡Œ SVD
            from bitsandbytes.functional import dequantize_4bit
            qs = original_layer.quant_state
            W_dequant = dequantize_4bit(
                W, quant_state=qs, quant_type="nf4"
            ).to(torch.float32).cpu()
        else:
            W_dequant = W.to(torch.float32).cpu()
        
        # Perform SVD (deterministic operation) / æ‰§è¡Œ SVDï¼ˆç¡®å®šæ€§è¿ç®—ï¼‰
        try:
            U, S, Vh = torch.linalg.svd(W_dequant, full_matrices=False)
        except Exception as e:
            print(f"âš ï¸ SVD failed, using zeros / SVD å¤±è´¥ï¼Œä½¿ç”¨é›¶çŸ©é˜µ: {e}")
            U = torch.zeros(self.out_features, min(self.out_features, self.in_features))
            S = torch.zeros(min(self.out_features, self.in_features))
            Vh = torch.zeros(min(self.out_features, self.in_features), self.in_features)
        
        # Keep only top 'rank' components / ä»…ä¿ç•™å‰ 'rank' ä¸ªæˆåˆ†
        U_r = U[:, :self.rank]
        S_r = S[:self.rank]
        Vh_r = Vh[:self.rank, :]
        
        # Register as buffers (frozen, not trainable) / æ³¨å†Œä¸º bufferï¼ˆå†»ç»“ï¼Œä¸å¯è®­ç»ƒï¼‰
        self.register_buffer('U', U_r.to(torch.bfloat16).to(device))
        self.register_buffer('S', S_r.to(torch.bfloat16).to(device))
        self.register_buffer('Vh', Vh_r.to(torch.bfloat16).to(device))
        
        # Generate fixed random projection matrix P / ç”Ÿæˆå›ºå®šéšæœºæŠ•å½±çŸ©é˜µ P
        # NOTE: This uses the global random seed set before apply_tiny_lora
        # æ³¨æ„ï¼šè¿™ä½¿ç”¨åœ¨ apply_tiny_lora ä¹‹å‰è®¾ç½®çš„å…¨å±€éšæœºç§å­
        P = torch.randn(u_dim, self.rank, device=device, dtype=torch.bfloat16)
        self.register_buffer('P', P)
        
        # Store original weight and bias / å­˜å‚¨åŸå§‹æƒé‡å’Œåç½®
        if hasattr(original_layer, 'quant_state'):
            # Keep quantized weight / ä¿ç•™é‡åŒ–æƒé‡
            self.register_buffer('W_base', W)
            self.quant_state = original_layer.quant_state
        else:
            self.register_buffer('W_base', W.to(torch.bfloat16))
        
        if original_layer.bias is not None:
            self.register_buffer('bias', original_layer.bias.data.to(torch.bfloat16))
        else:
            self.bias = None

    def forward(self, x):
        """Forward pass with TinyLoRA delta / TinyLoRA å¢é‡çš„å‰å‘ä¼ æ’­"""
        # Base weight computation / åŸºç¡€æƒé‡è®¡ç®—
        if hasattr(self, 'quant_state'):
            # Use quantized computation / ä½¿ç”¨é‡åŒ–è®¡ç®—
            from bitsandbytes.functional import dequantize_4bit
            W_base = dequantize_4bit(self.W_base, quant_state=self.quant_state, quant_type="nf4")
            out = torch.nn.functional.linear(x, W_base, None)
        else:
            out = torch.nn.functional.linear(x, self.W_base, None)
        
        # TinyLoRA delta: U @ S @ (v^T @ P) @ Vh
        # global_v shape: (u,), P shape: (u, rank)
        v = self.global_params_ref.global_v  # Access shared parameter / è®¿é—®å…±äº«å‚æ•°
        vP = v @ self.P  # (rank,)
        S_vP = self.S * vP  # (rank,)
        delta = self.U @ torch.diag(S_vP) @ self.Vh  # (out_features, in_features)
        
        # Add delta to output / å°†å¢é‡æ·»åŠ åˆ°è¾“å‡º
        out = out + torch.nn.functional.linear(x, delta, None)
        
        # Add bias if exists / å¦‚æœå­˜åœ¨åç½®åˆ™æ·»åŠ 
        if self.bias is not None:
            out = out + self.bias
        
        return out


def apply_tiny_lora(model, global_params_ref):
    """
    éå†æ¨¡å‹ï¼Œå°†æ‰€æœ‰ç›®æ ‡ Linear å±‚æ›¿æ¢ä¸º TinyLoRALinearï¼Œ
    å¹¶ä¼ å…¥å¯¹ global_params å®¹å™¨çš„å¼•ç”¨ï¼Œå®ç°è®ºæ–‡ä¸­çš„ Tiling (å…¨å‚æ•°å…±äº«)ã€‚
    
    Traverse the model and replace all target Linear layers with TinyLoRALinear,
    passing reference to global_params container to achieve Tiling (full parameter sharing) from the paper.
    """
    # Qwen/Llama target module names / Qwen/Llama ç›®æ ‡æ¨¡å—åç§°
    target_suffixes = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    
    replaced_count = 0
    
    # Recursive function to traverse submodules / é€’å½’å‡½æ•°éå†å­æ¨¡å—
    for name, child in model.named_children():
        # Check if this is a target layer / æ£€æŸ¥æ˜¯å¦ä¸ºç›®æ ‡å±‚
        if isinstance(child, nn.Linear) and any(name.endswith(suffix) for suffix in target_suffixes):
            # Replace with TinyLoRALinear / æ›¿æ¢ä¸º TinyLoRALinear
            tiny_lora_layer = TinyLoRALinear(child, rank=2, global_params_ref=global_params_ref)
            setattr(model, name, tiny_lora_layer)
            replaced_count += 1
            print(f"  âœ“ Replaced / å·²æ›¿æ¢: {name}")
        else:
            # Recursively process child modules / é€’å½’å¤„ç†å­æ¨¡å—
            replaced_count += apply_tiny_lora(child, global_params_ref)
            
    return replaced_count


# ========== Code Compilation and Execution ==========

def compile_and_run(code, test_cases, timeout=2):
    """
    Compile and run C++ code against multiple test cases, return reward
    ç¼–è¯‘å¹¶è¿è¡Œ C++ ä»£ç ï¼Œå¯¹å¤šä¸ªæµ‹è¯•ç”¨ä¾‹è¿›è¡Œè¯„æµ‹ï¼Œè¿”å›å¥–åŠ±
    
    Args:
        code: C++ source code / C++ æºä»£ç 
        test_cases: list of dicts, each containing 'input' and 'output' / æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
        timeout: execution timeout in seconds / æ‰§è¡Œè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        float: 0.0 (compile fail) / 0.5 (partial pass) / 1.0 (all pass)
               0.0ï¼ˆç¼–è¯‘å¤±è´¥ï¼‰ / 0.5ï¼ˆéƒ¨åˆ†é€šè¿‡ï¼‰ / 1.0ï¼ˆå…¨éƒ¨é€šè¿‡ï¼‰
    """
    # Remove freopen statements / ç§»é™¤ freopen è¯­å¥
    code = re.sub(r'freopen\s*\(.*?\);', '', code, flags=re.IGNORECASE)
    
    # Create temp directory / åˆ›å»ºä¸´æ—¶ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        src_path = os.path.join(temp_dir, "solution.cpp")
        exe_path = os.path.join(temp_dir, "solution.out")
        
        # Write source file / å†™å…¥æºæ–‡ä»¶
        with open(src_path, "w", encoding="utf-8") as f:
            f.write(code)
        
        # Compile / ç¼–è¯‘
        compile_cmd = ["g++", "-O2", "-std=c++17", src_path, "-o", exe_path]
        try:
            result = subprocess.run(
                compile_cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                timeout=10,
                text=True
            )
            if result.returncode != 0:
                return 0.0  # Compilation failed / ç¼–è¯‘å¤±è´¥
        except subprocess.TimeoutExpired:
            return 0.0
        except Exception:
            return 0.0
        
        # Run test cases / è¿è¡Œæµ‹è¯•ç”¨ä¾‹
        passed = 0
        total = len(test_cases)
        
        if total == 0:
            return 0.5  # No test cases, give partial credit / æ— æµ‹è¯•ç”¨ä¾‹ï¼Œç»™éƒ¨åˆ†åˆ†æ•°
        
        for tc in test_cases:
            input_data = tc.get('input', '')
            expected_output = tc.get('output', '').strip()
            
            try:
                result = subprocess.run(
                    [exe_path],
                    input=input_data,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    timeout=timeout,
                    text=True
                )
                
                actual_output = result.stdout.strip()
                
                if actual_output == expected_output:
                    passed += 1
            except subprocess.TimeoutExpired:
                continue
            except Exception:
                continue
        
        # Calculate reward / è®¡ç®—å¥–åŠ±
        if passed == 0:
            return 0.5  # Compiled but no tests passed / ç¼–è¯‘æˆåŠŸä½†æœªé€šè¿‡æµ‹è¯•
        elif passed == total:
            return 1.0  # All tests passed / å…¨éƒ¨é€šè¿‡
        else:
            return 0.5  # Partial pass / éƒ¨åˆ†é€šè¿‡


def convert_hf_tests_to_list(hf_tests):
    """
    Convert HuggingFace test format to list of test cases
    å°† HuggingFace æµ‹è¯•æ ¼å¼è½¬æ¢ä¸ºæµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
    
    Args:
        hf_tests: dict with 'input' and 'output' as lists / åŒ…å« 'input' å’Œ 'output' åˆ—è¡¨çš„å­—å…¸
    
    Returns:
        list of dicts, each with 'input' and 'output' / æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
    """
    if not isinstance(hf_tests, dict):
        return []
    
    inputs = hf_tests.get('input', [])
    outputs = hf_tests.get('output', [])
    
    if not isinstance(inputs, list) or not isinstance(outputs, list):
        return []
    
    return [
        {'input': inp, 'output': out}
        for inp, out in zip(inputs, outputs)
    ]


# ========== Model Loading Utilities ==========

def get_model_and_tokenizer(model_path, use_4bit=True):
    """
    Load model and tokenizer with 4-bit quantization
    åŠ è½½ 4-bit é‡åŒ–çš„æ¨¡å‹å’Œåˆ†è¯å™¨
    
    Args:
        model_path: Path to the model / æ¨¡å‹è·¯å¾„
        use_4bit: Whether to use 4-bit quantization / æ˜¯å¦ä½¿ç”¨ 4-bit é‡åŒ–
    
    Returns:
        tuple: (model, tokenizer)
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“¦ Loading model from / ä»ä»¥ä¸‹è·¯å¾„åŠ è½½æ¨¡å‹: {model_path}")
    print(f"{'='*60}\n")
    
    # Load tokenizer / åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
    )
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"
    
    # Configure quantization / é…ç½®é‡åŒ–
    if use_4bit:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
        )
        
        model.config.use_cache = False
        model = prepare_model_for_kbit_training(model)
    else:
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
        )
    
    print(f"âœ… Model and tokenizer loaded successfully / æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸ\n")
    
    return model, tokenizer


def extract_code_from_response(response):
    """
    Extract C++ code from model response
    ä»æ¨¡å‹å“åº”ä¸­æå– C++ ä»£ç 
    
    Args:
        response: Model generated text / æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
    
    Returns:
        str: Extracted C++ code or empty string / æå–çš„ C++ ä»£ç æˆ–ç©ºå­—ç¬¦ä¸²
    """
    # Try to extract code from markdown code block / å°è¯•ä» markdown ä»£ç å—æå–
    patterns = [
        r'```cpp\s*(.*?)\s*```',
        r'```c\+\+\s*(.*?)\s*```',
        r'```\s*(.*?)\s*```',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)
        if match:
            return match.group(1).strip()
    
    # If no code block found, try to find code with #include / å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»£ç å—ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å« #include çš„ä»£ç 
    if '#include' in response:
        # Extract from first #include to the end / ä»ç¬¬ä¸€ä¸ª #include æå–åˆ°æœ«å°¾
        start = response.find('#include')
        return response[start:].strip()
    
    return ""


def apply_chat_template(example, tokenizer):
    """
    Build prompt from problem description and public test cases.
    For deepmind/code_contests dataset structure.
    
    ä»é—®é¢˜æè¿°å’Œå…¬å¼€æµ‹è¯•ç”¨ä¾‹æ„å»ºæç¤ºã€‚
    é€‚ç”¨äº deepmind/code_contests æ•°æ®é›†ç»“æ„ã€‚
    
    Args:
        example: Dataset example / æ•°æ®é›†æ ·æœ¬
        tokenizer: Tokenizer for applying chat template / ç”¨äºåº”ç”¨èŠå¤©æ¨¡æ¿çš„åˆ†è¯å™¨
    
    Returns:
        dict: Example with 'prompt' field added / æ·»åŠ äº† 'prompt' å­—æ®µçš„æ ·æœ¬
    """
    # Extract problem description / æå–é—®é¢˜æè¿°
    description = example.get('description', '')
    
    # Build public test cases section / æ„å»ºå…¬å¼€æµ‹è¯•ç”¨ä¾‹éƒ¨åˆ†
    public_tests_section = ""
    public_tests = example.get('public_tests', {})
    
    if isinstance(public_tests, dict) and 'input' in public_tests and 'output' in public_tests:
        inputs = public_tests['input'] if isinstance(public_tests['input'], list) else [public_tests['input']]
        outputs = public_tests['output'] if isinstance(public_tests['output'], list) else [public_tests['output']]
        
        if inputs and outputs:
            public_tests_section = "\nã€Casesã€‘\n"
            for i, (inp, out) in enumerate(zip(inputs, outputs), 1):
                public_tests_section += f"Example {i}:\nInput:\n{inp}\nOutput:\n{out}\n"
    
    # Combine into final prompt / ç»„åˆæˆæœ€ç»ˆæç¤º
    final_prompt = f"""You will be given a programming contest problem. Please reason step by step and provide a complete C++ implementation.
Output the solution in a code block. Do not include debugging info or extra output. Limit reasoning to 128 tokens.


ã€Problem Descriptionã€‘
{description}

{public_tests_section}

Please provide your C++ solution:"""
    
    # Build Qwen chat template format / æ„å»º Qwen èŠå¤©æ¨¡æ¿æ ¼å¼
    messages = [
        {"role": "system", "content": "You are an expert competitive programmer. Output valid C++ code that compiles and solves the problem correctly."},
        {"role": "user", "content": final_prompt}
    ]
    
    # Apply chat template using tokenizer / ä½¿ç”¨åˆ†è¯å™¨åº”ç”¨èŠå¤©æ¨¡æ¿
    example['prompt'] = tokenizer.apply_chat_template(
        messages, 
        tokenize=False,
        add_generation_prompt=True
    )
    
    return example
