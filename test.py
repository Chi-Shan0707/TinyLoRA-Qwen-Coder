"""
test.py - Standalone Testing Script for TinyLoRA Model
æµ‹è¯•è„šæœ¬ - ç”¨äº TinyLoRA æ¨¡å‹çš„ç‹¬ç«‹æµ‹è¯•

Usage / ç”¨æ³•:
    python test.py [checkpoint_path] [num_samples]
    python test.py --baseline [num_samples]
    
Example / ç¤ºä¾‹:
    python test.py ./output/luoguqwencoder-lora/tiny_lora_v.pt 50
    python test.py ./output/luoguqwencoder-lora/best_tiny_lora_v.pt 100
    python test.py --baseline 50          # Test base model for comparison
"""

import os
import sys
import torch
from datasets import load_dataset
from utils import (
    get_model_and_tokenizer,
    TinyLoRAGlobalParams,
    apply_tiny_lora,
    compile_and_run,
    convert_hf_tests_to_list,
    extract_code_from_response,
    apply_chat_template,
)
from tqdm import tqdm


def test_model(checkpoint_path, num_samples=50, test_data_path="./local_code_contests/code_contests_test.jsonl", baseline=False):
    """
    Test a trained TinyLoRA model or baseline model on the test dataset
    åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šæµ‹è¯•è®­ç»ƒå¥½çš„ TinyLoRA æ¨¡å‹æˆ–åŸºåº§æ¨¡å‹
    
    Args:
        checkpoint_path: Path to the .pt checkpoint file / .pt æ£€æŸ¥ç‚¹æ–‡ä»¶çš„è·¯å¾„
        num_samples: Number of samples to test / è¦æµ‹è¯•çš„æ ·æœ¬æ•°
        test_data_path: Path to test dataset / æµ‹è¯•æ•°æ®é›†è·¯å¾„
        baseline: If True, test base model without TinyLoRA / å¦‚æœä¸º Trueï¼Œæµ‹è¯•åŸºåº§æ¨¡å‹ï¼ˆä¸å« TinyLoRAï¼‰
    
    Returns:
        dict: Test metrics / æµ‹è¯•æŒ‡æ ‡
    """
    print(f"\n{'='*80}")
    mode_str = "Baseline Model" if baseline else "TinyLoRA Model"
    print(f"ğŸ§ª {mode_str} Testing / {mode_str} æµ‹è¯•")
    print(f"{'='*80}\n")
    
    # ========== Step 1: Load checkpoint or set seed / åŠ è½½æ£€æŸ¥ç‚¹æˆ–è®¾ç½®ç§å­ ==========
    if not baseline:
        print(f"ğŸ“¦ Loading checkpoint / æ­£åœ¨åŠ è½½æ£€æŸ¥ç‚¹...")
        print(f"   Path / è·¯å¾„: {checkpoint_path}\n")
        
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"Checkpoint not found / æ£€æŸ¥ç‚¹æœªæ‰¾åˆ°: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # Extract metadata / æå–å…ƒæ•°æ®
        u_value = checkpoint['u_value']
        rank = checkpoint['rank']
        seed = checkpoint['seed']
        model_id = checkpoint.get('model_id', 'qwen/Qwen2.5-Coder-3B-Instruct')
        global_v = checkpoint['global_v']
        
        print(f"âœ… Checkpoint loaded / æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
        print(f"   â€¢ u_value / u å€¼: {u_value}")
        print(f"   â€¢ rank / ç§©: {rank}")
        print(f"   â€¢ seed / éšæœºç§å­: {seed}")
        print(f"   â€¢ model_id / æ¨¡å‹ ID: {model_id}")
        print(f"   â€¢ global_v shape / global_v å½¢çŠ¶: {global_v.shape}\n")
        
        # ========== Step 2: Set random seed / è®¾ç½®éšæœºç§å­ ==========
        print(f"ğŸ² Setting random seed / æ­£åœ¨è®¾ç½®éšæœºç§å­: {seed}")
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        print(f"   âœ… Random seed set / éšæœºç§å­å·²è®¾ç½®\n")
    else:
        print(f"ğŸ² Setting random seed / æ­£åœ¨è®¾ç½®éšæœºç§å­: 42")
        torch.manual_seed(42)
        torch.cuda.manual_seed(42)
        print(f"   âœ… Random seed set / éšæœºç§å­å·²è®¾ç½®\n")
    
    # ========== Step 3: Load base model / åŠ è½½åŸºåº§æ¨¡å‹ ==========
    # Check if model exists locally / æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨äºæœ¬åœ°
    local_model_path = "./models/Qwen2.5-Coder-3B-Instruct"
    if os.path.exists(local_model_path):
        model_path = local_model_path
    else:
        model_id = 'qwen/Qwen2.5-Coder-3B-Instruct' if baseline else model_id
        model_path = model_id
    
    model, tokenizer = get_model_and_tokenizer(model_path, use_4bit=True, for_inference=True)
    
    # ========== Step 4: Conditionally inject TinyLoRA / æ¡ä»¶æ€§æ³¨å…¥ TinyLoRA ==========
    if not baseline:
        print(f"ğŸ”§ Injecting TinyLoRA layers / æ­£åœ¨æ³¨å…¥ TinyLoRA å±‚...")
        
        # Get device / è·å–è®¾å¤‡
        device = model.model.layers[0].self_attn.q_proj.weight.device
        
        # Create global params container / åˆ›å»ºå…¨å±€å‚æ•°å®¹å™¨
        global_params = TinyLoRAGlobalParams(u_dim=u_value, device=device, dtype=torch.bfloat16)
        
        # Register to model / æ³¨å†Œåˆ°æ¨¡å‹
        model.tiny_lora_params = global_params
        
        # ã€å…³é”®ã€‘é‡æ–°è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ P çŸ©é˜µä¸è®­ç»ƒæ—¶ä¸€è‡´
        # CRITICAL: Re-seed right before apply_tiny_lora to match training P matrices
        # In train_rl.py, seed is set RIGHT BEFORE apply_tiny_lora, not before model loading
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        
        # Apply TinyLoRA (this will use the fixed random seed) / åº”ç”¨ TinyLoRAï¼ˆä¼šä½¿ç”¨å›ºå®šçš„éšæœºç§å­ï¼‰
        total_replaced = apply_tiny_lora(model, global_params)
        print(f"   âœ… TinyLoRA injected / TinyLoRA å·²æ³¨å…¥: {total_replaced} layers replaced / å±‚å·²æ›¿æ¢\n")
        
        # ========== Step 5: Load trained weights / åŠ è½½è®­ç»ƒæƒé‡ ==========
        print(f"ğŸ’¾ Loading trained weights / æ­£åœ¨åŠ è½½è®­ç»ƒæƒé‡...")
        with torch.no_grad():
            global_params.global_v.copy_(global_v.to(global_params.global_v.dtype).to(device))
        print(f"   âœ… Trained weights loaded / è®­ç»ƒæƒé‡å·²åŠ è½½\n")
    else:
        print(f"â­ï¸  Skipping TinyLoRA injection (baseline mode) / è·³è¿‡ TinyLoRA æ³¨å…¥ï¼ˆåŸºåº§æ¨¡å‹æ¨¡å¼ï¼‰\n")
    
    # Verify trainable parameters / éªŒè¯å¯è®­ç»ƒå‚æ•°
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    all_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“Š Model Statistics / æ¨¡å‹ç»Ÿè®¡:")
    print(f"   â€¢ Total parameters / æ€»å‚æ•°: {all_params:,}")
    if not baseline:
        print(f"   â€¢ Trainable parameters / å¯è®­ç»ƒå‚æ•°: {trainable_params}")
        print(f"   â€¢ Compression ratio / å‹ç¼©æ¯”: {all_params / trainable_params:.1f}x\n")
    
    # ========== Step 6: Load test dataset / åŠ è½½æµ‹è¯•æ•°æ®é›† ==========
    print(f"ğŸ“ Loading test dataset / æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®é›†...")
    print(f"   Path / è·¯å¾„: {test_data_path}\n")
    
    if not os.path.exists(test_data_path):
        raise FileNotFoundError(f"Test dataset not found / æµ‹è¯•æ•°æ®é›†æœªæ‰¾åˆ°: {test_data_path}")
    
    test_dataset = load_dataset("json", data_files=test_data_path, split="train")
    
    # Limit samples / é™åˆ¶æ ·æœ¬æ•°
    num_samples = min(num_samples, len(test_dataset))
    print(f"   âœ… Dataset loaded / æ•°æ®é›†å·²åŠ è½½: {len(test_dataset)} total samples / æ€»æ ·æœ¬")
    print(f"   ğŸ“Š Testing on / æµ‹è¯•æ ·æœ¬æ•°: {num_samples} samples / æ ·æœ¬\n")
    
    # Apply chat template if needed / å¦‚æœéœ€è¦ï¼Œåº”ç”¨èŠå¤©æ¨¡æ¿
    if 'prompt' not in test_dataset.column_names:
        print(f"ğŸ”„ Applying chat template... / æ­£åœ¨åº”ç”¨èŠå¤©æ¨¡æ¿...")
        test_dataset = test_dataset.map(lambda x: apply_chat_template(x, tokenizer))
        print(f"   âœ… Chat template applied / èŠå¤©æ¨¡æ¿å·²åº”ç”¨\n")
    
    # ========== Step 7: Run evaluation / è¿è¡Œè¯„ä¼° ==========
    print(f"{'='*80}")
    print(f"ğŸš€ Starting evaluation / å¼€å§‹è¯„ä¼°...")
    print(f"{'='*80}\n")
    
    model.eval()
    
    total_score = 0.0
    compile_success = 0
    partial_pass = 0
    full_pass = 0
    no_code_extracted = 0
    
    with torch.no_grad():
        for i in tqdm(range(num_samples), desc="Testing / æµ‹è¯•ä¸­"):
            sample = test_dataset[i]
            
            # Get prompt / è·å–æç¤º
            prompt = sample.get('prompt', '')
            if not prompt:
                continue
            
            # Prepare test cases / å‡†å¤‡æµ‹è¯•ç”¨ä¾‹
            public_tests = sample.get('public_tests', {})
            private_tests = sample.get('private_tests', {})
            generated_tests = sample.get('generated_tests', {})
            
            all_test_cases = []
            all_test_cases.extend(convert_hf_tests_to_list(public_tests))
            all_test_cases.extend(convert_hf_tests_to_list(private_tests))
            all_test_cases.extend(convert_hf_tests_to_list(generated_tests))
            
            if not all_test_cases:
                continue
            
            # Tokenize and generate / åˆ†è¯å¹¶ç”Ÿæˆ
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
            
            try:
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=1024,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                )
                
                # Decode response / è§£ç å“åº”
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # Remove prompt from response / ä»å“åº”ä¸­ç§»é™¤æç¤º
                if prompt in response:
                    response = response[len(prompt):].strip()
                
                # Extract code / æå–ä»£ç 
                code = extract_code_from_response(response)
                
                if not code:
                    no_code_extracted += 1
                    continue
                
                # Compile and run / ç¼–è¯‘å¹¶è¿è¡Œ
                reward = compile_and_run(code, all_test_cases)
                
                total_score += reward
                
                # Track statistics / è·Ÿè¸ªç»Ÿè®¡ä¿¡æ¯
                if reward >= 0.5:
                    compile_success += 1
                if reward == 0.5:
                    partial_pass += 1
                elif reward == 1.0:
                    full_pass += 1
                    
            except Exception as e:
                print(f"\nâš ï¸ Error during generation / ç”Ÿæˆæ—¶å‡ºé”™: {e}")
                continue
    
    # ========== Step 8: Calculate and report metrics / è®¡ç®—å¹¶æŠ¥å‘ŠæŒ‡æ ‡ ==========
    print(f"\n{'='*80}")
    print(f"âœ… Evaluation complete / è¯„ä¼°å®Œæˆ")
    print(f"{'='*80}\n")
    
    avg_score = total_score / num_samples if num_samples > 0 else 0.0
    compile_rate = compile_success / num_samples if num_samples > 0 else 0.0
    pass_at_1 = full_pass / num_samples if num_samples > 0 else 0.0
    
    print(f"ğŸ“Š Test Results / æµ‹è¯•ç»“æœ:")
    print(f"   â€¢ Total Samples / æ€»æ ·æœ¬æ•°: {num_samples}")
    print(f"   â€¢ Average Score / å¹³å‡åˆ†æ•°: {avg_score:.4f}")
    print(f"   â€¢ Compile Rate / ç¼–è¯‘æˆåŠŸç‡: {compile_rate:.2%} ({compile_success}/{num_samples})")
    print(f"   â€¢ Pass@1 / å®Œå…¨é€šè¿‡ç‡: {pass_at_1:.2%} ({full_pass}/{num_samples})")
    print(f"   â€¢ Partial Pass / éƒ¨åˆ†é€šè¿‡: {partial_pass}/{num_samples}")
    print(f"   â€¢ No Code Extracted / æœªæå–åˆ°ä»£ç : {no_code_extracted}/{num_samples}")
    print(f"{'='*80}\n")
    
    results = {
        'total_samples': num_samples,
        'avg_score': avg_score,
        'compile_rate': compile_rate,
        'pass_at_1': pass_at_1,
        'compile_success': compile_success,
        'partial_pass': partial_pass,
        'full_pass': full_pass,
        'no_code_extracted': no_code_extracted,
    }
    
    return results


if __name__ == "__main__":
    """
    Main entry point for standalone testing
    ç‹¬ç«‹æµ‹è¯•çš„ä¸»å…¥å£
    """
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Test a trained TinyLoRA model / æµ‹è¯•è®­ç»ƒå¥½çš„ TinyLoRA æ¨¡å‹"
    )
    parser.add_argument(
        "--checkpoint_path",
        nargs='?',
        default="./output/luoguqwencoder-lora/tiny_lora_v.pt",
        help="Path to checkpoint .pt file / æ£€æŸ¥ç‚¹ .pt æ–‡ä»¶çš„è·¯å¾„"
    )
    parser.add_argument(
        "--num_samples",
        nargs='?',
        type=int,
        default=50,
        help="Number of samples to test / è¦æµ‹è¯•çš„æ ·æœ¬æ•°"
    )
    parser.add_argument(
        "--test_data",
        type=str,
        default="./local_code_contests/code_contests_test.jsonl",
        help="Path to test dataset / æµ‹è¯•æ•°æ®é›†è·¯å¾„"
    )
    parser.add_argument(
        "--baseline",
        action="store_true",
        help="Test base model without TinyLoRA (for comparison) / æµ‹è¯•åŸºåº§æ¨¡å‹ï¼ˆä¸å« TinyLoRAï¼Œç”¨äºå¯¹æ¯”ï¼‰"
    )
    
    args = parser.parse_args()
    
    mode_str = "Baseline Model" if args.baseline else "TinyLoRA Model"
    print(f"\nğŸ¯ {mode_str} Testing / {mode_str} æµ‹è¯•")
    if not args.baseline:
        print(f"   Checkpoint / æ£€æŸ¥ç‚¹: {args.checkpoint_path}")
    print(f"   Samples / æ ·æœ¬æ•°: {args.num_samples}")
    print(f"   Test Data / æµ‹è¯•æ•°æ®: {args.test_data}\n")
    
    # Run testing / è¿è¡Œæµ‹è¯•
    results = test_model(
        checkpoint_path=args.checkpoint_path,
        num_samples=args.num_samples,
        test_data_path=args.test_data,
        baseline=args.baseline
    )
    
    print(f"âœ… Testing complete! / æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ¯ Final Pass@1 / æœ€ç»ˆé€šè¿‡ç‡: {results['pass_at_1']:.2%}\n")
