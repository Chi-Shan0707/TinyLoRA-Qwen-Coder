import torch
import torch.nn as nn
import os
import sys
from datasets import load_dataset, load_from_disk
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
)
from peft import prepare_model_for_kbit_training
from trl import GRPOTrainer, GRPOConfig
from modelscope.hub.snapshot_download import snapshot_download
import bitsandbytes as bnb

print("âœ… All libraries imported successfully! / æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸï¼\nğŸ“ Usage example: python train_rl.py 16 1000\n(First arg: TinyLoRA u value, Second arg: max training samples)")

# ========== argument parsing ==========
# ========== å‘½ä»¤è¡Œå‚æ•°ï¼šu å€¼ å’Œ æœ€å¤§æ ·æœ¬æ•° ==========
U_VALUE = int(sys.argv[1]) if len(sys.argv) > 1 else 16
MAX_SAMPLES = int(sys.argv[2]) if len(sys.argv) > 2 else 2000

print(f"TinyLoRA u value / uå€¼: {U_VALUE}")
if MAX_SAMPLES is not None:
    print(f"Max training samples / æœ€å¤§è®­ç»ƒæ ·æœ¬æ•°: {MAX_SAMPLES}")
else:
    print(f"Max training samples / æœ€å¤§è®­ç»ƒæ ·æœ¬æ•°: unlimited")

# ========== Dataset Configuration ==========
# ========== æ•°æ®é›†é…ç½® ==========
# Filter configuration for deepmind/code_contests dataset
# ç”¨äº deepmind/code_contests æ•°æ®é›†çš„è¿‡æ»¤é…ç½®
# source: The original source of the problem, with possible values including UNKNOWN_SOURCE (0),CODECHEF (1), CODEFORCES (2), HACKEREARTH (3), CODEJAM (4), ATCODER (5) and AIZU (6).
# difficulty: A representation of the difficulty of the problem with possible values including UNKNOWN_DIFFICULTY (0), EASY (1), MEDIUM (2), HARD (3), HARDER (4), HARDEST (5), EXTERNAL (6), A (7), B (8), C (9), D (10), E (11), F (12), G (13), H (14), I (15), J (16), K (17), L (18), M (19), N (20), O (21), P (22), Q (23), R (24), S (25), T (26), U (27) and V (28). Note that different sources use different, non-comparable gradings. For Codeforces problems, cf_rating is a more reliable measure of difficulty when available.
# Structure / ç»“æ„:
#   Key: source ID (integer) / é”®ï¼šæ•°æ®æº IDï¼ˆæ•´æ•°ï¼‰
#   Value: list of allowed difficulty IDs / å€¼ï¼šå…è®¸çš„éš¾åº¦ ID åˆ—è¡¨
#

# Difficulty mapping reference (ignored cf_rating for now to keep it simple):
# éš¾åº¦æ˜ å°„å‚è€ƒï¼ˆä¸ºç®€åŒ–èµ·è§æš‚æ—¶å¿½ç•¥ cf_ratingï¼‰ï¼š
#   Source 2 (Codeforces) & Source 5 (AtCoder):
#     7=A, 8=B, 9=C, 10=D, 11=E, 12=F, 13=G, 14=H...
#   Source 1 & Source 3 (Other platforms):
#     1=EASY, 2=MEDIUM, 3=HARD, 4=VERY_HARD...
#
DATASET_CONFIG = {
    2: [7, 8],      # Codeforces: A-B level (Introductory) / Codeforcesï¼šA-B çº§åˆ«ï¼ˆå…¥é—¨ï¼‰
    5: [7, 8],      # AtCoder: A-B level (Introductory) / AtCoderï¼šA-B çº§åˆ«ï¼ˆå…¥é—¨ï¼‰
    1: [1],         # General platforms: EASY only / é€šç”¨å¹³å°ï¼šä»…ç®€å•éš¾åº¦
    3: [1],         # General platforms: EASY only / é€šç”¨å¹³å°ï¼šä»…ç®€å•éš¾åº¦
}

# ========== Reward Scaling Configuration ==========
# ========== å¥–åŠ±ç¼©æ”¾é…ç½® ==========
# Hierarchical scaling: Source (1st) -> Difficulty (2nd)
# å±‚çº§ç¼©æ”¾ï¼šæ•°æ®æºï¼ˆç¬¬ä¸€å…³é”®å­—ï¼‰ -> éš¾åº¦ï¼ˆç¬¬äºŒå…³é”®å­—ï¼‰
# Note: These multipliers are applied to the base reward (0.5 for compile, up to 1.0 for pass)
# æ³¨æ„ï¼šè¿™äº›å€æ•°åº”ç”¨äºåŸºç¡€å¥–åŠ±ï¼ˆç¼–è¯‘æˆåŠŸ 0.5ï¼Œé€šè¿‡æ‰€æœ‰æµ‹è¯•æœ€é«˜ 1.0ï¼‰
REWARD_SCALING_CONFIG = {
    2: {          # Codeforces
        7: 1.0,   # A level: baseline / Açº§ï¼šåŸºå‡†
        8: 1.1,   # B level: slightly higher / Bçº§ï¼šç•¥é«˜
    },
    5: {          # AtCoder
        7: 1.0,   # A level: baseline / Açº§ï¼šåŸºå‡†
        8: 1.1,   # B level: slightly higher / Bçº§ï¼šç•¥é«˜
    },
    1: { 1: 1.0 }, # General platforms: EASY / é€šç”¨å¹³å°ï¼šç®€å•
    3: { 1: 1.0 }, # General platforms: EASY / é€šç”¨å¹³å°ï¼šç®€å•
}

# ========== Model Configuration ==========
# ========== æ¨¡å‹é…ç½® ==========
MS_MODEL_ID = "qwen/Qwen2.5-Coder-3B-Instruct"
LOCAL_MODEL_DIR = "./models/Qwen2.5-Coder-3B-Instruct"
OUTPUT_DIR = "./output/luoguqwencoder-lora"



#  Qwen2.5-Coder-3B-Instruct
# ========== ä¸‹è½½æ¨¡å‹ ==========
if not os.path.exists(LOCAL_MODEL_DIR):
    print(f"Download from ModelScope/ä»ModelScopeä¸‹è½½æ¨¡å‹ {MS_MODEL_ID} åˆ° {LOCAL_MODEL_DIR}...")
    snapshot_download(
        repo_id=MS_MODEL_ID,
        local_dir=LOCAL_MODEL_DIR,
    )
    print("æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
else:
    print(f"Load from local/æœ¬åœ°å·²å­˜åœ¨æ¨¡å‹ï¼Œç›´æ¥åŠ è½½ï¼š{LOCAL_MODEL_DIR}")

# ========== Load tokenizer =========
# ========== åŠ è½½ tokenizer ==========
tokenizer = AutoTokenizer.from_pretrained(
    LOCAL_MODEL_DIR,
    trust_remote_code=True,
)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"


# ========== Load model =====

# ========== åŠ è½½æ¨¡å‹ï¼ˆ4bit é‡åŒ–ï¼‰==========
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)
model = AutoModelForCausalLM.from_pretrained(
    LOCAL_MODEL_DIR,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    # torch_dtype=torch.bfloat16,
    dtype=torch.bfloat16,
)
model.config.use_cache = False

# å‡†å¤‡æ¨¡å‹è¿›è¡Œ k-bit è®­ç»ƒ
model = prepare_model_for_kbit_training(model)


# ========== Define TinyLoRA Layers ==========
# ========== å®šä¹‰ TinyLoRA å±‚ ==========

# è·å–æ¨¡å‹ç¬¬ä¸€å±‚çš„è®¾å¤‡ (é€šå¸¸æ˜¯ cuda:0)
device = model.model.layers[0].self_attn.q_proj.weight.device
print(f"Model device/æ¨¡å‹ä¸»è®¾å¤‡: {device}")

# ã€ä¿®å¤é”™è¯¯1ã€‘åˆ›å»ºä¸€ä¸ª wrapper module æ¥æ­£ç¡®æ³¨å†Œ global_v
class TinyLoRAGlobalParams(nn.Module):
    """ä¸“é—¨ç”¨äºæ³¨å†Œå…¨å±€å…±äº«å‘é‡çš„å®¹å™¨"""
    def __init__(self, u_dim=16, device='cpu', dtype=torch.bfloat16):
        super().__init__()
        # è¿™æ ·æ³¨å†Œæ‰ä¼šè¢« model.named_parameters() è¯†åˆ«
        self.global_v = nn.Parameter(torch.zeros(u_dim, device=device, dtype=dtype))
    
    def forward(self):
        # å®¹å™¨æ¨¡å—ä¸éœ€è¦å®é™…çš„å‰å‘é€»è¾‘
        return self.global_v

# åˆ›å»ºå…¨å±€å‚æ•°å®¹å™¨
global_params = TinyLoRAGlobalParams(u_dim=U_VALUE, device=device, dtype=torch.bfloat16)


class TinyLoRALinear(nn.Module):
    def __init__(self, original_layer, rank = 2, u = None, global_params_ref=None):
        if u is None:
            u = U_VALUE

    # R= v_1 P_1 + v_2 P_2 + ... + v_u P_u
    # véƒ½æ˜¯scalar
    # Péƒ½æ˜¯rank x rankçš„çŸ©é˜µ
    # global_params_ref: æŒ‡å‘åŒ…å« global_v çš„å®¹å™¨æ¨¡å—

        super().__init__()
        # å¿…å…ˆç»§æ‰¿çˆ¶ç±»çš„åˆå§‹åŒ–å‡½æ•°ï¼Œæ‰èƒ½ä½¿ç”¨ nn.Module çš„åŠŸèƒ½ï¼ˆä¾‹å¦‚æ³¨å†Œå‚æ•°å’Œç¼“å†²åŒºï¼‰ã€‚
        
        #  super().__init__() æ˜¯ä»€ä¹ˆï¼Ÿ
        # è¿™æ˜¯ Python é¢å‘å¯¹è±¡ç¼–ç¨‹ï¼ˆOOPï¼‰çš„æ ‡å‡†å†™æ³•ã€‚
        # å«ä¹‰ï¼šè°ƒç”¨çˆ¶ç±»ï¼ˆParent Classï¼‰çš„åˆå§‹åŒ–å‡½æ•°ã€‚
        # åœ¨è¿™é‡Œçš„ä½œç”¨ï¼šä½ çš„ç±» TinyLoRALinear ç»§æ‰¿è‡ª nn.Moduleï¼ˆPyTorch çš„ç¥ç»ç½‘ç»œåŸºç±»ï¼‰ã€‚æ‰§è¡Œ super().__init__() æ˜¯ä¸ºäº†è®© PyTorch çš„æœºåˆ¶ç”Ÿæ•ˆï¼Œæ¯”å¦‚ï¼š
        # æ³¨å†Œä½ å®šä¹‰çš„ self.v ä¸ºå¯è®­ç»ƒå‚æ•°ã€‚
        # æ³¨å†Œ self.U, self.S ç­‰ä¸º Bufferï¼ˆä¸è®­ç»ƒçš„å‚æ•°ï¼‰ã€‚


        print(f"original_layer.device: {original_layer.weight.device}, dtype: {original_layer.weight.dtype}")

        original_device = original_layer.weight.device # è®°å½•åŸdevice


        self.base_layer = original_layer
        
      
        if global_params_ref is None:
            raise RuntimeError("å¿…é¡»ä¼ å…¥ global_params_refï¼")
        self.global_params_ref = global_params_ref

        W = original_layer.weight.data.float()
        if hasattr(original_layer.weight, "quant_state"):
         
            W_real = bnb.functional.dequantize_4bit(
                original_layer.weight.data, 
                original_layer.weight.quant_state,
                quant_type="nf4"  # ä¸ BitsAndBytesConfig ä¸­çš„é…ç½®ä¸€è‡´
            )
        else:
            # éé‡åŒ–æƒ…å†µ
            W_real = original_layer.weight.data


        W_real_on_cpu = W_real.float().cpu()

        U, S ,Vh = torch.linalg.svd( W_real_on_cpu ,full_matrices=False)

        # SVD åˆ†è§£ W çŸ©é˜µ
        # W = U S Vh 
        # Vhæ˜¯ Vçš„Hermitian transposedï¼Œå…±è½­è½¬ç½®
        # å†»ç»“ U, S, V (LoRA-XS çš„éª¨æ¶)

        

        # å°†ç»“æœè½¬å› BFloat16 å¹¶ç§»å› GPU
        # æˆªæ–­å¹¶æ³¨å†Œ(å³å›ºå®šä½)
        # å»ºè®®è½¬å› bf16 çœæ˜¾å­˜
        # 
        # è¿™ä¸€æ­¥ä¹Ÿæ˜¯ä¸ºäº†è®© TinyLoRA çš„å‚æ•°å’Œä¸»æ¨¡å‹ç²¾åº¦ä¿æŒä¸€è‡´
        
        target_dtype = torch.bfloat16

        self.register_buffer('U', U[:, :rank].to(original_device).to(target_dtype)) 
        self.register_buffer('S', torch.diag(S[:rank]).to(original_device).to(target_dtype))
        self.register_buffer('Vh', Vh[:rank, :].to(original_device).to(target_dtype))
        
        # å›ºå®šéšæœºçŸ©é˜µ P  (For TinyLoRA)
        self.register_buffer('P', torch.randn(u, rank, rank, device=original_device, dtype=target_dtype))

    def forward(self, x):
        # åŠ¨æ€ä»å®¹å™¨ä¸­è·å– global_vï¼Œè€Œä¸æ˜¯ä½œä¸ºè‡ªå·±çš„å±æ€§
        # è¿™æ ·ç¡®ä¿ v åªè¢« model.tiny_lora_params æ³¨å†Œä¸€æ¬¡
        v = self.global_params_ref.global_v
        
        # è®¡ç®— TinyLoRA çš„å¢é‡çŸ©é˜µ R = sum_i(v_i * P_i)
        # æ³¨æ„ï¼šä¸èƒ½ç”¨ 'u, urr -> rr'ï¼Œå› ä¸º einsum è¾“å‡ºä¸­åŒä¸€ä¸‹æ ‡ä¸èƒ½é‡å¤
        # å¿…é¡»ç”¨ä¸åŒå­—æ¯åŒºåˆ†ä¸¤ä¸ª rank ç»´åº¦
        R = torch.einsum('u, uij -> ij', v, self.P)
        # é‡ç»„å¢é‡æƒé‡
        delta_W = self.U @ self.S @ R @ self.Vh
        # å‰å‘ä¼ æ’­ï¼šx * (W + delta_W)^T
        return self.base_layer(x) + x @ delta_W.t()


def apply_tiny_lora(model, global_params_ref):
    """
    éå†æ¨¡å‹ï¼Œå°†æ‰€æœ‰ç›®æ ‡ Linear å±‚æ›¿æ¢ä¸º TinyLoRALinearï¼Œ
    å¹¶ä¼ å…¥å¯¹ global_params å®¹å™¨çš„å¼•ç”¨ï¼Œå®ç°è®ºæ–‡ä¸­çš„ Tiling (å…¨å‚æ•°å…±äº«)ã€‚
    """
    # Qwen/Llama çš„ç›®æ ‡æ¨¡å—åç§°é€šå¸¸åŒ…å«è¿™äº›
    target_suffixes = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    
    # è®¡æ•°å™¨
    replaced_count = 0
    
    # é€’å½’å‡½æ•°ï¼šéå†å­æ¨¡å—
    for name, child in model.named_children():
        # å¦‚æœæ˜¯ç›®æ ‡ Linear å±‚
        if isinstance(child, (nn.Linear, bnb.nn.Linear4bit)) and any(name.endswith(s) for s in target_suffixes):
            # 1. åˆ›å»º TinyLoRA å±‚ï¼Œä¼ å…¥ global_params å®¹å™¨çš„å¼•ç”¨
            new_layer = TinyLoRALinear(child, rank=2, u=U_VALUE, global_params_ref=global_params_ref)
            
            # 2. æ›¿æ¢æ‰åŸæ¨¡å— (Monkey Patch)
            setattr(model, name, new_layer)
            replaced_count += 1
            print(f"âœ… Replace successfully/å·²æ›¿æ¢: {name} -> TinyLoRA (Shared)")
            
        else:
            # ç»§ç»­é€’å½’éå†å­æ¨¡å— (ä¾‹å¦‚ model.layers.0.self_attn...)
            replaced_count += apply_tiny_lora(child, global_params_ref)
            
    return replaced_count

# ========== æ‰§è¡Œæ›¿æ¢ ==========
print("Start replacing/æ­£åœ¨åº”ç”¨ TinyLoRA Tiling (å‚æ•°å…±äº«)...")

print("It's normal to see many lines of 'replace'./çœ‹åˆ°å¾ˆå¤šæ›¿æ¢æ—¥å¿—æ˜¯æ­£å¸¸çš„ã€‚")
# ã€å…³é”®ã€‘å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿ P çŸ©é˜µå¯å¤ç°
# ä¿å­˜æ¨¡å‹æ—¶åªå­˜ v å‘é‡ï¼ŒåŠ è½½æ—¶éœ€è¦ç”¨ç›¸åŒç§å­é‡å»º P çŸ©é˜µ
TINYLORA_SEED = 42
torch.manual_seed(TINYLORA_SEED)
torch.cuda.manual_seed(TINYLORA_SEED)
print(f"âœ… Fix TinyLoRA seed/å·²å›ºå®š TinyLoRA éšæœºç§å­: {TINYLORA_SEED}")

# ã€å…³é”®ä¿®å¤ã€‘å…ˆå°† global_params æ³¨å†Œä¸ºæ¨¡å‹çš„å­æ¨¡å—
# è¿™æ ·åœ¨å±‚æ›¿æ¢æ—¶ï¼ŒTinyLoRALinear å°±èƒ½é€šè¿‡å¼•ç”¨è®¿é—®åˆ°å·²æ³¨å†Œçš„ global_v
model.tiny_lora_params = global_params
print(f"âœ… Register global_params to model/å·²å°† global_params æ³¨å†Œåˆ°æ¨¡å‹")

# ç„¶åå†è¿›è¡Œå±‚æ›¿æ¢ï¼Œä¼ å…¥ global_params å®¹å™¨æœ¬èº«
total_replaced = apply_tiny_lora(model, global_params)
print(f"âœ… Replace completed/æ›¿æ¢å®Œæˆï¼å…±æ›¿æ¢äº† {total_replaced} ä¸ªæ¨¡å—ã€‚")

# ========== å…³é”®æ­¥éª¤ï¼šå†»ç»“é™¤ v ä»¥å¤–çš„æ‰€æœ‰å‚æ•° ==========
print("Freezing parameters/æ­£åœ¨å†»ç»“æ¨¡å‹å‚æ•°...")

# ã€æ›´ä¼˜é›…çš„æ–¹æ¡ˆã€‘ç›´æ¥é€šè¿‡å¯¹è±¡å¼•ç”¨æ“ä½œï¼Œä¸ä¾èµ–å­—ç¬¦ä¸²åŒ¹é…
# 1. ç¬¬ä¸€æ­¥ï¼šå…¨å±€å†»ç»“æ‰€æœ‰å‚æ•°
model.requires_grad_(False)

# 2. ç¬¬äºŒæ­¥ï¼šç²¾å‡†è§£å†» global_v
# ç›´æ¥é€šè¿‡å¯¹è±¡å¼•ç”¨æ“ä½œï¼Œç»å¯¹ç¨³å¥
global_params.global_v.requires_grad = True
print(f"âœ… Trainable parameter/å¯è®­ç»ƒå‚æ•°: global_v, shape={global_params.global_v.shape}")

# éªŒè¯å¯è®­ç»ƒå‚æ•°
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
all_params = sum(p.numel() for p in model.parameters())
print(f"\nTotal parameters/æ€»å‚æ•°é‡: {all_params:,}")
print(f"Trainable parameters/å¯è®­ç»ƒå‚æ•°é‡: {trainable_params}")
if trainable_params != U_VALUE:
    raise RuntimeError(f"è­¦å‘Šï¼šå¯è®­ç»ƒå‚æ•°æ•°é‡ä¸º {trainable_params}ï¼Œé¢„æœŸä¸º {U_VALUE}ï¼")

import re
import subprocess
import tempfile
import os

def compile_and_run(code, test_cases):
    """
    Compile and run code against multiple test cases, return reward / ç¼–è¯‘å¹¶è¿è¡Œä»£ç 
    test_cases: list of dicts, each containing 'input' and 'output' / æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
    Return: 0.0 (compile fail) / 0.5 (partial pass) / 1.0 (all pass)
    è¿”å›ï¼š0.0ï¼ˆç¼–è¯‘å¤±è´¥ï¼‰ / 0.5ï¼ˆéƒ¨åˆ†é€šè¿‡ï¼‰ / 1.0ï¼ˆå…¨éƒ¨é€šè¿‡ï¼‰
    """
    code = re.sub(r'freopen\s*\(.*?\);', '', code, flags=re.IGNORECASE)
    
    # Create temp directory / åˆ›å»ºä¸´æ—¶ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        src_file = os.path.join(temp_dir, "solution.cpp")
        exe_file = os.path.join(temp_dir, "solution")
        
        # Write C++ code / å†™å…¥ C++ ä»£ç 
        with open(src_file, 'w', encoding='utf-8') as f:
            f.write(code)
            
        # Compile with -O2 optimization / ç¼–è¯‘
        try:
            compile_result = subprocess.run(
                ['g++', src_file, '-o', exe_file, '-O2'],
                capture_output=True, text=True, timeout=5
            )
            if compile_result.returncode != 0:
                return 0.0  # Compile failed / ç¼–è¯‘å¤±è´¥
        except subprocess.TimeoutExpired:
            return 0.0  # Compile timeout / ç¼–è¯‘è¶…æ—¶

        # Run all test cases / è¿è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        passed = 0
        for test_case in test_cases:
            input_data = test_case['input']
            expected_output = test_case['output'].strip()
            
            try:
                run_result = subprocess.run(
                    [exe_file],
                    input=input_data,
                    capture_output=True,
                    text=True,
                    timeout=2 
                )
                
                actual_output = run_result.stdout.strip()
                
                if actual_output == expected_output:
                    passed += 1
                    
            except (subprocess.TimeoutExpired, Exception):
                pass  # Test case failed / æµ‹è¯•ç”¨ä¾‹å¤±è´¥
        
        # Return score: 1.0 if all passed, 0.5 if partial, 0.0 if none
        if passed == len(test_cases):
            return 1.0
        elif passed > 0:
            return 0.6 + float(passed/len(test_cases))*0.4  # Partial pass / éƒ¨åˆ†é€šè¿‡
        else:
            return 0.5  # At least compiled / è‡³å°‘ç¼–è¯‘æˆåŠŸ

def code_reward_func(prompts, completions, public_tests=None, private_tests=None, generated_tests=None, source=None, difficulty=None, **kwargs):
    """
    GRPO reward function for code evaluation / GRPO çš„å¥–åŠ±å‡½æ•°
    
    For deepmind/code_contests dataset:
    - public_tests, private_tests, generated_tests: dicts with 'input' and 'output' as lists
    - We evaluate against public_tests, private_tests, and generated_tests
    
    å¯¹äº deepmind/code_contests æ•°æ®é›†ï¼š
    - public_tests, private_tests, generated_testsï¼šåŒ…å« 'input' å’Œ 'output' åˆ—è¡¨çš„å­—å…¸
    - å¯¹ public_tests, private_tests å’Œ generated_tests è¿›è¡Œè¯„ä¼°
    
    Reward rules / å¥–åŠ±è§„åˆ™ï¼š
    - Compile fail or invalid format: 0.0 / ç¼–è¯‘å¤±è´¥æˆ–æ— æ•ˆæ ¼å¼ï¼š0.0
    - Compile success, partial tests pass: 0.5 / ç¼–è¯‘æˆåŠŸï¼Œéƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼š0.5
    - All tests pass: 1.0 / æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼š1.0
    """
    rewards = []
    
    # Convert None to empty lists / å°† None è½¬æ¢ä¸ºç©ºåˆ—è¡¨
    if public_tests is None:
        public_tests = [None] * len(completions)
    if private_tests is None:
        private_tests = [None] * len(completions)
    if generated_tests is None:
        generated_tests = [None] * len(completions)
    if source is None:
        source = [0] * len(completions)
    if difficulty is None:
        difficulty = [0] * len(completions)
    
    # Iterate through each generated completion / éå†æ¯ä¸€æ¡ç”Ÿæˆçš„å›å¤
    for completion, pub_test, priv_test, gen_test, src, diff in zip(
        completions, public_tests, private_tests, generated_tests, source, difficulty
    ):
        # 1. Extract code block / æå–ä»£ç å—
        match = re.search(r"```(?:cpp|c\+\+)?\n(.*?)```", completion, re.DOTALL)
        
        if not match:
            # Fallback: look for raw code with #include / å¤‡é€‰ï¼šå¯»æ‰¾åŒ…å« #include çš„è£¸ä»£ç 
            if "#include" in completion:
                code = completion
            else:
                rewards.append(0.0)  # Invalid format / æ ¼å¼å®Œå…¨ä¸å¯¹
                continue
        else:
            code = match.group(1)

        # 2. Parse test cases from dict format / ä»å­—å…¸æ ¼å¼è§£ææµ‹è¯•ç”¨ä¾‹
        test_cases_list = []
        
        # Public tests: dict with 'input' and 'output' as lists / å…¬å¼€æµ‹è¯•
        if pub_test and isinstance(pub_test, dict) and 'input' in pub_test and 'output' in pub_test:
            inputs = pub_test['input'] if isinstance(pub_test['input'], list) else [pub_test['input']]
            outputs = pub_test['output'] if isinstance(pub_test['output'], list) else [pub_test['output']]
            for inp, out in zip(inputs, outputs):
                test_cases_list.append({'input': inp, 'output': out})
        
        # Private tests: dict with 'input' and 'output' as lists / ç§æœ‰æµ‹è¯•
        if priv_test and isinstance(priv_test, dict) and 'input' in priv_test and 'output' in priv_test:
            inputs = priv_test['input'] if isinstance(priv_test['input'], list) else [priv_test['input']]
            outputs = priv_test['output'] if isinstance(priv_test['output'], list) else [priv_test['output']]
            for inp, out in zip(inputs, outputs):
                test_cases_list.append({'input': inp, 'output': out})
        
        # Generated tests: dict with 'input' and 'output' as lists / ç”Ÿæˆçš„æµ‹è¯•
        if gen_test and isinstance(gen_test, dict) and 'input' in gen_test and 'output' in gen_test:
            inputs = gen_test['input'] if isinstance(gen_test['input'], list) else [gen_test['input']]
            outputs = gen_test['output'] if isinstance(gen_test['output'], list) else [gen_test['output']]
            for inp, out in zip(inputs, outputs):
                test_cases_list.append({'input': inp, 'output': out})
        
        # If no test cases extracted, give penalty / å¦‚æœæ²¡æœ‰æå–åˆ°æµ‹è¯•ç”¨ä¾‹ï¼Œç»™äºˆæƒ©ç½š
        if not test_cases_list:
            rewards.append(0.0)
            continue
        
        # 3. Run code against all test cases / å¯¹æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹è¿è¡Œä»£ç 
        base_reward = compile_and_run(code, test_cases_list)
        
        # 4. REWARD SCALING - Adjust based on source and difficulty
        # å¥–åŠ±ç¼©æ”¾ - æ ¹æ®æºå’Œéš¾åº¦ä» REWARD_SCALING_CONFIG æŸ¥æ‰¾
        # ============================================================================
        # Hierarchical lookup: Source -> Difficulty
        reward_multiplier = 1.0
        
        if src in REWARD_SCALING_CONFIG:
            source_scaling = REWARD_SCALING_CONFIG[src]
            if diff in source_scaling:
                reward_multiplier = source_scaling[diff]
            elif -1 in source_scaling: # Fallback for unknown difficulty in known source
                reward_multiplier = source_scaling[-1]
        
        # ============================================================================
        # Apply multiplier to base reward / å¯¹åŸºç¡€å¥–åŠ±åº”ç”¨å€æ•°
        final_reward = base_reward * reward_multiplier
        rewards.append(final_reward)
        
    return rewards
    

# ã€æœ€ç»ˆæ–¹æ¡ˆã€‘ç»•è¿‡ Trainer å¯¹çº¯é‡åŒ–æ¨¡å‹çš„æ£€æŸ¥
# Trainer çš„æ£€æŸ¥é€»è¾‘ (transformers/trainer.py):
#   _is_quantized_and_base_model = model.is_quantized AND NOT model._hf_peft_config_loaded
#   if _is_quantized_and_base_model and not isinstance(model, PeftModel): raise ValueError
#
# æˆ‘ä»¬çš„ TinyLoRA æ˜¯åˆæ³•çš„ adapterï¼ˆåªè®­ç»ƒ 16 ä¸ªå‚æ•°ï¼‰ï¼Œä½†ä¸æ˜¯æ ‡å‡† PeftModelã€‚
# è®¾ç½® _hf_peft_config_loaded = True è®©ç¬¬ä¸€é“æ£€æŸ¥ç›´æ¥ä¸º Falseï¼Œä¸ä¼šèµ°åˆ° isinstance åˆ¤æ–­ã€‚
# è¿™ä¸å½±å“å®é™…è®¡ç®—â€”â€”æƒé‡å·²ç»åœ¨å†…å­˜ä¸­é‡åŒ–ï¼ŒTinyLoRA å±‚æ­£ç¡®å¤„ç†äº†åé‡åŒ–ã€‚
model._hf_peft_config_loaded = True

print("âœ… Set _hf_peft_config_loaded=True / å·²è®¾ç½® _hf_peft_config_loaded=Trueï¼šbypass Trainer quantization check")

def filter_dataset(dataset, config, max_samples, seed=42):
    """
    Filter dataset based on source and difficulty configuration.
    æ ¹æ®æ•°æ®æºå’Œéš¾åº¦é…ç½®è¿‡æ»¤æ•°æ®é›†ã€‚
    
    Args:
        dataset: HuggingFace Dataset object / HuggingFace æ•°æ®é›†å¯¹è±¡
        config: Dict mapping source IDs to allowed difficulty lists / å°†æ•°æ®æº ID æ˜ å°„åˆ°å…è®¸çš„éš¾åº¦åˆ—è¡¨çš„å­—å…¸
        max_samples: Maximum number of samples after filtering / è¿‡æ»¤åçš„æœ€å¤§æ ·æœ¬æ•°
        seed: Random seed for shuffling / ç”¨äºæ‰“ä¹±çš„éšæœºç§å­
    
    Returns:
        Filtered and sampled dataset / è¿‡æ»¤å¹¶é‡‡æ ·åçš„æ•°æ®é›†
    """
    print("\n" + "="*60)
    print("ğŸ” Filtering dataset based on configuration...")
    print("ğŸ” æ ¹æ®é…ç½®è¿‡æ»¤æ•°æ®é›†...")
    print("="*60)
    
    # Log configuration / è®°å½•é…ç½®
    source_names = {
        1: "General Platform 1 / é€šç”¨å¹³å° 1",
        2: "Codeforces / Codeforces",
        3: "General Platform 3 / é€šç”¨å¹³å° 3",
        5: "AtCoder / AtCoder",
    }
    
    for source_id, difficulties in config.items():
        source_name = source_names.get(source_id, f"Source {source_id} / æ•°æ®æº {source_id}")
        print(f"ğŸ“Œ {source_name}: Keeping difficulties {difficulties} / ä¿ç•™éš¾åº¦ {difficulties}")
    
    # Filter function / è¿‡æ»¤å‡½æ•°
    def should_keep(example):
        source = example.get('source', -1)
        difficulty = example.get('difficulty', -1)
        
        # Check if source is in config / æ£€æŸ¥æ•°æ®æºæ˜¯å¦åœ¨é…ç½®ä¸­
        if source not in config:
            return False
        
        # Check if difficulty is allowed for this source / æ£€æŸ¥è¯¥æ•°æ®æºæ˜¯å¦å…è®¸æ­¤éš¾åº¦
        if difficulty not in config[source]:
            return False
        
        return True
    
    # Apply filter / åº”ç”¨è¿‡æ»¤
    print("\nâ³ Filtering in progress... / æ­£åœ¨è¿‡æ»¤...")
    original_size = len(dataset)
    filtered_dataset = dataset.filter(should_keep)
    filtered_size = len(filtered_dataset)
    
    print(f"âœ… Original dataset size / åŸå§‹æ•°æ®é›†å¤§å°: {original_size:,}")
    print(f"âœ… After filtering / è¿‡æ»¤å: {filtered_size:,} samples / æ ·æœ¬")
    print(f"ğŸ“Š Retention rate / ä¿ç•™ç‡: {filtered_size/original_size*100:.2f}%")
    
    # Apply max_samples limit with shuffling / åº”ç”¨æœ€å¤§æ ·æœ¬æ•°é™åˆ¶å¹¶æ‰“ä¹±
    if filtered_size > max_samples:
        print(f"\nğŸ² Shuffling and sampling {max_samples:,} from {filtered_size:,}...")
        print(f"ğŸ² æ‰“ä¹±å¹¶ä» {filtered_size:,} ä¸­é‡‡æ · {max_samples:,} ä¸ª...")
        filtered_dataset = filtered_dataset.shuffle(seed=seed).select(range(max_samples))
        final_size = len(filtered_dataset)
        print(f"âœ… Final training set size / æœ€ç»ˆè®­ç»ƒé›†å¤§å°: {final_size:,}")
    else:
        print(f"\nâœ… All {filtered_size:,} filtered samples will be used (below max_samples limit).")
        print(f"âœ… å°†ä½¿ç”¨å…¨éƒ¨ {filtered_size:,} ä¸ªè¿‡æ»¤åçš„æ ·æœ¬ï¼ˆä½äºæœ€å¤§æ ·æœ¬æ•°é™åˆ¶ï¼‰ã€‚")
        final_size = filtered_size
    
    print("="*60 + "\n")
    return filtered_dataset


def apply_chat_template(example):
    """
    Build prompt from problem description and public test cases.
    For deepmind/code_contests dataset structure.
    
    ä»é—®é¢˜æè¿°å’Œå…¬å¼€æµ‹è¯•ç”¨ä¾‹æ„å»ºæç¤ºã€‚
    é€‚ç”¨äº deepmind/code_contests æ•°æ®é›†ç»“æ„ã€‚
    """
    # Extract problem description / æå–é—®é¢˜æè¿°
    description = example.get('description', '')
    
    # Build public test cases section / æ„å»ºå…¬å¼€æµ‹è¯•ç”¨ä¾‹éƒ¨åˆ†
    public_tests_section = ""
    public_tests = example.get('public_tests', {})
    
    if isinstance(public_tests, dict) and 'input' in public_tests and 'output' in public_tests:
        inputs = public_tests['input'] if isinstance(public_tests['input'], list) else [public_tests['input']]
        outputs = public_tests['output'] if isinstance(public_tests['output'], list) else [public_tests['output']]
        
        if inputs and outputs:
            public_tests_section = "\nã€Casesã€‘\n"
            for i, (inp, out) in enumerate(zip(inputs, outputs), 1):
                public_tests_section += f"Test {i}:\n"
                public_tests_section += f"Input :\n{inp}\n"
                public_tests_section += f"Output:\n{out}\n"
    
    # Combine into final prompt / ç»„åˆæˆæœ€ç»ˆæç¤º
    final_prompt = f"""You will be given a programming contest problem. Please reason step by step and provide a complete C++ implementation.
Output the solution in a code block. Do not include debugging info or extra output. Limit reasoning to 128 tokens.


ã€Problem Description ã€‘
{description}

{public_tests_section}

Please provide your C++ solution :"""
    
    # Build Qwen chat template format / æ„å»º Qwen èŠå¤©æ¨¡æ¿æ ¼å¼
    messages = [
        {"role": "system", "content": "You are an expert competitive programmer. Output valid C++ code that compiles and solves the problem correctly."},
        {"role": "user", "content": final_prompt}
    ]
    
    # Apply chat template using tokenizer / ä½¿ç”¨åˆ†è¯å™¨åº”ç”¨èŠå¤©æ¨¡æ¿
    example['prompt'] = tokenizer.apply_chat_template(
        messages, 
        tokenize=False,
        add_generation_prompt=True
    )
    
    return example



# ========== Load dataset ==========
# When using load_dataset("json", data_files="....jsonl"),
# HuggingFace defaults to classifying the file as 'train' split.
# Note: data_files points to processed file path from download_dataset.py
# split="train" is important! Trainer needs Dataset object, not DatasetDict
# 
# ========== åŠ è½½æ•°æ®é›† ==========

rl_dataset = load_dataset(
    "json", 
    data_files="./local_code_contests/code_contests_train.jsonl",
    split="train"
)

# Apply filtering based on source and difficulty configuration
# æ ¹æ®æ•°æ®æºå’Œéš¾åº¦é…ç½®åº”ç”¨è¿‡æ»¤
rl_dataset = filter_dataset(
    dataset=rl_dataset,
    config=DATASET_CONFIG,
    max_samples=MAX_SAMPLES,
    seed=TINYLORA_SEED
)



# Apply template / åº”ç”¨æ¨¡ç‰ˆ
rl_dataset = rl_dataset.map(apply_chat_template)

# Print sample to verify / æ‰“å°ä¸€æ¡æ•°æ®éªŒè¯
print(f"âœ… Dataset loaded successfully! / æ•°æ®åŠ è½½æˆåŠŸï¼Total samples / æ ·æœ¬æ•°é‡: {len(rl_dataset)}")
print(f"ğŸ“ Sample data / æ ·ä¾‹æ•°æ®: {rl_dataset[0]}")



# ========== Configure and start GRPO training ==========
# Configure GRPO / é…ç½® GRPO
training_args = GRPOConfig(
    output_dir=OUTPUT_DIR,
    num_train_epochs=1,
    per_device_train_batch_size=1,  # Set to 1 if GPU memory insufficient / æ˜¾å­˜ä¸è¶³è®¾ä¸º 1
    gradient_accumulation_steps=8,  # Accumulate to simulate large batch / ç´¯ç§¯æ¢¯åº¦æ¨¡æ‹Ÿå¤§ Batch
    learning_rate=1e-5,             # RL learning rate usually small / RL å­¦ä¹ ç‡é€šå¸¸è¦å°
    num_generations=4,              # Group Size (G): sample 4 answers per iteration / æ¯æ¬¡é‡‡æ · 4 ä¸ªç­”æ¡ˆ
    max_completion_length=1024,     # Max generation length / ç”Ÿæˆçš„æœ€å¤§é•¿åº¦
    logging_steps=1,
    bf16=True,                      # Enable BF16 acceleration / å¼€å¯ BF16 åŠ é€Ÿ
    save_strategy="no",             # Disable auto checkpoint (TinyLoRA is non-standard PEFT)
)

# Initialize trainer / åˆå§‹åŒ–è®­ç»ƒå™¨
trainer = GRPOTrainer(
    model=model,
    reward_funcs=code_reward_func,  # Your judge function / ä½ çš„åˆ¤é¢˜å‡½æ•°
    args=training_args,
    train_dataset=rl_dataset,       # Processed data / å¤„ç†å¥½çš„æ•°æ®
    processing_class=tokenizer,     # Tokenizer
)

# Start training! / å¼€å§‹è®­ç»ƒï¼
print("ğŸš€ Starting TinyLoRA-RL training... / å¼€å§‹ TinyLoRA-RL è®­ç»ƒ...")
trainer.train()

# Save training results / ä¿å­˜è®­ç»ƒç»“æœ
# Note: peft's save_pretrained may not recognize custom layers
# Manually save global_v and metadata needed to rebuild model
# æ³¨æ„ï¼špeft çš„ save_pretrained å¯èƒ½ä¸è®¤ä½ çš„è‡ªå®šä¹‰å±‚
# æ‰‹åŠ¨ä¿å­˜ global_v ä»¥åŠé‡å»ºæ¨¡å‹æ‰€éœ€çš„å…ƒä¿¡æ¯
os.makedirs(OUTPUT_DIR, exist_ok=True)

save_dict = {
    "global_v": global_params.global_v.data,  # Trained v vector / è®­ç»ƒå¥½çš„ v å‘é‡
    "u_value": U_VALUE,                        # Dimension of v / v çš„ç»´åº¦
    "rank": 2,                                 # TinyLoRA rank
    "seed": TINYLORA_SEED,                     # P matrix random seed (for reproducibility)
    "model_id": MS_MODEL_ID,                   # Base model ID / åŸºåº§æ¨¡å‹ ID
    "total_replaced_layers": total_replaced,   # Number of replaced layers / æ›¿æ¢çš„å±‚æ•°
}
torch.save(save_dict, f"{OUTPUT_DIR}/tiny_lora_v.pt")
print(f"âœ… Training complete! / è®­ç»ƒå®Œæˆï¼Parameters saved to / å‚æ•°å·²ä¿å­˜è‡³ {OUTPUT_DIR}/tiny_lora_v.pt")
print(f"ğŸ“Š Save contents / ä¿å­˜å†…å®¹: global_v (shape={global_params.global_v.shape}), u={U_VALUE}, rank=2, seed={TINYLORA_SEED}")