import torch
import torch.nn as nn
import os
import sys
import re
import subprocess
import tempfile
from datasets import load_dataset, load_from_disk
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainerCallback,
)
from peft import prepare_model_for_kbit_training
from trl import GRPOTrainer, GRPOConfig
from modelscope.hub.snapshot_download import snapshot_download
import bitsandbytes as bnb

# Import shared utilities / å¯¼å…¥å…±äº«å·¥å…·
from utils import (
    TinyLoRAGlobalParams,
    TinyLoRALinear,
    apply_tiny_lora,
    compile_and_run,
    convert_hf_tests_to_list,
    extract_code_from_response,
    apply_chat_template,
)

print("âœ… All libraries imported successfully! / æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸï¼")
print("ğŸ“ Usage example: python train_rl.py [u_value] [max_samples] [--do_validate] [--val_steps N] [--val_samples N]")
print("   First arg: TinyLoRA u value (default: 16)")
print("   Second arg: max training samples (default: 2000)")
print("   --do_validate: Enable validation during training")
print("   --val_steps: Run validation every N steps (default: 100)")
print("   --val_samples: Number of validation samples (default: 10)\n")

# ========== argument parsing ==========
# ========== å‘½ä»¤è¡Œå‚æ•°ï¼šu å€¼ã€æœ€å¤§æ ·æœ¬æ•°ã€éªŒè¯å‚æ•° ==========
U_VALUE = int(sys.argv[1]) if len(sys.argv) > 1 and sys.argv[1].isdigit() else 16
MAX_SAMPLES = int(sys.argv[2]) if len(sys.argv) > 2 and sys.argv[2].isdigit() else 2000

# Validation arguments / éªŒè¯å‚æ•°
DO_VALIDATE = '--do_validate' in sys.argv
VAL_STEPS = 100  # Default / é»˜è®¤å€¼
VAL_SAMPLES = 10  # Default / é»˜è®¤å€¼

for i, arg in enumerate(sys.argv):
    if arg == '--val_steps' and i + 1 < len(sys.argv):
        VAL_STEPS = int(sys.argv[i + 1])
    elif arg == '--val_samples' and i + 1 < len(sys.argv):
        VAL_SAMPLES = int(sys.argv[i + 1])

print(f"\n{'='*60}")
print(f"ğŸ“‹ Training Configuration / è®­ç»ƒé…ç½®")
print(f"{'='*60}")
print(f"TinyLoRA u value / uå€¼: {U_VALUE}")
if MAX_SAMPLES is not None:
    print(f"Max training samples / æœ€å¤§è®­ç»ƒæ ·æœ¬æ•°: {MAX_SAMPLES}")
else:
    print(f"Max training samples / æœ€å¤§è®­ç»ƒæ ·æœ¬æ•°: unlimited")
print(f"Validation enabled / å¯ç”¨éªŒè¯: {DO_VALIDATE}")
if DO_VALIDATE:
    print(f"Validation frequency / éªŒè¯é¢‘ç‡: every {VAL_STEPS} steps / æ¯ {VAL_STEPS} æ­¥")
    print(f"Validation samples / éªŒè¯æ ·æœ¬æ•°: {VAL_SAMPLES}")
print(f"{'='*60}\n")

# ========== Dataset Configuration ==========
# ========== æ•°æ®é›†é…ç½® ==========
# Filter configuration for deepmind/code_contests dataset
# ç”¨äº deepmind/code_contests æ•°æ®é›†çš„è¿‡æ»¤é…ç½®
# source: The original source of the problem, with possible values including UNKNOWN_SOURCE (0),CODECHEF (1), CODEFORCES (2), HACKEREARTH (3), CODEJAM (4), ATCODER (5) and AIZU (6).
# difficulty: A representation of the difficulty of the problem with possible values including UNKNOWN_DIFFICULTY (0), EASY (1), MEDIUM (2), HARD (3), HARDER (4), HARDEST (5), EXTERNAL (6), A (7), B (8), C (9), D (10), E (11), F (12), G (13), H (14), I (15), J (16), K (17), L (18), M (19), N (20), O (21), P (22), Q (23), R (24), S (25), T (26), U (27) and V (28). Note that different sources use different, non-comparable gradings. For Codeforces problems, cf_rating is a more reliable measure of difficulty when available.
# Structure / ç»“æ„:
#   Key: source ID (integer) / é”®ï¼šæ•°æ®æº IDï¼ˆæ•´æ•°ï¼‰
#   Value: list of allowed difficulty IDs / å€¼ï¼šå…è®¸çš„éš¾åº¦ ID åˆ—è¡¨
#

# Difficulty mapping reference (ignored cf_rating for now to keep it simple):
# éš¾åº¦æ˜ å°„å‚è€ƒï¼ˆä¸ºç®€åŒ–èµ·è§æš‚æ—¶å¿½ç•¥ cf_ratingï¼‰ï¼š
#   Source 2 (Codeforces) & Source 5 (AtCoder):
#     7=A, 8=B, 9=C, 10=D, 11=E, 12=F, 13=G, 14=H...
#   Source 1 & Source 3 (Other platforms):
#     1=EASY, 2=MEDIUM, 3=HARD, 4=VERY_HARD...
#
DATASET_CONFIG = {
    2: [7, 8],      # Codeforces: A-B level (Introductory) / Codeforcesï¼šA-B çº§åˆ«ï¼ˆå…¥é—¨ï¼‰
    5: [7, 8],      # AtCoder: A-B level (Introductory) / AtCoderï¼šA-B çº§åˆ«ï¼ˆå…¥é—¨ï¼‰
    1: [1],         # General platforms: EASY only / é€šç”¨å¹³å°ï¼šä»…ç®€å•éš¾åº¦
    3: [1],         # General platforms: EASY only / é€šç”¨å¹³å°ï¼šä»…ç®€å•éš¾åº¦
}

# ========== Reward Scaling Configuration ==========
# ========== å¥–åŠ±ç¼©æ”¾é…ç½® ==========
# Hierarchical scaling: Source (1st) -> Difficulty (2nd)
# å±‚çº§ç¼©æ”¾ï¼šæ•°æ®æºï¼ˆç¬¬ä¸€å…³é”®å­—ï¼‰ -> éš¾åº¦ï¼ˆç¬¬äºŒå…³é”®å­—ï¼‰
# Note: These multipliers are applied to the base reward (0.5 for compile, up to 1.0 for pass)
# æ³¨æ„ï¼šè¿™äº›å€æ•°åº”ç”¨äºåŸºç¡€å¥–åŠ±ï¼ˆç¼–è¯‘æˆåŠŸ 0.5ï¼Œé€šè¿‡æ‰€æœ‰æµ‹è¯•æœ€é«˜ 1.0ï¼‰
REWARD_SCALING_CONFIG = {
    2: {          # Codeforces
        7: 1.0,   # A level: baseline / Açº§ï¼šåŸºå‡†
        8: 1.1,   # B level: slightly higher / Bçº§ï¼šç•¥é«˜
    },
    5: {          # AtCoder
        7: 1.0,   # A level: baseline / Açº§ï¼šåŸºå‡†
        8: 1.1,   # B level: slightly higher / Bçº§ï¼šç•¥é«˜
    },
    1: { 1: 1.0 }, # General platforms: EASY / é€šç”¨å¹³å°ï¼šç®€å•
    3: { 1: 1.0 }, # General platforms: EASY / é€šç”¨å¹³å°ï¼šç®€å•
}

# ========== Model Configuration ==========
# ========== æ¨¡å‹é…ç½® ==========
MS_MODEL_ID = "qwen/Qwen2.5-Coder-3B-Instruct"
LOCAL_MODEL_DIR = "./models/Qwen2.5-Coder-3B-Instruct"
OUTPUT_DIR = "./output/luoguqwencoder-lora"



#  Qwen2.5-Coder-3B-Instruct
# ========== ä¸‹è½½æ¨¡å‹ ==========
if not os.path.exists(LOCAL_MODEL_DIR):
    print(f"Download from ModelScope/ä»ModelScopeä¸‹è½½æ¨¡å‹ {MS_MODEL_ID} åˆ° {LOCAL_MODEL_DIR}...")
    snapshot_download(
        repo_id=MS_MODEL_ID,
        local_dir=LOCAL_MODEL_DIR,
    )
    print("æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
else:
    print(f"Load from local/æœ¬åœ°å·²å­˜åœ¨æ¨¡å‹ï¼Œç›´æ¥åŠ è½½ï¼š{LOCAL_MODEL_DIR}")

# ========== Load tokenizer =========
# ========== åŠ è½½ tokenizer ==========
tokenizer = AutoTokenizer.from_pretrained(
    LOCAL_MODEL_DIR,
    trust_remote_code=True,
)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"


# ========== Load model =====

# ========== åŠ è½½æ¨¡å‹ï¼ˆ4bit é‡åŒ–ï¼‰==========
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)
model = AutoModelForCausalLM.from_pretrained(
    LOCAL_MODEL_DIR,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    # torch_dtype=torch.bfloat16,
    dtype=torch.bfloat16,
)
model.config.use_cache = False

# å‡†å¤‡æ¨¡å‹è¿›è¡Œ k-bit è®­ç»ƒ
model = prepare_model_for_kbit_training(model)


# ========== Define TinyLoRA Layers ==========
# ========== å®šä¹‰ TinyLoRA å±‚ ==========
# Note: TinyLoRA classes are now imported from utils.py
# æ³¨æ„ï¼šTinyLoRA ç±»ç°åœ¨ä» utils.py å¯¼å…¥

# è·å–æ¨¡å‹ç¬¬ä¸€å±‚çš„è®¾å¤‡ (é€šå¸¸æ˜¯ cuda:0)
device = model.model.layers[0].self_attn.q_proj.weight.device
print(f"Model device/æ¨¡å‹ä¸»è®¾å¤‡: {device}")

# åˆ›å»ºå…¨å±€å‚æ•°å®¹å™¨
global_params = TinyLoRAGlobalParams(u_dim=U_VALUE, device=device, dtype=torch.bfloat16)

# ========== æ‰§è¡Œæ›¿æ¢ ==========
print("Start replacing/æ­£åœ¨åº”ç”¨ TinyLoRA Tiling (å‚æ•°å…±äº«)...")

print("It's normal to see many lines of 'replace'./çœ‹åˆ°å¾ˆå¤šæ›¿æ¢æ—¥å¿—æ˜¯æ­£å¸¸çš„ã€‚")
# ã€å…³é”®ã€‘å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿ P çŸ©é˜µå¯å¤ç°
# ä¿å­˜æ¨¡å‹æ—¶åªå­˜ v å‘é‡ï¼ŒåŠ è½½æ—¶éœ€è¦ç”¨ç›¸åŒç§å­é‡å»º P çŸ©é˜µ

TINYLORA_SEED = 212
torch.manual_seed(TINYLORA_SEED)
torch.cuda.manual_seed(TINYLORA_SEED)
print(f"âœ… Fix TinyLoRA seed/å·²å›ºå®š TinyLoRA éšæœºç§å­: {TINYLORA_SEED}")

# ã€å…³é”®ä¿®å¤ã€‘å…ˆå°† global_params æ³¨å†Œä¸ºæ¨¡å‹çš„å­æ¨¡å—
# è¿™æ ·åœ¨å±‚æ›¿æ¢æ—¶ï¼ŒTinyLoRALinear å°±èƒ½é€šè¿‡å¼•ç”¨è®¿é—®åˆ°å·²æ³¨å†Œçš„ global_v
model.tiny_lora_params = global_params
print(f"âœ… Register global_params to model/å·²å°† global_params æ³¨å†Œåˆ°æ¨¡å‹")

# ç„¶åå†è¿›è¡Œå±‚æ›¿æ¢ï¼Œä¼ å…¥ global_params å®¹å™¨æœ¬èº«
total_replaced = apply_tiny_lora(model, global_params)
print(f"âœ… Replace completed/æ›¿æ¢å®Œæˆï¼å…±æ›¿æ¢äº† {total_replaced} ä¸ªæ¨¡å—ã€‚")

# ========== å…³é”®æ­¥éª¤ï¼šå†»ç»“é™¤ v ä»¥å¤–çš„æ‰€æœ‰å‚æ•° ==========
print("Freezing parameters/æ­£åœ¨å†»ç»“æ¨¡å‹å‚æ•°...")

# ã€æ›´ä¼˜é›…çš„æ–¹æ¡ˆã€‘ç›´æ¥é€šè¿‡å¯¹è±¡å¼•ç”¨æ“ä½œï¼Œä¸ä¾èµ–å­—ç¬¦ä¸²åŒ¹é…
# 1. ç¬¬ä¸€æ­¥ï¼šå…¨å±€å†»ç»“æ‰€æœ‰å‚æ•°
model.requires_grad_(False)

# 2. ç¬¬äºŒæ­¥ï¼šç²¾å‡†è§£å†» global_v
# ç›´æ¥é€šè¿‡å¯¹è±¡å¼•ç”¨æ“ä½œï¼Œç»å¯¹ç¨³å¥
global_params.global_v.requires_grad = True
print(f"âœ… Trainable parameter/å¯è®­ç»ƒå‚æ•°: global_v, shape={global_params.global_v.shape}")

# éªŒè¯å¯è®­ç»ƒå‚æ•°
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
all_params = sum(p.numel() for p in model.parameters())
print(f"\nTotal parameters/æ€»å‚æ•°é‡: {all_params:,}")
print(f"Trainable parameters/å¯è®­ç»ƒå‚æ•°é‡: {trainable_params}")
if trainable_params != U_VALUE:
    raise RuntimeError(f"è­¦å‘Šï¼šå¯è®­ç»ƒå‚æ•°æ•°é‡ä¸º {trainable_params}ï¼Œé¢„æœŸä¸º {U_VALUE}ï¼")

# ========== Code Reward Function ==========

# ========== Code Reward Function ==========
# Note: compile_and_run is now imported from utils.py / æ³¨æ„ï¼šcompile_and_run ç°åœ¨ä» utils.py å¯¼å…¥

def code_reward_func(prompts, completions, public_tests=None, private_tests=None, generated_tests=None, source=None, difficulty=None, **kwargs):
    """
    GRPO reward function for code evaluation / GRPO çš„å¥–åŠ±å‡½æ•°
    
    For deepmind/code_contests dataset:
    - public_tests, private_tests, generated_tests: dicts with 'input' and 'output' as lists
    - We evaluate against public_tests, private_tests, and generated_tests
    
    å¯¹äº deepmind/code_contests æ•°æ®é›†ï¼š
    - public_tests, private_tests, generated_testsï¼šåŒ…å« 'input' å’Œ 'output' åˆ—è¡¨çš„å­—å…¸
    - å¯¹ public_tests, private_tests å’Œ generated_tests è¿›è¡Œè¯„ä¼°
    
    Reward rules / å¥–åŠ±è§„åˆ™ï¼š
    - Compile fail or invalid format: 0.0 / ç¼–è¯‘å¤±è´¥æˆ–æ— æ•ˆæ ¼å¼ï¼š0.0
    - Compile success, partial tests pass: 0.5-0.99 / ç¼–è¯‘æˆåŠŸï¼Œéƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼š0.5-0.99
    - All tests pass: 1.0 / æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼š1.0
    """
    rewards = []
    
    # Convert None to empty lists / å°† None è½¬æ¢ä¸ºç©ºåˆ—è¡¨
    if public_tests is None:
        public_tests = [None] * len(completions)
    if private_tests is None:
        private_tests = [None] * len(completions)
    if generated_tests is None:
        generated_tests = [None] * len(completions)
    if source is None:
        source = [0] * len(completions)
    if difficulty is None:
        difficulty = [0] * len(completions)
    
    # Iterate through each generated completion / éå†æ¯ä¸€æ¡ç”Ÿæˆçš„å›å¤
    for completion, pub_test, priv_test, gen_test, src, diff in zip(
        completions, public_tests, private_tests, generated_tests, source, difficulty
    ):
        # 1. Extract code using utility function / ä½¿ç”¨å·¥å…·å‡½æ•°æå–ä»£ç 
        code = extract_code_from_response(completion)
        
        if not code:
            rewards.append(0.0)  # No valid code found / æœªæ‰¾åˆ°æœ‰æ•ˆä»£ç 
            continue

        # 2. Parse test cases using utility function / ä½¿ç”¨å·¥å…·å‡½æ•°è§£ææµ‹è¯•ç”¨ä¾‹
        test_cases_list = []
        test_cases_list.extend(convert_hf_tests_to_list(pub_test))
        test_cases_list.extend(convert_hf_tests_to_list(priv_test))
        test_cases_list.extend(convert_hf_tests_to_list(gen_test))
        
        # If no test cases extracted, give penalty / å¦‚æœæ²¡æœ‰æå–åˆ°æµ‹è¯•ç”¨ä¾‹ï¼Œç»™äºˆæƒ©ç½š
        if not test_cases_list:
            rewards.append(0.0)
            continue
        
        # 3. Run code against all test cases / å¯¹æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹è¿è¡Œä»£ç 
        base_reward = compile_and_run(code, test_cases_list)
        
        # 4. REWARD SCALING - Adjust based on source and difficulty
        # å¥–åŠ±ç¼©æ”¾ - æ ¹æ®æºå’Œéš¾åº¦ä» REWARD_SCALING_CONFIG æŸ¥æ‰¾
        reward_multiplier = 1.0
        
        if src in REWARD_SCALING_CONFIG:
            source_scaling = REWARD_SCALING_CONFIG[src]
            if diff in source_scaling:
                reward_multiplier = source_scaling[diff]
            elif -1 in source_scaling:  # Fallback for unknown difficulty
                reward_multiplier = source_scaling[-1]
        
        final_reward = base_reward * reward_multiplier
        rewards.append(final_reward)
        
    return rewards
        
       

# ã€æœ€ç»ˆæ–¹æ¡ˆã€‘ç»•è¿‡ Trainer å¯¹çº¯é‡åŒ–æ¨¡å‹çš„æ£€æŸ¥
# Trainer çš„æ£€æŸ¥é€»è¾‘ (transformers/trainer.py):
#   _is_quantized_and_base_model = model.is_quantized AND NOT model._hf_peft_config_loaded
#   if _is_quantized_and_base_model and not isinstance(model, PeftModel): raise ValueError
#
# æˆ‘ä»¬çš„ TinyLoRA æ˜¯åˆæ³•çš„ adapterï¼ˆåªè®­ç»ƒ 16 ä¸ªå‚æ•°ï¼‰ï¼Œä½†ä¸æ˜¯æ ‡å‡† PeftModelã€‚
# è®¾ç½® _hf_peft_config_loaded = True è®©ç¬¬ä¸€é“æ£€æŸ¥ç›´æ¥ä¸º Falseï¼Œä¸ä¼šèµ°åˆ° isinstance åˆ¤æ–­ã€‚
# è¿™ä¸å½±å“å®é™…è®¡ç®—â€”â€”æƒé‡å·²ç»åœ¨å†…å­˜ä¸­é‡åŒ–ï¼ŒTinyLoRA å±‚æ­£ç¡®å¤„ç†äº†åé‡åŒ–ã€‚
model._hf_peft_config_loaded = True

print("âœ… Set _hf_peft_config_loaded=True / å·²è®¾ç½® _hf_peft_config_loaded=Trueï¼šbypass Trainer quantization check")

def filter_dataset(dataset, config, max_samples, seed=42):
    """
    Filter dataset based on source and difficulty configuration.
    æ ¹æ®æ•°æ®æºå’Œéš¾åº¦é…ç½®è¿‡æ»¤æ•°æ®é›†ã€‚
    
    Args:
        dataset: HuggingFace Dataset object / HuggingFace æ•°æ®é›†å¯¹è±¡
        config: Dict mapping source IDs to allowed difficulty lists / å°†æ•°æ®æº ID æ˜ å°„åˆ°å…è®¸çš„éš¾åº¦åˆ—è¡¨çš„å­—å…¸
        max_samples: Maximum number of samples after filtering / è¿‡æ»¤åçš„æœ€å¤§æ ·æœ¬æ•°
        seed: Random seed for shuffling / ç”¨äºæ‰“ä¹±çš„éšæœºç§å­
    
    Returns:
        Filtered and sampled dataset / è¿‡æ»¤å¹¶é‡‡æ ·åçš„æ•°æ®é›†
    """
    print("\n" + "="*60)
    print("ğŸ” Filtering dataset based on configuration...")
    print("ğŸ” æ ¹æ®é…ç½®è¿‡æ»¤æ•°æ®é›†...")
    print("="*60)
    
    # Log configuration / è®°å½•é…ç½®
    source_names = {
        1: "General Platform 1 / é€šç”¨å¹³å° 1",
        2: "Codeforces / Codeforces",
        3: "General Platform 3 / é€šç”¨å¹³å° 3",
        5: "AtCoder / AtCoder",
    }
    
    for source_id, difficulties in config.items():
        source_name = source_names.get(source_id, f"Source {source_id} / æ•°æ®æº {source_id}")
        print(f"ğŸ“Œ {source_name}: Keeping difficulties {difficulties} / ä¿ç•™éš¾åº¦ {difficulties}")
    
    # Filter function / è¿‡æ»¤å‡½æ•°
    def should_keep(example):
        source = example.get('source', -1)
        difficulty = example.get('difficulty', -1)
        
        # Check if source is in config / æ£€æŸ¥æ•°æ®æºæ˜¯å¦åœ¨é…ç½®ä¸­
        if source not in config:
            return False
        
        # Check if difficulty is allowed for this source / æ£€æŸ¥è¯¥æ•°æ®æºæ˜¯å¦å…è®¸æ­¤éš¾åº¦
        if difficulty not in config[source]:
            return False
        
        return True
    
    # Apply filter / åº”ç”¨è¿‡æ»¤
    print("\nâ³ Filtering in progress... / æ­£åœ¨è¿‡æ»¤...")
    original_size = len(dataset)
    filtered_dataset = dataset.filter(should_keep)
    filtered_size = len(filtered_dataset)
    
    print(f"âœ… Original dataset size / åŸå§‹æ•°æ®é›†å¤§å°: {original_size:,}")
    print(f"âœ… After filtering / è¿‡æ»¤å: {filtered_size:,} samples / æ ·æœ¬")
    print(f"ğŸ“Š Retention rate / ä¿ç•™ç‡: {filtered_size/original_size*100:.2f}%")
    
    # Apply max_samples limit with shuffling / åº”ç”¨æœ€å¤§æ ·æœ¬æ•°é™åˆ¶å¹¶æ‰“ä¹±
    if filtered_size > max_samples:
        print(f"\nğŸ² Shuffling and sampling {max_samples:,} from {filtered_size:,}...")
        print(f"ğŸ² æ‰“ä¹±å¹¶ä» {filtered_size:,} ä¸­é‡‡æ · {max_samples:,} ä¸ª...")
        filtered_dataset = filtered_dataset.shuffle(seed=seed).select(range(max_samples))
        final_size = len(filtered_dataset)
        print(f"âœ… Final training set size / æœ€ç»ˆè®­ç»ƒé›†å¤§å°: {final_size:,}")
    else:
        print(f"\nâœ… All {filtered_size:,} filtered samples will be used (below max_samples limit).")
        print(f"âœ… å°†ä½¿ç”¨å…¨éƒ¨ {filtered_size:,} ä¸ªè¿‡æ»¤åçš„æ ·æœ¬ï¼ˆä½äºæœ€å¤§æ ·æœ¬æ•°é™åˆ¶ï¼‰ã€‚")
        final_size = filtered_size
    
    print("="*60 + "\n")
    return filtered_dataset


# Note: apply_chat_template is now imported from tiny_lora_utils.py
# æ³¨æ„ï¼šapply_chat_template ç°åœ¨ä» tiny_lora_utils.py å¯¼å…¥



# ========== Load dataset ==========
# When using load_dataset("json", data_files="....jsonl"),
# HuggingFace defaults to classifying the file as 'train' split.
# Note: data_files points to processed file path from download_dataset.py
# split="train" is important! Trainer needs Dataset object, not DatasetDict
# 
# ========== åŠ è½½æ•°æ®é›† ==========

rl_dataset = load_dataset(
    "json", 
    data_files="./local_code_contests/code_contests_train.jsonl",
    split="train"
)

# Apply filtering based on source and difficulty configuration
# æ ¹æ®æ•°æ®æºå’Œéš¾åº¦é…ç½®åº”ç”¨è¿‡æ»¤
rl_dataset = filter_dataset(
    dataset=rl_dataset,
    config=DATASET_CONFIG,
    max_samples=MAX_SAMPLES,
    seed=TINYLORA_SEED
)



# Apply template / åº”ç”¨æ¨¡ç‰ˆ
rl_dataset = rl_dataset.map(lambda x: apply_chat_template(x, tokenizer))

# Print sample to verify / æ‰“å°ä¸€æ¡æ•°æ®éªŒè¯
print(f"âœ… Dataset loaded successfully! / æ•°æ®åŠ è½½æˆåŠŸï¼Total samples / æ ·æœ¬æ•°é‡: {len(rl_dataset)}")
print(f"ğŸ“ Sample data / æ ·ä¾‹æ•°æ®: {rl_dataset[0]}")

# ========== Load Validation Dataset (if enabled) ==========
# ========== åŠ è½½éªŒè¯æ•°æ®é›†ï¼ˆå¦‚æœå¯ç”¨ï¼‰==========
val_dataset = None
if DO_VALIDATE:
    print(f"\n{'='*60}")
    print(f"ğŸ“Š Loading validation dataset / æ­£åœ¨åŠ è½½éªŒè¯æ•°æ®é›†...")
    print(f"{'='*60}\n")
    
    val_dataset = load_dataset(
        "json",
        data_files="./local_code_contests/code_contests_valid.jsonl",
        split="train"
    )
    
    # Apply chat template to validation dataset / å¯¹éªŒè¯æ•°æ®é›†åº”ç”¨æ¨¡æ¿
    val_dataset = val_dataset.map(lambda x: apply_chat_template(x, tokenizer))
    
    print(f"âœ… Validation dataset loaded / éªŒè¯æ•°æ®é›†åŠ è½½æˆåŠŸ: {len(val_dataset)} samples / æ ·æœ¬\n")

# ========== Define Validation Callback ==========
# ========== å®šä¹‰éªŒè¯å›è°ƒ ==========
class ValidationCallback(TrainerCallback):
    """
    Custom callback for validation during training / è®­ç»ƒæœŸé—´éªŒè¯çš„è‡ªå®šä¹‰å›è°ƒ
    Tracks best model and saves checkpoint / è·Ÿè¸ªæœ€ä½³æ¨¡å‹å¹¶ä¿å­˜æ£€æŸ¥ç‚¹
    """
    def __init__(self, val_dataset, val_steps, val_samples, output_dir, global_params, u_value, seed, model_id, total_replaced):
        self.val_dataset = val_dataset
        self.val_steps = val_steps
        self.val_samples = val_samples
        self.output_dir = output_dir
        self.global_params = global_params
        self.u_value = u_value
        self.seed = seed
        self.model_id = model_id
        self.total_replaced = total_replaced
        self.best_score = 0.0
    
    def on_step_end(self, args, state, control, **kwargs):
        """Run validation at specified intervals / åœ¨æŒ‡å®šé—´éš”è¿è¡ŒéªŒè¯"""
        if state.global_step % self.val_steps == 0 and state.global_step > 0:
            print(f"\n{'='*80}")
            print(f"ğŸ” Running validation at step {state.global_step} / åœ¨ç¬¬ {state.global_step} æ­¥è¿è¡ŒéªŒè¯")
            print(f"{'='*80}\n")
            
            # Import validation function / å¯¼å…¥éªŒè¯å‡½æ•°
            from validate import run_validation
            
            # Get model and tokenizer from kwargs / ä» kwargs è·å–æ¨¡å‹å’Œåˆ†è¯å™¨
            model = kwargs.get('model')
            tokenizer = kwargs.get('processing_class') or kwargs.get('tokenizer')
            
            # Run validation / è¿è¡ŒéªŒè¯
            results = run_validation(
                model=model,
                tokenizer=tokenizer,
                dataset=self.val_dataset,
                num_samples=self.val_samples,
            )
            
            # Check if this is the best model / æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            current_score = results['pass_at_1']
            
            if current_score > self.best_score:
                self.best_score = current_score
                print(f"\n{'='*80}")
                print(f"ğŸ‰ New best model! / æ–°çš„æœ€ä½³æ¨¡å‹ï¼")
                print(f"   Previous best Pass@1 / ä¹‹å‰æœ€ä½³é€šè¿‡ç‡: {self.best_score:.2%}")
                print(f"   Current Pass@1 / å½“å‰é€šè¿‡ç‡: {current_score:.2%}")
                print(f"{'='*80}\n")
                
                # Save best model / ä¿å­˜æœ€ä½³æ¨¡å‹
                best_save_dict = {
                    "global_v": self.global_params.global_v.data.clone(),
                    "u_value": self.u_value,
                    "rank": 2,
                    "seed": self.seed,
                    "model_id": self.model_id,
                    "total_replaced_layers": self.total_replaced,
                    "validation_score": current_score,
                    "step": state.global_step,
                }
                
                best_path = f"{self.output_dir}/best_tiny_lora_v.pt"
                torch.save(best_save_dict, best_path)
                print(f"ğŸ’¾ Best model saved to / æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {best_path}")
                print(f"ğŸ“Š Validation Pass@1 / éªŒè¯é€šè¿‡ç‡: {current_score:.2%}\n")
            else:
                print(f"ğŸ“Š Current Pass@1: {current_score:.2%} (Best: {self.best_score:.2%})\n")
        
        return control

# Prepare callbacks / å‡†å¤‡å›è°ƒ
callbacks = []
if DO_VALIDATE and val_dataset is not None:
    validation_callback = ValidationCallback(
        val_dataset=val_dataset,
        val_steps=VAL_STEPS,
        val_samples=VAL_SAMPLES,
        output_dir=OUTPUT_DIR,
        global_params=global_params,
        u_value=U_VALUE,
        seed=TINYLORA_SEED,
        model_id=MS_MODEL_ID,
        total_replaced=total_replaced,
    )
    callbacks.append(validation_callback)
    print(f"âœ… Validation callback registered / éªŒè¯å›è°ƒå·²æ³¨å†Œ")
    print(f"   Validation frequency / éªŒè¯é¢‘ç‡: every {VAL_STEPS} steps / æ¯ {VAL_STEPS} æ­¥")
    print(f"   Validation samples / éªŒè¯æ ·æœ¬æ•°: {VAL_SAMPLES}\n")



# ========== Configure and start GRPO training ==========
# Configure GRPO / é…ç½® GRPO
training_args = GRPOConfig(
    output_dir=OUTPUT_DIR,
    num_train_epochs=1,
    per_device_train_batch_size=1,  # Set to 1 if GPU memory insufficient / æ˜¾å­˜ä¸è¶³è®¾ä¸º 1
    gradient_accumulation_steps=8,  # Accumulate to simulate large batch / ç´¯ç§¯æ¢¯åº¦æ¨¡æ‹Ÿå¤§ Batch
    learning_rate=1e-5,             # RL learning rate usually small / RL å­¦ä¹ ç‡é€šå¸¸è¦å°
    num_generations=4,              # Group Size (G): sample 4 answers per iteration / æ¯æ¬¡é‡‡æ · 4 ä¸ªç­”æ¡ˆ
    max_completion_length=1024,     # Max generation length / ç”Ÿæˆçš„æœ€å¤§é•¿åº¦
    logging_steps=1,
    bf16=True,                      # Enable BF16 acceleration / å¼€å¯ BF16 åŠ é€Ÿ
    save_strategy="no",             # Disable auto checkpoint (TinyLoRA is non-standard PEFT)
)

# Initialize trainer / åˆå§‹åŒ–è®­ç»ƒå™¨
trainer = GRPOTrainer(
    model=model,
    reward_funcs=code_reward_func,  # Your judge function / ä½ çš„åˆ¤é¢˜å‡½æ•°
    args=training_args,
    train_dataset=rl_dataset,       # Processed data / å¤„ç†å¥½çš„æ•°æ®
    processing_class=tokenizer,     # Tokenizer
    callbacks=callbacks,            # Add validation callback / æ·»åŠ éªŒè¯å›è°ƒ
)

# Start training! / å¼€å§‹è®­ç»ƒï¼
print("ğŸš€ Starting TinyLoRA-RL training... / å¼€å§‹ TinyLoRA-RL è®­ç»ƒ...")
trainer.train()

# Save training results / ä¿å­˜è®­ç»ƒç»“æœ
# Note: peft's save_pretrained may not recognize custom layers
# Manually save global_v and metadata needed to rebuild model
# æ³¨æ„ï¼špeft çš„ save_pretrained å¯èƒ½ä¸è®¤ä½ çš„è‡ªå®šä¹‰å±‚
# æ‰‹åŠ¨ä¿å­˜ global_v ä»¥åŠé‡å»ºæ¨¡å‹æ‰€éœ€çš„å…ƒä¿¡æ¯
os.makedirs(OUTPUT_DIR, exist_ok=True)

save_dict = {
    "global_v": global_params.global_v.data,  # Trained v vector / è®­ç»ƒå¥½çš„ v å‘é‡
    "u_value": U_VALUE,                        # Dimension of v / v çš„ç»´åº¦
    "rank": 2,                                 # TinyLoRA rank
    "seed": TINYLORA_SEED,                     # P matrix random seed (for reproducibility)
    "model_id": MS_MODEL_ID,                   # Base model ID / åŸºåº§æ¨¡å‹ ID
    "total_replaced_layers": total_replaced,   # Number of replaced layers / æ›¿æ¢çš„å±‚æ•°
}
torch.save(save_dict, f"{OUTPUT_DIR}/tiny_lora_v.pt")
print(f"âœ… Training complete! / è®­ç»ƒå®Œæˆï¼Parameters saved to / å‚æ•°å·²ä¿å­˜è‡³ {OUTPUT_DIR}/tiny_lora_v.pt")
print(f"ğŸ“Š Save contents / ä¿å­˜å†…å®¹: global_v (shape={global_params.global_v.shape}), u={U_VALUE}, rank=2, seed={TINYLORA_SEED}")