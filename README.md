# TinyLoRA-Qwen-Coder Experiment

<div align="center">

**å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¶…å‚æ•°å‹ç¼©æ¨¡å‹ï¼šQwen2.5-Coder-Instruct on CodeContests**<br>

**Version 2.5**

[ä¸­æ–‡ç‰ˆæœ¬](#ä¸­æ–‡ç‰ˆæœ¬) | [English Version](#english-version)

æœ¬é¡¹ç›®æ˜¯åœ¨ [Qwen4Luogu-RL](https://github.com/Chi-Shan0707/Qwen4Luogu-RL) çš„åŸºç¡€ä¸Šè¿›è¡Œçš„æ”¹è¿›ä¸å®éªŒã€‚

å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œæˆ–è€…ä½ è§‰å¾—æœ‰ç‚¹æ„æ€ï¼Œè¯·ç‚¹å‡»å³ä¸Šè§’çš„ Star æ”¯æŒä¸€ä¸‹ï¼è¿™å¯¹æˆ‘å¾ˆé‡è¦ï¼Œä¸‡åˆ†æ„Ÿè°¢PwPï¼<br>
If you find this project useful or interesting, please give it a Star! ğŸŒŸ Your support is my greatest motivation.<br>
</div>

---

## æ›´æ–°æ—¥å¿— / Changelog

### v2.5 â€” å…³é”® Bug ä¿®å¤ (Critical Bug Fixes)

æœ¬æ¬¡æ›´æ–°ä¿®å¤äº†å¯¼è‡´**è®­ç»ƒé›¶æ¢¯åº¦ã€æµ‹è¯•é›¶ä»£ç æå–**çš„ä¸‰ä¸ªå…³é”®ç¼ºé™·ï¼š

| # | Bug æè¿° | å½±å“ | ä¿®å¤ |
| :---: | :--- | :--- | :--- |
| **1** | `global_v` åˆå§‹åŒ–ä¸º `randn` è€Œé `zeros` | æ‰€æœ‰çº¿æ€§å±‚ä»ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­èµ·å°±å—åˆ°å·¨å¤§çš„éšæœºæ‰°åŠ¨ï¼ˆ$\Delta W$ é‡çº§ ~400ï¼‰ï¼Œå¯¼è‡´æ¨¡å‹è¾“å‡ºä¹±ç ï¼Œæ— æ³•æå–ä»£ç ï¼ŒGRPO å¥–åŠ±å…¨ä¸º 0ï¼Œæ¢¯åº¦ä¸º 0ï¼Œå‚æ•°æ°¸è¿œæ— æ³•æ›´æ–° | `utils.py`: `TinyLoRAGlobalParams.__init__` ä¸­æ”¹ä¸º `torch.zeros(...)` |
| **2** | è®­ç»ƒä¸æµ‹è¯•çš„éšæœºç§å­å¯¹é½é”™è¯¯ | è®­ç»ƒæ—¶ç§å­åœ¨ `apply_tiny_lora` å‰**ç´§é‚»**è®¾ç½®ï¼›æµ‹è¯•æ—¶ç§å­åœ¨**æ¨¡å‹åŠ è½½å‰**è®¾ç½®ï¼Œæ¨¡å‹åŠ è½½æ¶ˆè€—å¤§é‡éšæœºçŠ¶æ€ï¼Œå¯¼è‡´ P çŸ©é˜µä¸ä¸€è‡´ï¼Œå·²è®­ç»ƒçš„ `v` å‘é‡ä¸é”™è¯¯çš„ P çŸ©é˜µæ­é…äº§ç”Ÿé”™è¯¯çš„ $\Delta W$ | `test.py`: åœ¨ `apply_tiny_lora` è°ƒç”¨å‰é‡æ–°è®¾ç½®ç§å­ |
| **3** | P çŸ©é˜µç¼ºå°‘ $1/\sqrt{r}$ ç¼©æ”¾ | æ¢¯åº¦é‡çº§æ§åˆ¶ä¸ä½³ï¼Œè®ºæ–‡å»ºè®®ä½¿ç”¨ç¼©æ”¾å› å­ä»¥ç¨³å®šæ–¹å·® | `utils.py`: P çŸ©é˜µç”Ÿæˆæ—¶é™¤ä»¥ `rank ** 0.5` |

**ç—‡çŠ¶é“¾ï¼ˆv2.0 åŠä¹‹å‰ï¼‰ï¼š**
```
global_v ~ N(0,1) â†’ Î”W é‡çº§çˆ†ç‚¸ â†’ æ¨¡å‹è¾“å‡ºä¹±ç  â†’ æ— æ³•æå–ä»£ç 
â†’ reward å…¨ä¸º 0 â†’ GRPO advantage = 0/0 â†’ grad_norm = 0, loss = 0
â†’ v æ°¸ä¸æ›´æ–° â†’ ä¿å­˜çš„ checkpoint ä»ä¸ºéšæœºå€¼ â†’ æµ‹è¯•åŒæ ·å¤±è´¥
```

> **é‡è¦**ï¼šv2.5 ä¿®æ”¹äº† P çŸ©é˜µç¼©æ”¾å’Œ `global_v` åˆå§‹åŒ–æ–¹å¼ï¼Œæ—§ç‰ˆæœ¬çš„ checkpointï¼ˆ`.pt` æ–‡ä»¶ï¼‰**ä¸å…¼å®¹**ï¼Œéœ€è¦é‡æ–°è®­ç»ƒã€‚

### v2.0 â€” æ¨¡å—åŒ–é‡æ„ & éªŒè¯ç³»ç»Ÿ
- å°†å…±äº«å·¥å…·æå–åˆ° `utils.py`
- æ–°å¢ `validate.py` å’Œ `test.py`
- æ”¯æŒè®­ç»ƒä¸­éªŒè¯ä¸æœ€ä½³æ¨¡å‹è‡ªåŠ¨ä¿å­˜
- æ”¯æŒåŸºçº¿æµ‹è¯•ï¼ˆ`--baseline`ï¼‰

---

# ä¸­æ–‡ç‰ˆæœ¬

## TinyLoRA-Qwen-Coder å®éªŒ

æœ¬ä»“åº“æ˜¯åŸã€ŒLuoguQwen LoRA å¾®è°ƒã€ï¼Œä¸€ä¸ª[åŸºäº SFTçš„é¡¹ç›®](https://github.com/Chi-Shan0707/Qwen4Luogu-SFT)ä»¥åŠ[å¼ºåŒ–å­¦ä¹ é¡¹ç›®](https://github.com/Chi-Shan0707/Qwen4Luogu-RL)çš„è¿›é˜¶è¿›åŒ–ç‰ˆï¼š

*ä»¥ä¸‹æ˜¯SFTæ—¶çš„å¿ƒè·¯å†ç¨‹*<br>
> ä»€ä¹ˆï¼Œä½ é—®æˆ‘ä¸ºä»€ä¹ˆè¦æŒ‘é€‰ Qwen2.5-1.5B-Instruct è¿›è¡Œå¾®è°ƒï¼Ÿ<br>
> â€”â€” é‚£å½“ç„¶æ˜¯å› ä¸ºå®ƒå‚æ•°é‡å°å•¦ã€‚<br>
>
> ä»€ä¹ˆï¼Œä½ ç»§ç»­é—®æˆ‘ä¸ºä»€ä¹ˆä¸æŒ‘é€‰ Qwen2.5-Coder-1.5B-Instruct è¿›è¡Œå¾®è°ƒï¼Ÿ<br>
> ~~æˆ‘å¦‚æœåœ¨è¿™é˜¿é‡Œè¿›è¡Œè¿‡ä»£ç è®­ç»ƒä¸Šçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå“ªèƒ½çœ‹å¾—å‡ºæˆ‘å¾®è°ƒçš„æ•ˆæœï¼Ÿ~~<br>
> ~~å¥½å§ï¼Œå…¶å®æ˜¯æˆ‘é—®åƒé—®æœ‰ä»€ä¹ˆå‚æ•°é‡å°çš„æ¨¡å‹ï¼Œå®ƒæ¨èäº†è¿™ä¸ªï¼Œç„¶åæˆ‘ä¸€æ—¶é—´å¿˜è®°ç»§ç»­å»æœé›†ä¿¡æ¯ï¼Œç›´æ¥å¼€ææƒ¹ï¼Œç»“æœè®­ç»ƒåˆ°ä¸€åŠæ‰åœ¨ ModelScope ä¸Šåˆ·åˆ° Qwen2.5-Coder-1.5B-Instructã€‚PWP~~<br>
> ~~ç¬¬ä¸€éå®åœ¨å¤ªå·®äº†ï¼Œåæ­£è¿˜è¦å†è®­ç»ƒä¸€éï¼Œè¿˜æ˜¯å¼„ Qwen2.5-Coder-1.5B-Instruct å§~~<br>
> è¿™ä¸ªä¹Ÿå¤ªå·®åŠ²äº†ï¼Œä¸Š 7B å§ PwP<br>
> *ä¸å¯¹ï¼Œä¸ºä»€ä¹ˆç–¯ç‹‚æŠ¥ mismatch å•Šå•Šï¼Ÿä» 1.5Bâ†’7B æˆ‘å•¥éƒ½æ²¡æ”¹å•Šï¼Ÿ*<br>
> *ç–¯ç‹‚ debugï¼Œç–¯ç‹‚ç ”ç©¶æ ¼å¼â€¦â€¦*<br>
> ç®—äº†ï¼Œæ ¼å¼å¼„æˆæ‰€è°“çš„æ ‡å‡†å‹å§ã€‚<br>
> 7B æ ¹æœ¬è·‘ä¸åŠ¨å•Šï¼Œåªèƒ½ 3Bã€‚<br>
> ~~å•Šè®­ç»ƒå®Œäº†ï¼Œå‚æ•°æ ¹æœ¬ä¸Šä¼ ä¸åŠ¨å•Šï¼Ÿå•Šï¼Œhuggingface ä¹Ÿä¸Šä¼ ä¸åŠ¨å•Š PwP~~<br>

ç„¶åï¼Œ6å·æ™šä¸Šï¼Œ~~å¤©åŠ©æˆ‘ä¹Ÿ~~ï¼Œæˆ‘çœ‹åˆ°äº†TinyLoRAçš„è®ºæ–‡ï¼Œæ‰€ä»¥æˆ‘å°±å¼€å§‹äº†è¿™é¡¹å°è¯•ï¼ˆæˆ–è€…å¯ä»¥è¯´â€œå¤ç°â€ï¼‰ï¼Œæˆæœè§[Qwen4Luogu-RL](https://github.com/Chi-Shan0707/Qwen4Luogu-RL)ï¼š
- åŸºåº§ï¼šQwen2.5-Coder-3B-Instructï¼Œ4bit é‡åŒ–ä»¥æŒ¤çˆ†æœ€åä¸€ç‚¹æ˜¾å­˜ï¼›
- è®­ç»ƒï¼šä¸ç”¨ SFTï¼Œç”¨ RLï¼ˆGRPOï¼‰ï¼›
- æ•°æ®ï¼šLuoguä¸Šçš„é¢˜ç›®
- å‚æ•°ï¼šå…¨æ¨¡å‹åªä¿ç•™ **16 ä¸ªå¯è®­ç»ƒæ ‡é‡å‚æ•°**ï¼›
- ä»»åŠ¡ï¼šç”¨ã€Œç¼–è¯‘+è¿è¡Œ C++ ä»£ç ã€çš„æ–¹å¼åœ¨ CodeContests é¢˜ç›®ä¸Šæä»£ç å¼ºåŒ–å­¦ä¹ ã€‚
<br>

åœ¨[Qwen4Luogu-RL](https://github.com/Chi-Shan0707/Qwen4Luogu-RL)ä¸­ï¼Œè¿™ä¸ª`train_rl.py`æ˜¯å¯ä»¥è¿è¡Œä¸”è®­ç»ƒçš„ï¼Œä½†æ˜¯èƒ½æˆåŠŸè¿è¡Œ+é€šè¿‡æ ·ä¾‹æµ‹è¯•çš„ï¼Œåä¸å­˜ä¸€ï¼ˆå¹¶æ²¡æœ‰å¤¸å¼ ï¼‰ã€‚<br>
åŸå› å¯èƒ½æœ‰:
- æç¤ºè¯å†™çš„ä¸å¥½ï¼Œä¸‹ä¸€æ­¥éœ€è¦æ˜ç¡®ã€æ˜¯å¦è¦æ¨ç†è·¯å¾„ã€‘ç­‰ç»†èŠ‚ï¼Œå¹¶å¼€å±•Prompt Engineering
- tokenæ•°é‡æˆªå–çš„å¤ªå°‘ï¼Œç›®å‰æ˜¯1024ï¼Œä½†æ˜¯è¿™ä¸ªä¹Ÿä¼šå¸¦æ¥æˆæœ¬
- GRPOæ—¶ç”Ÿæˆç­”æ¡ˆæ•°é‡å¤ªå°‘
- è®­ç»ƒæ‰€ç”¨çš„luogué¢˜ç›®å¤ªéš¾
- RLçš„rewardå†™çš„ä¸å¤Ÿå¥½
- 3Bæ¨¡å‹æœ¬èº«èƒ½åŠ›ä¸è¡Œ<br>

<br>

æ‰€ä»¥åœ¨è¿™é‡Œï¼Œæˆ‘é‡‡ç”¨äº†å¦ä¸€ä¸ªæ•°æ®é›† [deepmind/code_contests](https://huggingface.co/datasets/deepmind/code_contests) ï¼Œå…¶å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

> - **é¢˜ç›®è§„æ¨¡æ›´å¤§**ï¼šæ‹¥æœ‰æµ·é‡çš„ç«èµ›çº§é¢˜ç›®ã€‚
> - **è‹±è¯­ç¯å¢ƒ**ï¼šé€‚é…ä¸»æµä»£ç æ¨¡å‹çš„è®­ç»ƒåå¥½ã€‚
> - **éš¾åº¦è°ƒæ§**ï¼šæ”¯æŒé¢˜ç›®éš¾åº¦çš„ç²¾ç»†åŒ–ç­›é€‰ã€‚
> - **æµ‹è¯•ç”¨ä¾‹æå…¶ä¸°å¯Œ**ï¼šæ˜¾è‘—æå‡æ¨¡å‹é€»è¾‘éªŒè¯çš„å‡†ç¡®æ€§ã€‚

---

## ç›®å½•

- [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
- [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)
- [è®ºæ–‡å¤ç°](#è®ºæ–‡å¤ç°)
- [æ ¸å¿ƒç‰¹ç‚¹](#æ ¸å¿ƒç‰¹ç‚¹)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æ•°æ®å‡†å¤‡ä¸æ ¼å¼](#æ•°æ®å‡†å¤‡ä¸æ ¼å¼)
- [è®­ç»ƒæµç¨‹ï¼ˆRL / GRPOï¼‰](#è®­ç»ƒæµç¨‹rl--grpo)
- [éªŒè¯ä¸æµ‹è¯•](#éªŒè¯ä¸æµ‹è¯•)
- [TinyLoRA Tiling æŠ€æœ¯ç»†èŠ‚](#tinylora-tiling-æŠ€æœ¯ç»†èŠ‚)
- [å¥–åŠ±å‡½æ•°ï¼šç¼–è¯‘è¿è¡Œ C++ ä»£ç ](#å¥–åŠ±å‡½æ•°ç¼–è¯‘è¿è¡Œ-c-ä»£ç )
- [èµ„æºæ¶ˆè€—ä¸æ³¨æ„äº‹é¡¹](#èµ„æºæ¶ˆè€—ä¸æ³¨æ„äº‹é¡¹)
- [å¼€æºä¸è®¸å¯è¯](#å¼€æºä¸è®¸å¯è¯)
- [å¼•ç”¨](#å¼•ç”¨)


---

## é¡¹ç›®æ¦‚è¿°

LuoguQwen-RL çš„ç›®æ ‡æ˜¯ï¼š

> åœ¨æ˜¾å­˜å—é™ï¼ˆ3B æ¨¡å‹ + 4bit é‡åŒ–ï¼‰ä¸”å‚æ•°æè‡´å‹ç¼©ï¼ˆä»… 16 ä¸ªå‚æ•°ï¼‰çš„å‰æä¸‹ï¼Œ
> é€šè¿‡å¼ºåŒ–å­¦ä¹ è®© Qwen2.5-Coder åœ¨ CodeContests ç«èµ›é¢˜ä¸Šå­¦ä¼šã€Œèƒ½è¿‡æ ·ä¾‹ã€çš„ C++ ä»£ç ç”Ÿæˆã€‚

æœ¬ä»“åº“å¹¶ä¸æ˜¯å‡­ç©ºè®¾è®¡çš„ï¼Œè€Œæ˜¯ä¸€ä¸ª**TinyLoRA è®ºæ–‡æ–¹å‘çš„å¤ç°ä¸å˜ä½“å®éªŒ**ï¼š

- `theory/README.md` ä¸­ç»™å‡ºäº† TinyLoRA / GRPO çš„ç†è®ºä¸å·¥ç¨‹ç»†èŠ‚æ¢³ç†ï¼›
- æœ¬é¡¹ç›®åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå°† TinyLoRA çš„æ€æƒ³ä»æ•°å­¦æ¨ç†ï¼ˆå¦‚ GSM8Kï¼‰è¿ç§»åˆ°**ä»£ç ç”Ÿæˆ + ç¼–è¯‘æ‰§è¡Œå¥–åŠ±**åœºæ™¯ï¼›
- è®ºæ–‡ä¸­ç»å…¸è®¾ç½®æ˜¯ 7B æ¨¡å‹ + 13 ä¸ªå‚æ•°ï¼Œæœ¬ä»“åº“ä½¿ç”¨ 3B Coder æ¨¡å‹ + 16 ä¸ªå‚æ•°ï¼Œä¿æŒã€Œæä½ç§© + å…¨å±€å…±äº«ã€è¿™ä¸€ç²¾ç¥å†…æ ¸ã€‚

æ ¸å¿ƒè„šæœ¬ï¼š

- `train_rl.py`ï¼š
  - åŠ è½½ 4bit é‡åŒ–çš„ `Qwen2.5-Coder-3B-Instruct`ï¼›
  - å°†æŒ‡å®š Linear å±‚æ›¿æ¢ä¸ºè‡ªå®šä¹‰ `TinyLoRALinear`ï¼Œå¹¶é€šè¿‡å…±äº«å‘é‡ `global_v` å®ç° TinyLoRA Tilingï¼›
  - ä½¿ç”¨ TRL çš„ `GRPOTrainer` è¿›è¡Œä»£ç å¼ºåŒ–å­¦ä¹ ï¼›
  - å¥–åŠ±æ¥è‡ªæœ¬åœ° `g++` ç¼–è¯‘ + æµ‹è¯•ç”¨ä¾‹æ‰§è¡Œé€šè¿‡ç‡ã€‚
- `download_dataset.py`ï¼š
  - ä» DeepMind çš„ `code_contests` æ•°æ®é›†ï¼ˆAlphaCodeï¼‰ä¸­æµå¼ä¸‹è½½ã€è¿‡æ»¤å¹¶ä¿å­˜ä¸ºæœ¬åœ° JSONL æ ¼å¼ã€‚
- `verify_pipeline.py`ï¼š
  - ç”¨äºéªŒè¯æ¨¡å‹åŠ è½½ã€ç”Ÿæˆã€ä»£ç æå–ä¸ç¼–è¯‘è¿è¡Œçš„ç«¯åˆ°ç«¯æµæ°´çº¿ï¼ˆç¤ºä¾‹ï¼šåŠ è½½æ¨¡å‹å¹¶å°è¯•ç”¨ç»™å®šæ ·ä¾‹å¯¹ç”Ÿæˆä»£ç è¿›è¡Œç¼–è¯‘è¿è¡Œè¯„æµ‹ï¼‰ã€‚

ç›®å½•ç»“æ„ï¼ˆèŠ‚é€‰ï¼‰ï¼š

- `train_rl.py`ï¼šä¸»è®­ç»ƒè„šæœ¬ï¼ˆTinyLoRA + GRPOï¼‰ã€‚
- `download_dataset.py`ï¼šæµå¼ä¸‹è½½å¹¶é¢„å¤„ç† CodeContests æ•°æ®ã€‚
- `verify_pipeline.py`ï¼šéªŒè¯ model->generate->extract->compile æµç¨‹çš„è„šæœ¬ã€‚
- `local_code_contests/`ï¼šæœ¬åœ°å­˜å‚¨çš„ CodeContests è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®ï¼ˆJSONL æ ¼å¼ï¼‰ã€‚
- `models/Qwen2.5-Coder-3B-Instruct/`ï¼šåŸºåº§æ¨¡å‹ç›®å½•ï¼ˆå¯é€šè¿‡ ModelScope è‡ªåŠ¨ä¸‹è½½ï¼‰ã€‚
- `output/`ï¼šRL è®­ç»ƒè¾“å‡ºç›®å½•ï¼ˆåŒ…æ‹¬æœ€ç»ˆçš„ `tiny_lora_v.pt`ï¼Œå†…å« `global_v` å‘é‡åŠé‡å»ºæ‰€éœ€çš„å…ƒä¿¡æ¯ï¼‰ã€‚

---
## é¡¹ç›®ç»“æ„

æœ¬é¡¹ç›®é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œå°†è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é€»è¾‘åˆ†ç¦»ï¼Œæé«˜ä»£ç å¯ç»´æŠ¤æ€§å’Œå¯é‡å¤æ€§ã€‚

### æ ¸å¿ƒæ¨¡å—

#### 1. `utils.py` - å…±äº«å·¥å…·æ¨¡å—

åŒ…å«æ‰€æœ‰è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•å…±äº«çš„æ ¸å¿ƒåŠŸèƒ½ï¼š

- **TinyLoRA ç±»**ï¼š
  - `TinyLoRAGlobalParams`: å…¨å±€å…±äº«å‘é‡å®¹å™¨
  - `TinyLoRALinear`: è‡ªå®šä¹‰ TinyLoRA çº¿æ€§å±‚
  - `apply_tiny_lora()`: å°† TinyLoRA å±‚æ³¨å…¥æ¨¡å‹

- **ä»£ç è¯„ä¼°åŠŸèƒ½**ï¼š
  - `compile_and_run()`: C++ ä»£ç ç¼–è¯‘å’Œè¿è¡Œ
  - `extract_code_from_response()`: ä»æ¨¡å‹å“åº”ä¸­æå–ä»£ç 
  - `convert_hf_tests_to_list()`: è½¬æ¢æµ‹è¯•ç”¨ä¾‹æ ¼å¼

- **æ¨¡å‹åŠ è½½å·¥å…·**ï¼š
  - `get_model_and_tokenizer()`: åŠ è½½ 4-bit é‡åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨

#### 2. `train_rl.py` - è®­ç»ƒè„šæœ¬

ä¸»è®­ç»ƒè„šæœ¬ï¼Œæ”¯æŒå¯é€‰çš„éªŒè¯åŠŸèƒ½ï¼š

```bash
# åŸºæœ¬è®­ç»ƒ / Basic training
python train_rl.py [u_value] [max_samples]

# å¸¦éªŒè¯çš„è®­ç»ƒ / Training with validation
python train_rl.py 16 2000 --do_validate --val_steps 100 --val_samples 10
```

**å‘½ä»¤è¡Œå‚æ•° / Command-line Arguments:**
- `u_value`: TinyLoRA å…±äº«å‘é‡ç»´åº¦ï¼ˆé»˜è®¤ 16ï¼‰
- `max_samples`: æœ€å¤§è®­ç»ƒæ ·æœ¬æ•°ï¼ˆé»˜è®¤ 2000ï¼‰
- `--do_validate`: å¯ç”¨è®­ç»ƒæœŸé—´éªŒè¯
- `--val_steps N`: æ¯ N æ­¥è¿›è¡Œä¸€æ¬¡éªŒè¯ï¼ˆé»˜è®¤ 100ï¼‰
- `--val_samples N`: éªŒè¯æ ·æœ¬æ•°ï¼ˆé»˜è®¤ 10ï¼‰

**éªŒè¯åŠŸèƒ½**ï¼š
- è®­ç»ƒæœŸé—´è‡ªåŠ¨è¿è¡ŒéªŒè¯
- è·Ÿè¸ªæœ€ä½³ Pass@1 åˆ†æ•°
- è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹è‡³ `best_tiny_lora_v.pt`

#### 3. `validate.py` - éªŒè¯è„šæœ¬

å¯ä»¥ä½œä¸ºç‹¬ç«‹è„šæœ¬è¿è¡Œï¼Œä¹Ÿå¯ä»¥è¢« `train_rl.py` å¯¼å…¥ï¼š

```bash
# ç‹¬ç«‹éªŒè¯ / Standalone validation
python validate.py [num_samples]
```

**åŠŸèƒ½**ï¼š
- åŠ è½½è®­ç»ƒå¥½çš„æ£€æŸ¥ç‚¹
- åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹
- è®¡ç®— Pass@1ã€ç¼–è¯‘æˆåŠŸç‡ç­‰æŒ‡æ ‡

#### 4. `test.py` - æµ‹è¯•è„šæœ¬

ç”¨äºåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹ï¼š

```bash
# åŸºæœ¬æµ‹è¯• - æµ‹è¯• TinyLoRA å¾®è°ƒæ¨¡å‹ï¼ˆä½¿ç”¨å‘½åå‚æ•°ï¼‰
python test.py --checkpoint_path <path> --num_samples <N>

# åŸºçº¿æµ‹è¯• - æµ‹è¯•åŸå§‹åŸºåº§æ¨¡å‹ï¼ˆä¸å« TinyLoRAï¼Œä½¿ç”¨å‘½åå‚æ•°ï¼‰
python test.py --baseline --num_samples <N>

# ç¤ºä¾‹ / Examples
python test.py --checkpoint_path ./output/luoguqwencoder-lora/tiny_lora_v.pt --num_samples 50      # æµ‹è¯•å¾®è°ƒæ¨¡å‹
python test.py --checkpoint_path ./output/luoguqwencoder-lora/best_tiny_lora_v.pt --num_samples 100 # æµ‹è¯•æœ€ä½³æ¨¡å‹
python test.py --baseline --num_samples 50                                                      # æµ‹è¯•åŸºåº§æ¨¡å‹ï¼ˆå¯¹æ¯”ï¼‰
python test.py --checkpoint_path ./output/luoguqwencoder-lora/tiny_lora_v.pt --num_samples 50 --test_data ./local_code_contests/code_contests_test.jsonl
```

**åŠŸèƒ½**ï¼š
- **TinyLoRA æ¨¡å¼**ï¼ˆé»˜è®¤ï¼‰ï¼šä» `.pt` æ£€æŸ¥ç‚¹åŠ è½½å…ƒæ•°æ®ï¼ˆseed, u_value, rankï¼‰
  - è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ P çŸ©é˜µä¸€è‡´
  - åŠ è½½åŸºåº§æ¨¡å‹å¹¶æ³¨å…¥ TinyLoRA
  - åŠ è½½è®­ç»ƒæƒé‡ `global_v`
- **åŸºçº¿æ¨¡å¼**ï¼ˆ`--baseline`ï¼‰ï¼šç›´æ¥è¿è¡ŒåŸå§‹åŸºåº§æ¨¡å‹ï¼Œç”¨äºå¯¹æ¯”å¾®è°ƒæ•ˆæœ
  - ä¸åŠ è½½æ£€æŸ¥ç‚¹
  - ä¸æ³¨å…¥ TinyLoRA
  - å¯è§†åŒ–å¾®è°ƒçš„æ€§èƒ½æå‡
- åœ¨æµ‹è¯•é›†ä¸Šè¿è¡Œè¯„ä¼°

#### 5. `download_dataset.py` - æ•°æ®é›†ä¸‹è½½è„šæœ¬

ä» HuggingFace æµå¼ä¸‹è½½ CodeContests æ•°æ®é›†ï¼š

```bash
python download_dataset.py
```

### å·¥ä½œæµç¨‹

```mermaid
graph LR
    A[ä¸‹è½½æ•°æ®é›†] --> B[è®­ç»ƒæ¨¡å‹]
    B --> C[éªŒè¯ï¼ˆå¯é€‰ï¼‰]
    C --> D[ä¿å­˜æœ€ä½³æ¨¡å‹]
    D --> E[æµ‹è¯•è¯„ä¼°]
    
    style A fill:#e1f5ff
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#fce4ec
```

1. **æ•°æ®å‡†å¤‡**: è¿è¡Œ `download_dataset.py` ä¸‹è½½å¹¶é¢„å¤„ç†æ•°æ®
2. **æ¨¡å‹è®­ç»ƒ**: è¿è¡Œ `train_rl.py` è¿›è¡Œ RL è®­ç»ƒï¼ˆå¯é€‰å¸¦éªŒè¯ï¼‰
3. **è®­ç»ƒä¸­éªŒè¯**: å¦‚æœå¯ç”¨ `--do_validate`ï¼Œä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸéªŒè¯
4. **ä¿å­˜æœ€ä½³æ¨¡å‹**: éªŒè¯åˆ†æ•°æé«˜æ—¶è‡ªåŠ¨ä¿å­˜ `best_tiny_lora_v.pt`
5. **æœ€ç»ˆæµ‹è¯•**: è¿è¡Œ `test.py` åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹

### æ–‡ä»¶ä¾èµ–å…³ç³»

```
utils.py (åŸºç¡€å·¥å…·)
  â”‚
  â”œâ”€â”€ train_rl.py (å¯¼å…¥å¹¶ä½¿ç”¨)
  â”‚   â””â”€â”€ è°ƒç”¨ validate.py
  â”‚
  â”œâ”€â”€ validate.py (å¯¼å…¥å¹¶ä½¿ç”¨)
  â”‚
  â””â”€â”€ test.py (å¯¼å…¥å¹¶ä½¿ç”¨)
```

æ‰€æœ‰è„šæœ¬éƒ½ä¾èµ– `utils.py` ä¸­çš„å…±äº«åŠŸèƒ½ï¼Œç¡®ä¿ä»£ç ä¸€è‡´æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚

---
## è®ºæ–‡å¤ç°

[cite_start]æœ¬é¡¹ç›®æ˜¯è®ºæ–‡ **"Learning to Reason in 13 Parameters" (Morris et al., 2026)** çš„éå®˜æ–¹å¤ç°ä¸å·¥ç¨‹é€‚é… [cite: 2]ã€‚

### 1. æ ¸å¿ƒç†è®ºï¼šTinyLoRA
åŸè®ºæ–‡æå‡ºäº†ä¸€ç§æç«¯çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³• **TinyLoRA**ï¼Œæ—¨åœ¨æ‰“ç ´ LoRA çš„ç§©ï¼ˆRankï¼‰é™åˆ¶ã€‚
- [cite_start]**ç—›ç‚¹**ï¼šä¼ ç»Ÿ LoRA å³ä½¿ Rank=1ï¼Œå…¶å‚æ•°é‡ä»ä¸æ¨¡å‹å®½åº¦ $d$ æˆæ­£æ¯”ï¼ˆ$O(d \times r)$ï¼‰ï¼Œå¯¹äº 7B æ¨¡å‹çº¦ä¸ºæ•°ç™¾ä¸‡å‚æ•° [cite: 17, 158]ã€‚
- [cite_start]**åˆ›æ–°**ï¼šTinyLoRA åˆ©ç”¨ SVD å†»ç»“åŸæƒé‡çš„ç‰¹å¾æ–¹å‘ ($U, V$)ï¼Œä»…å­¦ä¹ ä¸€ä¸ªæå°çš„å‘é‡ $v$ã€‚é€šè¿‡åœ¨ä¸åŒå±‚ä¹‹é—´å…±äº«è¿™ä¸ªå‘é‡ï¼ˆ**Tiling**ï¼‰ï¼Œå¯å°†å…¨ç½‘å¯è®­ç»ƒå‚æ•°å‹ç¼©è‡³ä¸ªä½æ•° [cite: 7, 175, 181]ã€‚
- **å…¬å¼**ï¼š
  $$W' = W + U \Sigma (\sum_{i=1}^{u} v_i P_i) V^\top$$
  [cite_start]å…¶ä¸­ $U, \Sigma, V$ æ¥è‡ªåŸæƒé‡çš„ SVD åˆ†è§£ï¼ˆå†»ç»“ï¼‰ï¼Œ$P$ æ˜¯å›ºå®šéšæœºæŠ•å½±ï¼Œ$v$ æ˜¯å”¯ä¸€çš„å¯è®­ç»ƒå‚æ•° [cite: 173, 174]ã€‚

### 2. ä¸ºä»€ä¹ˆå¿…é¡»æ˜¯ RLï¼Ÿ
[cite_start]è®ºæ–‡çš„æ ¸å¿ƒå‘ç°æ˜¯ï¼š**åœ¨å¦‚æ­¤æç«¯çš„å‚æ•°é™åˆ¶ä¸‹ï¼ˆ<100 å‚æ•°ï¼‰ï¼ŒSFTï¼ˆç›‘ç£å¾®è°ƒï¼‰å‡ ä¹å®Œå…¨å¤±æ•ˆï¼Œåªæœ‰ RLï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰èƒ½å¥æ•ˆ** [cite: 10, 65]ã€‚
- [cite_start]**SFT çš„å±€é™**ï¼šSFT å¼ºè¿«æ¨¡å‹è®°å¿†å‚è€ƒç­”æ¡ˆçš„æ ¼å¼å’Œé£æ ¼ï¼ˆ"Noise"ï¼‰ï¼Œè¿™éœ€è¦è¾ƒå¤§çš„å®¹é‡ [cite: 147, 148]ã€‚
- [cite_start]**RL çš„ä¼˜åŠ¿**ï¼šRL ä»…å…³æ³¨æœ€ç»ˆç»“æœçš„å¯¹é”™ï¼ˆ"Signal"ï¼‰ï¼Œå…è®¸æ¨¡å‹å¿½ç•¥æ— å…³ç»†èŠ‚ã€‚TinyLoRA æ­£æ˜¯åˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œåœ¨ä»…æœ‰ 13 ä¸ªå‚æ•°çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ GRPO ç®—æ³•åœ¨ GSM8K ä¸Šè¾¾åˆ°äº† 91% çš„å‡†ç¡®ç‡ [cite: 64, 149]ã€‚

### 3. æœ¬é¡¹ç›®çš„â€œé­”æ”¹â€é€‚é…
æˆ‘ä»¬éµå¾ªè®ºæ–‡çš„ç²¾ç¥å†…æ ¸ï¼Œä½†é’ˆå¯¹**ä»£ç ç”Ÿæˆä»»åŠ¡**å’Œ**æ¶ˆè´¹çº§æ˜¾å¡**è¿›è¡Œäº†é€‚é…ï¼š

| ç‰¹æ€§ | åŸè®ºæ–‡è®¾ç½® (Paper) | æœ¬é¡¹ç›®é€‚é… (Ours) |
| :--- | :--- | :--- |
| **ä»»åŠ¡é¢†åŸŸ** | [cite_start]æ•°å­¦æ¨ç† (GSM8K, MATH) [cite: 8] | **ä»£ç ç«èµ› (CodeContests / AlphaCode)** |
| **åŸºåº§æ¨¡å‹** | [cite_start]Qwen2.5-7B / Llama-3 [cite: 64] | **Qwen2.5-Coder-3B-Instruct** |
| **å‚æ•°é‡** | 13 å‚æ•° ($u=13$) | **16 å‚æ•° ($u=16$)** ï¼ˆå¯è°ƒï¼‰|
| **ç²¾åº¦å¤„ç†** | [cite_start]BF16 / FP32 [cite: 8] | **4-bit é‡åŒ– (NF4) + åŠ¨æ€åé‡åŒ– SVD** |
| **å¥–åŠ±æœºåˆ¶** | ç­”æ¡ˆåŒ¹é… (Exact Match) | **g++ ç¼–è¯‘ + æµ‹è¯•ç”¨ä¾‹è¿è¡Œ (RLVR)** |
| **æ˜¾å­˜ä¼˜åŒ–** | éœ€é«˜æ˜¾å­˜ (A100/H100) | **é€‚é…å•å¡æ¶ˆè´¹çº§ GPU (16GB+)** |

> **å…³é”®å·¥ç¨‹æŒ‘æˆ˜**ï¼šåŸè®ºæ–‡æœªæ¶‰åŠ 4-bit é‡åŒ–æ¨¡å‹ã€‚æœ¬é¡¹ç›®é¢å¤–å®ç°äº†åœ¨åˆå§‹åŒ–é˜¶æ®µå¯¹ 4-bit æƒé‡è¿›è¡Œ `dequantize` è§£åŒ…ï¼Œåœ¨ CPU ä¸Šå®Œæˆ FP32 ç²¾åº¦çš„ SVD åˆ†è§£ï¼Œå†è½¬å› BF16 æ³¨å†Œä¸º Buffer çš„æµç¨‹ï¼Œä»è€Œåœ¨ä½æ˜¾å­˜ç¯å¢ƒä¸‹å®ç°äº† TinyLoRA åˆå§‹åŒ–ã€‚

## æ ¸å¿ƒç‰¹ç‚¹

- **æè‡´å‚æ•°å‹ç¼©**ï¼š
  - æ•´ä¸ªæ¨¡å‹çš„å¯è®­ç»ƒå‚æ•°åªæœ‰ä¸€ä¸ªå‘é‡ `global_v âˆˆ R^{16}`ï¼›
  - å…¨ç½‘æ‰€æœ‰è¢«æ›¿æ¢çš„ Linear å±‚éƒ½å…±äº«è¿™ 16 ä¸ªæ ‡é‡ï¼›
  - ä½ å¯ä»¥é€šè¿‡è¿è¡Œ `train_rl.py` æˆ– `verify_pipeline.py` æ¥æŸ¥çœ‹æ¨¡å‹å‚æ•°ä¿¡æ¯ï¼ˆæ€»å‚æ•°é‡ / å¯è®­ç»ƒå‚æ•°é‡ / å‹ç¼©ç‡ï¼‰ã€‚

- **TinyLoRA Tiling**ï¼š
  - å¯¹åŸå§‹ Linear æƒé‡ï¼ˆåŒ…æ‹¬ 4bit é‡åŒ–æƒé‡ï¼‰åš SVD åˆ†è§£ï¼Œå¾—åˆ°å›ºå®šçš„éª¨æ¶ `U, S, Vh`ï¼›
  - å†é€šè¿‡éšæœºçŸ©é˜µç°‡ `P âˆˆ R^{uÃ—rÃ—r}` ä¸å…±äº«å‘é‡ `v âˆˆ R^u` é‡æ„ä¸€ä¸ªä½ç§©å¢é‡ï¼›
  - åªè®­ç»ƒ `v`ï¼Œå®ç°è®ºæ–‡ä¸­çš„ Tiling / å…¨å‚æ•°å…±äº«ã€‚

- **çœŸå®ä»£ç ç¯å¢ƒå¥–åŠ±**ï¼š
  - æŠŠæ¨¡å‹ç”Ÿæˆçš„ C++ ä»£ç å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼›
  - ä½¿ç”¨ç³»ç»Ÿ `g++` ç¼–è¯‘ï¼›
  - ä¸‰æ¡£ç¦»æ•£ rewardï¼šç¼–è¯‘å¤±è´¥=0ï¼Œç¼–è¯‘æˆåŠŸä½†æ ·ä¾‹é”™è¯¯=0.5ï¼Œé€šè¿‡æ ·ä¾‹=1.0ï¼›
  - ä»£ç ä¸é€šè¿‡ç¼–è¯‘ / è¶…æ—¶ / è¿è¡Œé”™è¯¯ -> reward ç›´æ¥è¶‹è¿‘äº 0ã€‚

- **æ˜¾å­˜å‹å¥½**ï¼š
  - åŸºåº§ä¸º 3B Coder æ¨¡å‹ï¼Œç»“åˆ bitsandbytes 4bit é‡åŒ– + BF16 è®¡ç®—ï¼›
  - åœ¨å•å¡æœ‰é™æ˜¾å­˜ç¯å¢ƒä¸‹ä¹Ÿèƒ½è·‘å®Œæ•´çš„ RL loopï¼ˆå½“ç„¶ï¼Œä¼šæ¯”è¾ƒæ…¢ï¼‰ã€‚

---

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

å»ºè®®ä½¿ç”¨ Linux + Python 3.10 åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œå¹¶ç¡®ä¿å·²å®‰è£… `g++` ç¼–è¯‘å™¨ã€‚

```bash
python -m venv .venv
source .venv/bin/activate

pip install -r requirements.txt
```

> æç¤ºï¼š`requirements.txt` ä¸­å·²åŒ…å« `torch`ã€`transformers`ã€`datasets`ã€`trl`ã€`peft`ã€`bitsandbytes`ã€`modelscope` ç­‰ä¾èµ–ã€‚

### 2. ä¸‹è½½åŸºåº§æ¨¡å‹

`train_rl.py` ä¼šåœ¨æœ¬åœ°ä¸å­˜åœ¨æ¨¡å‹æ—¶ï¼Œè‡ªåŠ¨é€šè¿‡ ModelScope ä¸‹è½½ï¼š

- æ¨¡å‹ IDï¼š`qwen/Qwen2.5-Coder-3B-Instruct`
- é»˜è®¤æœ¬åœ°è·¯å¾„ï¼š`./models/Qwen2.5-Coder-3B-Instruct`

ä½ ä¹Ÿå¯ä»¥æ˜¾å¼è°ƒç”¨ï¼š

```python
from modelscope.hub.snapshot_download import snapshot_download

snapshot_download(
    repo_id="qwen/Qwen2.5-Coder-3B-Instruct",
    local_dir="./models/Qwen2.5-Coder-3B-Instruct",
)
```

### 3. å‡†å¤‡ CodeContests æ•°æ®

è¿è¡Œä»¥ä¸‹è„šæœ¬ä» Hugging Face æµå¼ä¸‹è½½å¹¶é¢„å¤„ç†æ•°æ®ï¼š

```bash
python download_dataset.py
```

`download_dataset.py` ä¼šï¼š
- ä» DeepMind çš„ `code_contests` æ•°æ®é›†ä¸­æµå¼è¯»å–æ•°æ®ï¼›
- ä»…ä¿ç•™ `description`, `public_tests` ç­‰æ ¸å¿ƒå­—æ®µä»¥èŠ‚çœç©ºé—´ï¼›
- å°†æ•°æ®ä¿å­˜åˆ° `./local_code_contests/` ç›®å½•ä¸‹çš„ JSONL æ–‡ä»¶ä¸­ã€‚

### 4. éªŒè¯æµæ°´çº¿

åœ¨çœŸæ­£è®­ç»ƒå‰ï¼Œå¯æ‰§è¡Œä»¥ä¸‹è„šæœ¬æ£€æŸ¥ç¯å¢ƒæ˜¯å¦å‡†å¤‡å°±ç»ªï¼š

```bash
python verify_pipeline.py
```

`verify_pipeline.py` ä¼šåŠ è½½æ¨¡å‹ï¼Œå¹¶å°è¯•å¯¹é¢„è®¾çš„æµ‹è¯•ç”¨ä¾‹è¿›è¡Œç”Ÿæˆã€æå–ä¸ç¼–è¯‘è¿è¡Œã€‚

### 5. å¯åŠ¨ RL è®­ç»ƒ

åŸºç¡€ç”¨æ³•ï¼ˆä½¿ç”¨é»˜è®¤u=16ï¼Œè®­ç»ƒå…¨éƒ¨æ•°æ®ï¼‰ï¼š

```bash
python train_rl.py
```

è‡ªå®šä¹‰ TinyLoRA å‚æ•°æ•°é‡ï¼ˆu å€¼ï¼‰ï¼š

```bash
python train_rl.py 32     # ä½¿ç”¨ u=32ï¼ˆ32 ä¸ªå¯è®­ç»ƒå‚æ•°ï¼‰
python train_rl.py 8      # ä½¿ç”¨ u=8ï¼ˆ8 ä¸ªå¯è®­ç»ƒå‚æ•°ï¼‰
```

é™åˆ¶è®­ç»ƒæ ·æœ¬æ•°é‡ï¼š

```bash
python train_rl.py 16 100      # u=16ï¼Œä»…è®­ç»ƒå‰ 100 ä¸ªæ ·æœ¬
python train_rl.py 32 50       # u=32ï¼Œä»…è®­ç»ƒå‰ 50 ä¸ªæ ·æœ¬
python train_rl.py 16          # u=16ï¼Œè®­ç»ƒå…¨éƒ¨æ ·æœ¬ï¼ˆç¬¬äºŒä¸ªå‚æ•°å¯çœç•¥ï¼‰
```

> **å‚æ•°è¯´æ˜**ï¼š
> - **ç¬¬ä¸€ä¸ªå‚æ•°** `u`ï¼šTinyLoRA ä¸­å…±äº«å‘é‡ `global_v` çš„ç»´åº¦ï¼Œå³å¯è®­ç»ƒå‚æ•°çš„æ€»æ•°ã€‚è‹¥ä¸æä¾›ï¼Œåˆ™é»˜è®¤ä½¿ç”¨ `u=16`ã€‚
> - **ç¬¬äºŒä¸ªå‚æ•°** `MAX_SAMPLES`ï¼šæœ€å¤šè®­ç»ƒçš„æ ·æœ¬æ•°é‡ã€‚è‹¥ä¸æä¾›ï¼Œåˆ™ä½¿ç”¨å…¨éƒ¨æ•°æ®é›†ã€‚è¿™ä¸ªå‚æ•°åœ¨å¿«é€Ÿå®éªŒã€è°ƒè¯•è¶…å‚æ•°æˆ–æ˜¾å­˜ä¸è¶³æ—¶éå¸¸æœ‰ç”¨ã€‚ ç›®å‰æ˜¯shuffleåå–æ•°æ®é›†ï¼ˆåŸæ•°æ®é›†æ¯é“é¢˜ç›®ä¼šå‡ºç°2æ¬¡ï¼‰ï¼Œæ•…å¦‚æ­¤ã€‚ï¼ˆç›®å‰ç§å­æ˜¯å…¨å±€çš„TINYLORA_SEEDã€‚ï¼‰

`train_rl.py` å°†ä¼šï¼š

1. ç¡®ä¿åŸºåº§æ¨¡å‹å·²å‡†å¤‡å¥½ï¼ˆå¿…è¦æ—¶è‡ªåŠ¨ä¸‹è½½ï¼‰ï¼›
2. ä»¥ 4bit é‡åŒ–æ–¹å¼åŠ è½½ `Qwen2.5-Coder-3B-Instruct`ï¼›
3. æ ¹æ®å‘½ä»¤è¡Œå‚æ•°åˆ›å»º u ç»´çš„å…±äº«å‘é‡ï¼›
4. æ³¨å…¥ TinyLoRA Tilingï¼ˆå…¨å±€å…±äº« `global_v`ï¼‰ï¼›
5. ä» `./local_code_contests/code_contests_train.jsonl` è¯»å– RL æ•°æ®ï¼›
6. è‹¥æŒ‡å®šäº† `MAX_SAMPLES`ï¼Œåˆ™ä»…é€‰å–å‰ N ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒï¼›
7. ä½¿ç”¨ `GRPOTrainer` è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼›
8. è®­ç»ƒå®Œæˆåï¼Œå°†è®­ç»ƒç»“æœä¿å­˜ä¸º `output/tiny_lora_v.pt`ã€‚

**ä¿å­˜å†…å®¹**ï¼š`tiny_lora_v.pt` æ˜¯ä¸€ä¸ª dictï¼ŒåŒ…å«è¿˜åŸæ¨¡å‹æ‰€éœ€çš„å…¨éƒ¨ä¿¡æ¯ï¼š

```python
{
    "global_v": tensor([...]),     # è®­ç»ƒå¥½çš„ v å‘é‡ï¼Œshape=(u,)
    "u_value": 32,                 # v çš„ç»´åº¦
    "rank": 2,                     # TinyLoRA çš„ rank
    "seed": 42,                    # P çŸ©é˜µçš„éšæœºç§å­ï¼ˆç”¨äºå¤ç°ï¼‰
    "model_id": "qwen/Qwen2.5-Coder-3B-Instruct",  # åŸºåº§æ¨¡å‹ ID
    "total_replaced_layers": 252,  # æ›¿æ¢çš„å±‚æ•°
}
```

> **è¿˜åŸæ–¹å¼**ï¼šåŠ è½½åŸºåº§æ¨¡å‹ â†’ ç”¨ç›¸åŒ `seed` å›ºå®šéšæœºç§å­ â†’ ç”¨ç›¸åŒ `u_value` å’Œ `rank` æ‰§è¡Œ `apply_tiny_lora` â†’ å°† `global_v` åŠ è½½å› `global_params.global_v`ã€‚ç§å­ç›¸åŒä¿è¯ P çŸ©é˜µå®Œå…¨ä¸€è‡´ï¼ŒSVD æ˜¯ç¡®å®šæ€§è¿ç®—æ‰€ä»¥ U/S/Vh ä¹Ÿä¸€è‡´ã€‚
>
> **â— v2.5 å…³é”®æç¤º**ï¼šç§å­å¿…é¡»åœ¨ `apply_tiny_lora` è°ƒç”¨**ç´§å‰**è®¾ç½®ï¼Œ**ä¸èƒ½**åœ¨æ¨¡å‹åŠ è½½ä¹‹å‰è®¾ç½®ï¼ˆæ¨¡å‹åŠ è½½ä¼šæ¶ˆè€—éšæœºçŠ¶æ€ï¼Œå¯¼è‡´ P çŸ©é˜µä¸ä¸€è‡´ï¼‰ã€‚

å¦‚æœä½ æƒ³è‡ªå®šä¹‰è¾“å‡ºç›®å½•ï¼Œå¯ä»¥ä¿®æ”¹ `train_rl.py` é¡¶éƒ¨çš„ï¼š

```python
OUTPUT_DIR = "./output/luoguqwencoder-lora"
```

---

## æ•°æ®å‡†å¤‡ä¸æ ¼å¼

### ä¸Šæ¸¸æ•°æ®ï¼šCodeContests (AlphaCode)

- **æ•°æ®æ¥æº**ï¼šDeepMind `code_contests` (https://github.com/google-deepmind/code_contests)ã€‚

æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ª JSON å¯¹é½çš„é¢˜ç›®å¯¹è±¡ï¼ŒåŒ…å«ï¼š
- `description`: é¢˜ç›®æè¿°ã€‚
- `public_tests`: å…¬å¼€æµ‹è¯•æ ·ä¾‹ã€‚
- `private_tests`: éšè—æµ‹è¯•æ ·ä¾‹ã€‚

### RL è®­ç»ƒæ•°æ®ï¼šJSONL æ ¼å¼

`download_dataset.py` ç”Ÿæˆçš„ JSONL æ ¼å¼å¦‚ä¸‹ï¼š

```json
{
  "description": "<é¢˜ç›®æè¿°>",
  "public_tests": {
    "input": ["<input1>", "<input2>"],
    "output": ["<output1>", "<output2>"]
  }
}
```

åœ¨ `train_rl.py` ä¸­é€šè¿‡ `map(apply_chat_template)` å°†å…¶è½¬æ¢ä¸ºæ¨¡å‹æ‰€éœ€çš„ prompt æ ¼å¼ã€‚

### æ•°æ®è¿‡æ»¤ä¸å¥–åŠ±é…ç½® (Dataset & Reward Config)

ä¸ºäº†æé«˜è®­ç»ƒæ•ˆç‡å¹¶é’ˆå¯¹æ€§åœ°ä¼˜åŒ–ç‰¹å®šéš¾åº¦çš„é¢˜ç›®ï¼Œæˆ‘ä»¬åœ¨ `train_rl.py` ä¸­å¼•å…¥äº†åŸºäº **Source (æ¥æº)** å’Œ **Difficulty (éš¾åº¦)** çš„åŒå±‚è¿‡æ»¤ä¸å¥–åŠ±æœºåˆ¶ï¼š

1. **é¢˜ç›®è¿‡æ»¤ (`DATASET_CONFIG`)**ï¼š
   - å…è®¸æ ¹æ®ä¸åŒçš„åˆ¤é¢˜å¹³å°ï¼ˆå¦‚ Codeforcesã€AtCoderï¼‰é€‰æ‹©ç‰¹å®šçš„éš¾åº¦åŒºé—´ã€‚
   - é»˜è®¤é…ç½®ï¼šä»…ä¿ç•™ Codeforces å’Œ AtCoder çš„ A-B çº§åˆ«å…¥é—¨é¢˜ç›®ï¼ˆDifficulty 7, 8ï¼‰ã€‚

2. **å¥–åŠ±ç¼©æ”¾ (`REWARD_SCALING_CONFIG`)**ï¼š
   - **ç¬¬ä¸€å…³é”®å­— (Source)**ï¼šæ ¹æ®å¹³å°è°ƒæ•´åŸºç¡€æƒé‡ã€‚
   - **ç¬¬äºŒå…³é”®å­— (Difficulty)**ï¼šåœ¨åŒä¸€å¹³å°å†…ï¼Œé’ˆå¯¹æ›´éš¾çš„é¢˜ç›®ç»™äºˆæ›´é«˜çš„å¥–åŠ±å€æ•°ï¼ˆMultiplierï¼‰ã€‚
   - ç¤ºä¾‹ï¼šé€šè¿‡ B çº§é¢˜ç›®çš„å¥–åŠ±æ¯”é€šè¿‡ A çº§é¢˜ç›®é«˜å‡º 10% ($1.1 \times$ vs $1.0 \times$)ã€‚

è¿™ç§æœºåˆ¶å¯ä»¥å¸®åŠ©æ¨¡å‹åœ¨ä¿æŒã€Œèƒ½è¿‡æ ·ä¾‹ã€çš„åŸºç¡€ä¸Šï¼Œä¼˜å…ˆå­¦ä¹ å…·æœ‰æŒ‘æˆ˜æ€§çš„é€»è¾‘ã€‚

---

## è®­ç»ƒæµç¨‹ï¼ˆRL / GRPOï¼‰

æ ¸å¿ƒè®­ç»ƒé€»è¾‘ä½äº `train_rl.py`ï¼š

1. **æ¨¡å‹åŠ è½½ä¸é‡åŒ–**
   - ä½¿ç”¨ `BitsAndBytesConfig`ï¼š
     - `load_in_4bit=True`
     - `bnb_4bit_quant_type="nf4"`
     - `bnb_4bit_use_double_quant=True`
     - `bnb_4bit_compute_dtype=torch.float16`
   - é€šè¿‡ `device_map="auto"` å°†æ¨¡å‹è‡ªåŠ¨åˆ‡åˆ†åˆ°å¯ç”¨ GPUã€‚

2. **TinyLoRA æ³¨å…¥ä¸å‚æ•°å†»ç»“**
   - åˆ›å»ºå…¨å±€å…±äº«å‘é‡ï¼ˆç»´åº¦ç”±å‘½ä»¤è¡Œå‚æ•° `u` å†³å®šï¼Œé»˜è®¤16ï¼‰ï¼š
     - `global_v = nn.Parameter(torch.zeros(U_VALUE))`
   - **æ³¨æ„ (v2.5)**ï¼šå¿…é¡»åˆå§‹åŒ–ä¸º `zeros`ï¼Œä¸èƒ½ä½¿ç”¨ `randn`ï¼`randn` åˆå§‹åŒ–ä¼šå¯¼è‡´ $\Delta W$ çˆ†ç‚¸ã€æ¨¡å‹ä¹±ç ã€æ¢¯åº¦ä¸ºé›¶çš„è¿é”æ•…éšœã€‚
   - é€šè¿‡ `apply_tiny_lora(model, global_v)`ï¼š
     - éå†æ¨¡å‹å­æ¨¡å—ï¼›
     - æ‰¾åˆ°åå­—ä»¥ `q_proj / k_proj / v_proj / o_proj / gate_proj / up_proj / down_proj` ç»“å°¾çš„ `nn.Linear`ï¼›
     - æ›¿æ¢ä¸º `TinyLoRALinear`ï¼›
   - éšåï¼š
     - ä»…ä¿ç•™ `global_v` çš„ `requires_grad=True`ï¼›
     - å…¶ä»–æ‰€æœ‰å‚æ•°å…¨éƒ¨ `requires_grad=False`ã€‚

3. **GRPO é…ç½®**

`train_rl.py` ä¸­ä½¿ç”¨çš„ç¤ºä¾‹è¶…å‚æ•°ï¼š

- `num_train_epochs=1`
- `per_device_train_batch_size=1`
- `gradient_accumulation_steps=8`
- `learning_rate=1e-5`
- `num_generations=4`ï¼ˆGroup Size Gï¼Œæ¯ä¸ªæ ·æœ¬é‡‡æ · 4 ä¸ªç­”æ¡ˆï¼‰
- `max_completion_length=512`
- `bf16=True`

ä½ å¯ä»¥æ ¹æ®æ˜¾å­˜ä¸è®­ç»ƒæ—¶é—´éœ€æ±‚è°ƒæ•´ä¸Šé¢çš„å‚æ•°ã€‚

4. **è®­ç»ƒå¾ªç¯**

GRPO çš„æ•´ä½“æµç¨‹ç®€è¦ä¸ºï¼š

- å¯¹äºæ¯ä¸ªæ ·æœ¬ `prompt`ï¼š
  1. é‡‡æ ·å¤šä¸ª `completions`ï¼ˆC++ ä»£ç ï¼‰ï¼›
  2. è°ƒç”¨ `code_reward_func` å¯¹æ¯ä¸ª completion ç¼–è¯‘ + è¿è¡Œï¼Œå¾—åˆ° rewardï¼›
  3. ä½¿ç”¨ GRPO ç®—æ³•æ ¹æ® reward æ›´æ–°ç­–ç•¥ï¼ˆè¿™é‡Œå°±æ˜¯æ›´æ–° 16 ç»´çš„ `global_v`ï¼‰ã€‚


æ”¯æŒè‡ªå®šä¹‰çš„GRPOï¼Œå¦‚rewardè®¾ç½®ã€‚

---

## éªŒè¯ä¸æµ‹è¯•

### è®­ç»ƒä¸­éªŒè¯ï¼ˆValidation during Trainingï¼‰

ä» v2.0 å¼€å§‹ï¼Œ`train_rl.py` æ”¯æŒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸè¿è¡ŒéªŒè¯ï¼š

```bash
# å¯ç”¨éªŒè¯ / Enable validation
python train_rl.py 16 2000 --do_validate --val_steps 100 --val_samples 10
```

**å‚æ•°è¯´æ˜**ï¼š
- `--do_validate`: å¯ç”¨éªŒè¯åŠŸèƒ½
- `--val_steps N`: æ¯ N ä¸ªè®­ç»ƒæ­¥éª¤è¿è¡Œä¸€æ¬¡éªŒè¯ï¼ˆé»˜è®¤ 100ï¼‰
- `--val_samples N`: æ¯æ¬¡éªŒè¯ä½¿ç”¨çš„æ ·æœ¬æ•°ï¼ˆé»˜è®¤ 10ï¼‰

**éªŒè¯æµç¨‹**ï¼š
1. åœ¨æŒ‡å®šæ­¥éª¤è§¦å‘éªŒè¯å›è°ƒ
2. åœ¨éªŒè¯é›†ä¸Šç”Ÿæˆä»£ç å¹¶ç¼–è¯‘è¿è¡Œ
3. è®¡ç®— Pass@1ã€ç¼–è¯‘æˆåŠŸç‡ç­‰æŒ‡æ ‡
4. å¦‚æœ Pass@1 æé«˜ï¼Œè‡ªåŠ¨ä¿å­˜åˆ° `best_tiny_lora_v.pt`

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
================================================================================
ğŸ” Running validation at step 100 / åœ¨ç¬¬ 100 æ­¥è¿è¡ŒéªŒè¯
================================================================================

ğŸ“Š Validation Results / éªŒè¯ç»“æœ:
  â€¢ Average Score / å¹³å‡åˆ†æ•°: 0.6500
  â€¢ Compile Rate / ç¼–è¯‘æˆåŠŸç‡: 80.00%
  â€¢ Pass@1 / é€šè¿‡ç‡: 35.00%
  â€¢ Compile Success / ç¼–è¯‘æˆåŠŸ: 8/10
  â€¢ Full Pass / å®Œå…¨é€šè¿‡: 3/10

================================================================================
ğŸ‰ New best model! / æ–°çš„æœ€ä½³æ¨¡å‹ï¼
   Previous best Pass@1 / ä¹‹å‰æœ€ä½³é€šè¿‡ç‡: 0.00%
   Current Pass@1 / å½“å‰é€šè¿‡ç‡: 35.00%
================================================================================

ğŸ’¾ Best model saved to / æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: ./output/luoguqwencoder-lora/best_tiny_lora_v.pt
```

### ç‹¬ç«‹éªŒè¯ï¼ˆStandalone Validationï¼‰

ä¹Ÿå¯ä»¥åœ¨è®­ç»ƒåå•ç‹¬è¿è¡ŒéªŒè¯ï¼š

```bash
# ä½¿ç”¨é»˜è®¤è®¾ç½® / Use default settings
python validate.py

# è‡ªå®šä¹‰éªŒè¯æ ·æœ¬æ•° / Custom number of samples
python validate.py 50
```

`validate.py` ä¼šï¼š
1. åŠ è½½ `./output/luoguqwencoder-lora/tiny_lora_v.pt` æ£€æŸ¥ç‚¹
2. é‡å»º TinyLoRA æ¨¡å‹
3. åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
4. è¾“å‡ºè¯¦ç»†æŒ‡æ ‡

### æµ‹è¯•è¯„ä¼°ï¼ˆTestingï¼‰

åœ¨è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ `test.py` åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°ï¼š

```bash
# åŸºæœ¬ç”¨æ³• / Basic usage (named args)
python test.py --checkpoint_path ./output/luoguqwencoder-lora/tiny_lora_v.pt --num_samples 50

# æµ‹è¯•æœ€ä½³æ¨¡å‹ / Test best model
python test.py --checkpoint_path ./output/luoguqwencoder-lora/best_tiny_lora_v.pt --num_samples 100

# è‡ªå®šä¹‰æµ‹è¯•æ•°æ® / Custom test data
python test.py --checkpoint_path ./output/luoguqwencoder-lora/tiny_lora_v.pt --num_samples 50 --test_data ./local_code_contests/code_contests_test.jsonl
```

**å‘½ä»¤è¡Œå‚æ•°**ï¼š
- `--checkpoint_path`: æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆé»˜è®¤ `./output/luoguqwencoder-lora/tiny_lora_v.pt`ï¼‰
- `--num_samples`: æµ‹è¯•æ ·æœ¬æ•°ï¼ˆé»˜è®¤ 50ï¼‰
- `--test_data`: æµ‹è¯•æ•°æ®é›†è·¯å¾„

**æµ‹è¯•æµç¨‹**ï¼š
1. **åŠ è½½æ£€æŸ¥ç‚¹**ï¼šè¯»å– `.pt` æ–‡ä»¶ä¸­çš„å…ƒæ•°æ®ï¼ˆ`seed`, `u_value`, `rank`ï¼‰
2. **è®¾ç½®éšæœºç§å­**ï¼šä½¿ç”¨ `torch.manual_seed(seed)` ç¡®ä¿ P çŸ©é˜µä¸€è‡´
3. **åŠ è½½åŸºåº§æ¨¡å‹**ï¼šä½¿ç”¨ 4-bit é‡åŒ–åŠ è½½ `Qwen2.5-Coder-3B-Instruct`
4. **æ³¨å…¥ TinyLoRA**ï¼šä½¿ç”¨ç›¸åŒçš„ `u_value` å’Œ `seed` æ‰§è¡Œ `apply_tiny_lora`
5. **åŠ è½½æƒé‡**ï¼šå°† `global_v` åŠ è½½åˆ°æ¨¡å‹
6. **è¿è¡Œè¯„ä¼°**ï¼šåœ¨æµ‹è¯•é›†ä¸Šç”Ÿæˆä»£ç å¹¶è¯„ä¼°

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
================================================================================
âœ… Evaluation complete / è¯„ä¼°å®Œæˆ
================================================================================

ğŸ“Š Test Results / æµ‹è¯•ç»“æœ:
   â€¢ Total Samples / æ€»æ ·æœ¬æ•°: 50
   â€¢ Average Score / å¹³å‡åˆ†æ•°: 0.7200
   â€¢ Compile Rate / ç¼–è¯‘æˆåŠŸç‡: 86.00% (43/50)
   â€¢ Pass@1 / å®Œå…¨é€šè¿‡ç‡: 42.00% (21/50)
   â€¢ Partial Pass / éƒ¨åˆ†é€šè¿‡: 22/50
   â€¢ No Code Extracted / æœªæå–åˆ°ä»£ç : 7/50
================================================================================
```

### æ£€æŸ¥ç‚¹æ–‡ä»¶æ ¼å¼

è®­ç»ƒå’ŒéªŒè¯ä¿å­˜çš„ `.pt` æ–‡ä»¶åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š

```python
{
    "global_v": tensor([...]),           # è®­ç»ƒå¥½çš„å…±äº«å‘é‡
    "u_value": 16,                       # å‘é‡ç»´åº¦
    "rank": 2,                           # TinyLoRA ç§©
    "seed": 42,                          # éšæœºç§å­ï¼ˆç”¨äºé‡å»º P çŸ©é˜µï¼‰
    "model_id": "qwen/Qwen2.5-Coder-3B-Instruct",  # åŸºåº§æ¨¡å‹ ID
    "total_replaced_layers": 252,       # æ›¿æ¢çš„å±‚æ•°
    "validation_score": 0.42,           # éªŒè¯åˆ†æ•°ï¼ˆä»… best_tiny_lora_v.ptï¼‰
    "step": 500,                         # è®­ç»ƒæ­¥æ•°ï¼ˆä»… best_tiny_lora_v.ptï¼‰
}
```

**é‡è¦æç¤º**ï¼š
- éšæœºç§å­ `seed` å¯¹äºé‡ç°è‡³å…³é‡è¦ï¼ŒP çŸ©é˜µç”±å®ƒç”Ÿæˆ
- SVD åˆ†è§£æ˜¯ç¡®å®šæ€§è¿ç®—ï¼ŒU/S/Vh å¯å®Œå…¨å¤ç°
- åªè¦ `seed`, `u_value`, `rank` ç›¸åŒï¼Œå°±èƒ½å®Œå…¨é‡å»ºæ¨¡å‹

---

## TinyLoRA Tiling æŠ€æœ¯ç»†èŠ‚

è‡ªå®šä¹‰å±‚ `TinyLoRALinear` çš„æ ¸å¿ƒæ€æƒ³ï¼š

1. å¯¹åŸå§‹æƒé‡çŸ©é˜µ `W âˆˆ R^{outÃ—in}` åš SVDï¼š

   $$W = U S V^H$$

   - å®ç°ä¸­å…ˆå°† 4bit æƒé‡åé‡åŒ–ä¸º `W_real`ï¼Œå†åœ¨ CPU ä¸Šåš `torch.linalg.svd`ï¼›
   - åªå–å‰ `rank=2` ä¸ªå¥‡å¼‚å€¼åŠå¯¹åº”çš„åˆ— / è¡Œï¼Œå¾—åˆ°ç²¾ç®€ç‰ˆ `U, S, V^H`ï¼›
   - è¿™äº›å¼ é‡é€šè¿‡ `register_buffer` æ³¨å†Œä¸º Bufferï¼Œä¸å‚ä¸è®­ç»ƒã€‚

2. å®šä¹‰å…¨å±€å…±äº«å‚æ•°ï¼š

   - `v âˆˆ R^u`ï¼Œå…¶ä¸­ `u=16`ï¼›
   - éšæœºåˆå§‹åŒ–ä¸€ç»„å›ºå®šçŸ©é˜µç°‡ `P âˆˆ R^{uÃ—rÃ—r}`ï¼›
   - æ„é€ ï¼š

     $$R = \sum_{i=1}^{u} v_i P_i \in R^{rÃ—r}$$

3. æ„é€ å¢é‡æƒé‡ï¼š

   - $$\Delta W = U S R V^H$$
   - å®é™…å‰å‘ä¸­è®¡ç®—ï¼š

     $$y = x W^T + x (\Delta W)^T$$

4. Tilingï¼ˆè·¨å±‚å…±äº«ï¼‰

   - æ¨¡å‹ä¸­æ‰€æœ‰ç›®æ ‡ `nn.Linear` å±‚éƒ½å…±äº«åŒä¸€ä¸ª `v`ï¼›
   - æ•´ä¸ªæ¨¡å‹åªæœ‰è¿™ä¸€ç»„ 16 ç»´å‚æ•°åœ¨æ›´æ–°ã€‚

ä½ å¯ä»¥é€šè¿‡ `verify_pipeline.py` æˆ–ç›´æ¥è§‚å¯Ÿ `train_rl.py` çš„å¯åŠ¨æ—¥å¿—æ¥ç¡®è®¤ TinyLoRA æ³¨å…¥æ˜¯å¦æ­£ç¡®å¹¶æ£€æŸ¥å¯è®­ç»ƒå‚æ•°é‡ã€‚

---

## å¥–åŠ±å‡½æ•°ï¼šç¼–è¯‘è¿è¡Œ C++ ä»£ç 

å¥–åŠ±å‡½æ•°å®ç°ä½äº `train_rl.py` ä¸­çš„ `code_reward_func` ä¸ `compile_and_run`ï¼š

1. **ä»æ¨¡å‹è¾“å‡ºä¸­æå–ä»£ç **
   - ä¼˜å…ˆåŒ¹é…å½¢å¦‚ï¼š

     ```markdown
          ```cpp
          // C++ ä»£ç 
          ```
     ```

   - è‹¥æ²¡æœ‰æ˜¾å¼ä»£ç å—ï¼Œåˆ™å›é€€ä¸ºåªè¦åŒ…å« `#include` çš„è£¸ä»£ç æ®µï¼›
   - è‹¥ä»æ— æ³•è¯†åˆ«ï¼Œåˆ™ç›´æ¥ç»™ 0 åˆ†ã€‚

2. **ç¼–è¯‘é˜¶æ®µ**
   - å°†ä»£ç å†™å…¥ä¸´æ—¶ç›®å½•ä¸­çš„ `solution.cpp`ï¼›
   - é€šè¿‡æ­£åˆ™åˆ é™¤ä»£ç ä¸­çš„ `freopen(...)` ç­‰æ–‡ä»¶é‡å®šå‘è¯­å¥ï¼Œæ”¹ç”¨æ ‡å‡†è¾“å…¥è¾“å‡ºï¼›
   - ä½¿ç”¨ï¼š

     ```bash
     g++ solution.cpp -o solution -O2
     ```

   - ç¼–è¯‘å¤±è´¥ / è¶…æ—¶ -> æœ¬æ¬¡æ ·æœ¬ reward = 0ã€‚

3. **è¿è¡Œé˜¶æ®µ**
   - å¯¹æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼š
     - å°† `case["input"]` ä½œä¸º stdinï¼›
     - æ•è· stdoutï¼Œä¸ `case["output"]` è¿›è¡Œå­—ç¬¦ä¸²çº§æ¯”å¯¹ï¼ˆ`strip()` åï¼‰ï¼›
   - è¿è¡Œæœ‰è¶…æ—¶ä¿æŠ¤ï¼ˆä¾‹å¦‚ 2 ç§’ï¼‰ï¼Œé˜²æ­¢æ­»å¾ªç¯å¡æ­»è®­ç»ƒã€‚

4. **æ‰“åˆ†è§„åˆ™**

   å¥–åŠ±å‡½æ•°é‡‡ç”¨ä¸‰æ¡£è¯„åˆ†åˆ¶ï¼š

   - **ç¼–è¯‘å¤±è´¥** æˆ– **ä»£ç æ ¼å¼æ— æ•ˆ**ï¼š`reward = 0`
     - åŒ…æ‹¬ç¼–è¯‘é”™è¯¯ã€ç¼–è¯‘è¶…æ—¶ã€æ— æ³•æå–ä»£ç å—ç­‰æƒ…å†µï¼›
   
   - **ç¼–è¯‘æˆåŠŸä½†æµ‹è¯•ç”¨ä¾‹å¤±è´¥**ï¼š`reward = 0.5`
     - ä»£ç èƒ½é€šè¿‡ g++ ç¼–è¯‘ï¼Œä½†è¿è¡Œåä¸èƒ½é€šè¿‡å…¨éƒ¨æ ·ä¾‹æµ‹è¯•ï¼ˆå¯èƒ½é€šè¿‡éƒ¨åˆ†æˆ–å…¨éƒ¨å¤±è´¥ï¼‰ï¼›
   
   - **ç¼–è¯‘æˆåŠŸä¸”é€šè¿‡æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹**ï¼š`reward = 1.0`
     - ä»£ç æ—¢èƒ½ç¼–è¯‘æˆåŠŸï¼Œä¹Ÿèƒ½åœ¨æ‰€æœ‰æä¾›çš„æ ·ä¾‹ä¸Šäº§ç”Ÿæ­£ç¡®è¾“å‡ºã€‚

   **æ ¸å¿ƒå¼ºåŒ–ä¿¡å·**ï¼š
   - è¿™ç§è®¾è®¡é¼“åŠ±æ¨¡å‹å…ˆå­¦ä¼šç”Ÿæˆã€Œèƒ½ç¼–è¯‘çš„ä»£ç ã€ï¼ˆ0 â†’ 0.5 çš„è¿›æ­¥ï¼‰ï¼Œ
   - ç„¶ååœ¨ç¼–è¯‘åŸºç¡€ä¸Šè¿›ä¸€æ­¥ä¼˜åŒ–é€»è¾‘ä»¥é€šè¿‡æµ‹è¯•ç”¨ä¾‹ï¼ˆ0.5 â†’ 1.0 çš„è¿›æ­¥ï¼‰ã€‚
   - ç›¸æ¯”è¿ç»­æ‰“åˆ†ï¼Œç¦»æ•£ reward æä¾›äº†æ›´æ¸…æ™°çš„å­¦ä¹ é˜¶æ®µåˆ’åˆ†ã€‚

> è¿™æ„å‘³ç€æ¨¡å‹ä¸ä»…è¦ã€Œçœ‹èµ·æ¥åƒ C++ã€ï¼Œè¿˜è¦çœŸçš„èƒ½é€šè¿‡æ ·ä¾‹è¾“å…¥è¾“å‡ºï¼Œ
> å¼ºåŒ–ä¿¡å·æ¥è‡ªçœŸå®çš„ç¼–è¯‘å™¨ä¸è¿è¡Œç¯å¢ƒï¼Œè€Œéé™æ€æ‰“åˆ†ã€‚

---

## èµ„æºæ¶ˆè€—ä¸æ³¨æ„äº‹é¡¹

- **æ˜¾å­˜**ï¼š
  - 3B æ¨¡å‹ + 4bit é‡åŒ– + BF16 è®¡ç®—ï¼Œå•å¡ 16GB æ˜¾å­˜å¯ä»¥å°è¯•ï¼ˆä½†ä½™é‡ä¸ç®—å¤§ï¼‰ï¼›
  - RL + ç¼–è¯‘è¿è¡Œä¼šæ˜¾è‘—å¢åŠ æ—¶é—´æ¶ˆè€—ï¼Œè®­ç»ƒé€Ÿåº¦ä¼šæ¯”ä¼ ç»Ÿ LoRA SFT æ…¢å¾ˆå¤šã€‚

- **æ“ä½œç³»ç»Ÿ**ï¼š
  - æ¨è Linux ç¯å¢ƒï¼ˆå½“å‰è„šæœ¬åœ¨ Linux ä¸‹å¼€å‘ä¸æµ‹è¯•ï¼‰ï¼›
  - éœ€è¦å¯ç”¨çš„ `g++`ï¼Œå¹¶ä¸”èƒ½å¤Ÿåœ¨ä¸´æ—¶ç›®å½•ä¸‹åˆ›å»ºä¸æ‰§è¡Œå¯æ‰§è¡Œæ–‡ä»¶ã€‚

- **å®‰å…¨**ï¼š
  - å¼ºçƒˆä¸å»ºè®®å¯¹ä¸å—ä¿¡ä»»çš„æ•°æ®é›†è¿è¡Œæ­¤å¥–åŠ±å‡½æ•°ï¼›
  - æœ¬é¡¹ç›®çš„å‡è®¾æ˜¯ã€Œæ•°æ®é›†æ¥æºå¯ä¿¡ã€ä¸”ä»…ç”¨äºç ”ç©¶ç¯å¢ƒã€‚

---

## å¼€æºä¸è®¸å¯è¯

### é¡¹ç›®è®¸å¯è¯
- æœ¬ä»“åº“è„šæœ¬é‡‡ç”¨ **CC BY 4.0** è®¸å¯è¯ï¼ˆCreative Commons Attribution 4.0 International licenseï¼‰ï¼Œä»¥ç¬¦åˆæ•°æ®é›†å¼•ç”¨è¦æ±‚ã€‚

### æ•°æ®é›†è®¸å¯è¯
æœ¬é¡¹ç›®ä½¿ç”¨çš„ **CodeContests (AlphaCode)** æ•°æ®é›†éµå¾ª **CC BY 4.0** è®¸å¯è¯ã€‚
æ­¤å¤–åŒ…å«ä»¥ä¸‹è´¡çŒ®ï¼š
- Codeforces ç´ ææ¥æºäº [codeforces.com](http://codeforces.com)ã€‚
- Description2Code ç´ ææ¥æºäº [Description2Code Dataset](https://github.com/multi30k/dataset)ï¼Œé‡‡ç”¨ MIT è®¸å¯è¯ã€‚
- CodeNet ç´ ææ¥æºäº [Project_CodeNet](https://github.com/IBM/Project_CodeNet)ï¼Œé‡‡ç”¨ Apache 2.0 è®¸å¯è¯ã€‚

### æ¨¡å‹è®¸å¯è¯
- åŸºåº§æ¨¡å‹ `Qwen2.5-Coder-3B-Instruct` ç”± Qwen å›¢é˜Ÿæä¾›ï¼Œè¯·éµå®ˆå…¶åŸå§‹è®¸å¯è¯ã€‚

---

## å¼•ç”¨

å¦‚æœæ‚¨è§‰å¾—æœ¬é¡¹ç›®å¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨ä»¥ä¸‹è®ºæ–‡ï¼š

**TinyLoRA è®ºæ–‡ï¼š**
```bibtex
@article{morris2026learning,
  title={Learning to Reason in 13 Parameters},
  author={Morris, John X and Mireshghallah, Niloofar and Ibrahim, Mark and Mahloujifar, Saeed},
  journal={arXiv preprint arXiv:2602.04118},
  year={2026}
}
```

**AlphaCode/CodeContests:**
```bibtex
@article{li2022competition,
  title={Competition-Level Code Generation with AlphaCode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and
    Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and
    Keeling, James and Gimeno, Felix and Dal Lago, Agustin and
    Hubert, Thomas and Choy, Peter and de Masson d'Autume, Cyprien and
    Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and
    Gowal, Sven and Cherepanov, Alexey and Molloy, James and
    Mankowitz, Daniel and Sutherland Robson, Esme and Kohli, Pushmeet and
    de Freitas, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
  journal={arXiv preprint arXiv:2203.07814},
  year={2022}
}
```

**æ•°æ®é›†ï¼ˆAlphaCode/CodeContestsï¼‰å¼•ç”¨ï¼š**
```bibtex
@article{li2022competition,
  title={Competition-Level Code Generation with AlphaCode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and
    Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and
    Keeling, James and Gimeno, Felix and Dal Lago, Agustin and
    Hubert, Thomas and Choy, Peter and de Masson d'Autume, Cyprien and
    Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and
    Gowal, Sven and Cherepanov, Alexey and Molloy, James and
    Mankowitz, Daniel and Sutherland Robson, Esme and Kohli, Pushmeet and
    de Freitas, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
  journal={arXiv preprint arXiv:2203.07814},
  year={2022}
}
```

---

## English Version

### TinyLoRA-Qwen-Coder Experiment


TinyLoRA-Qwen-CoderL is an advanced evolution of the original [LuoguQwen SFT project](https://github.com/Chi-Shan0707/Qwen4Luogu-SFT) and [Qwen4Luogu-RL project](https://github.com/Chi-Shan0707/Qwen4Luogu-RL).

### Changelog

#### v2.5 â€” Critical Bug Fixes

This release fixes three critical bugs that caused **zero gradients during training and zero code extraction during testing**:

| # | Bug | Impact | Fix |
| :---: | :--- | :--- | :--- |
| **1** | `global_v` initialized with `randn` instead of `zeros` | Every linear layer received a massive random perturbation ($\Delta W$ magnitude ~400) from the first forward pass, turning model outputs into gibberish. No code could be extracted, all GRPO rewards were 0, gradients were 0, and `v` was never updated. | `utils.py`: Changed `TinyLoRAGlobalParams.__init__` to use `torch.zeros(...)` |
| **2** | Random seed misalignment between training and testing | During training, seed was set *immediately before* `apply_tiny_lora`. During testing, seed was set *before model loading*, which consumes extensive random state, causing P matrices to differ. The trained `v` vector paired with wrong P matrices produced incorrect $\Delta W$. | `test.py`: Re-seed right before `apply_tiny_lora` call |
| **3** | P matrix missing $1/\sqrt{r}$ scaling | Poor gradient magnitude conditioning; the paper recommends a scaling factor for variance stability. | `utils.py`: P matrix generation now divides by `rank ** 0.5` |

**Symptom chain (v2.0 and earlier):**
```
global_v ~ N(0,1) â†’ Î”W explodes â†’ model outputs gibberish â†’ no code extracted
â†’ all rewards = 0 â†’ GRPO advantage = 0/0 â†’ grad_norm = 0, loss = 0
â†’ v never updates â†’ saved checkpoint still random â†’ test also fails
```

> **Important**: v2.5 changes P matrix scaling and `global_v` initialization. Checkpoints (`.pt` files) from previous versions are **incompatible** and require retraining.

#### v2.0 â€” Modular Refactor & Validation System
- Extracted shared utilities into `utils.py`
- Added `validate.py` and `test.py`
- Support for in-training validation with automatic best model saving
- Support for baseline testing (`--baseline`)

The goal of 
TinyLoRA-Qwen-Coder is:
> Under the constraints of extremely limited VRAM (3B model + 4bit quantization) and extreme parameter compression (only 16 trainable parameters), train Qwen2.5-Coder through Reinforcement Learning (RL) to generate C++ code that passes sample tests on CodeContests competitive programming problems.

This repository is an **unofficial reproduction and adaptation of the TinyLoRA paper**:
- `theory/README.md` provides theoretical insights into TinyLoRA / GRPO.
- We extend TinyLoRA from mathematical reasoning (GSM8K) to **code generation + compile-and-run rewards**.
- While the paper uses 7B models with 13 parameters, we use a 3B Coder model with 16 parameters, maintaining the "extreme low-rank + global sharing" core philosophy.

Currently, `train_rl.py` is functional, though achieving high pass rates on competitive problems remains a significant challenge due to:
- Prompt Engineering needs (e.g., explicit reasoning paths).
- Context window limitations (currently 1024 tokens).
- Group size (G) in GRPO vs. complexity of problems.
- Base model capacity (3B).

**Core Scripts:**
- `train_rl.py`: Main training script using 4-bit `Qwen2.5-Coder-3B-Instruct`, TinyLoRA Tiling, and `GRPOTrainer` with `g++` compilation rewards.
- `download_dataset.py`: Stream-downloads and prepocesses the `deepmind/code_contests` (AlphaCode) dataset.
- `verify_pipeline.py`: Validates the end-to-end flow of model loading, generation, extraction, and compilation.

### Paper Reproduction

This project is based on the paper **"Learning to Reason in 13 Parameters" (Morris et al., 2026)**.

#### 1. Core Theory: TinyLoRA
TinyLoRA is an extreme parameter-efficient fine-tuning method that breaks the rank limits of traditional LoRA ($O(d \times r)$). By freezing the original weight's characteristic directions ($U, V$) via SVD and learning only a tiny shared vector $v$ (**Tiling**), it compresses trainable parameters to single digits.

#### 2. Why Reinforcement Learning?
At such extreme parameter scales (<100 parameters), Supervised Fine-Tuning (SFT) often fails because it forces the model to memorize noise (formatting/styles). RL instead focuses on the "Signal" (correctness), allowing the model to ignore irrelevant details and succeed even with minimal capacity.

#### 3. Adaptation Table

| Feature | Paper Setting | Our Adaptation |
| :--- | :--- | :--- |
| **Domain** | Math Reasoning (GSM8K, MATH) | **Code Competitions (CodeContests)** |
| **Base Model** | Qwen2.5-7B / Llama-3 | **Qwen2.5-Coder-3B-Instruct** |
| **Parameters** | 13 parameters ($u=13$) | **16 parameters ($u=16$)**ï¼ˆcan be changedï¼‰ |
| **Precision** | BF16 / FP32 | **4-bit (NF4) + Dynamic Dequant SVD** |
| **Reward** | Exact Match | **g++ Compile + Test Case Execution** |
| **Optimization**| High-end GPUs (A100/H100) | **Consumer GPUs (16GB+ VRAM)** |

---

## Project Structure

This project adopts a modular design, separating training, validation, and testing logic for better maintainability and reproducibility.

### Core Modules

#### 1. `utils.py` - Shared Utilities Module

Contains all shared functionality for training, validation, and testing:

- **TinyLoRA Classes**:
  - `TinyLoRAGlobalParams`: Global shared vector container
  - `TinyLoRALinear`: Custom TinyLoRA linear layer
  - `apply_tiny_lora()`: Inject TinyLoRA layers into model

- **Code Evaluation Functions**:
  - `compile_and_run()`: C++ code compilation and execution
  - `extract_code_from_response()`: Extract code from model response
  - `convert_hf_tests_to_list()`: Convert test case format
  - `apply_chat_template()`: Build prompts from problem descriptions

- **Model Loading Utilities**:
  - `get_model_and_tokenizer()`: Load 4-bit quantized model and tokenizer

#### 2. `train_rl.py` - Training Script

Main training script with optional validation support:

```bash
# Basic training
python train_rl.py [u_value] [max_samples]

# Training with validation
python train_rl.py 16 2000 --do_validate --val_steps 100 --val_samples 10
```

**Command-line Arguments:**
- `u_value`: TinyLoRA shared vector dimension (default: 16)
- `max_samples`: Maximum training samples (default: 2000)
- `--do_validate`: Enable validation during training
- `--val_steps N`: Run validation every N steps (default: 100)
- `--val_samples N`: Number of validation samples (default: 10)

**Validation Features:**
- Automatic validation during training
- Tracks best Pass@1 score
- Auto-saves best model to `best_tiny_lora_v.pt`

#### 3. `validate.py` - Validation Script

Can be run standalone or imported by `train_rl.py`:

```bash
# Standalone validation
python validate.py [num_samples]
```

**Features:**
- Loads trained checkpoint
- Evaluates on validation set
- Calculates Pass@1, compile rate, and other metrics

#### 4. `test.py` - Testing Script

For final evaluation on the test dataset (named parameters):

```bash
# Basic testing
python test.py --checkpoint_path <path> --num_samples <N>

# Baseline testing (base model without TinyLoRA)
python test.py --baseline --num_samples <N>

# Examples
python test.py --checkpoint_path ./output/luoguqwencoder-lora/tiny_lora_v.pt --num_samples 50
python test.py --checkpoint_path ./output/luoguqwencoder-lora/best_tiny_lora_v.pt --num_samples 100
```

**Features:**
- Loads metadata from `.pt` checkpoint (seed, u_value, rank)
- Sets random seed to ensure identical P matrices
- Loads base model and injects TinyLoRA
- Loads trained weights `global_v`
- Runs evaluation on test set

#### 5. `download_dataset.py` - Dataset Download Script

Stream-downloads CodeContests dataset from HuggingFace:

```bash
python download_dataset.py
```

### Workflow

1. **Data Preparation**: Run `download_dataset.py` to download and preprocess data
2. **Model Training**: Run `train_rl.py` for RL training (optional with validation)
3. **Training Validation**: If `--do_validate` is enabled, periodic validation runs automatically
4. **Save Best Model**: Automatically saves to `best_tiny_lora_v.pt` when validation score improves
5. **Final Testing**: Run `test.py` to evaluate model on test set

### File Dependencies

```
utils.py (base utilities)
  â”‚
  â”œâ”€â”€ train_rl.py (imports and uses)
  â”‚   â””â”€â”€ calls validate.py
  â”‚
  â”œâ”€â”€ validate.py (imports and uses)
  â”‚
  â””â”€â”€ test.py (imports and uses)
```

All scripts depend on shared functionality in `utils.py`, ensuring code consistency and maintainability.

---

## Validation & Testing

### Validation During Training

From v2.0, `train_rl.py` supports periodic validation during training:

```bash
# Enable validation
python train_rl.py 16 2000 --do_validate --val_steps 100 --val_samples 10
```

**Parameters:**
- `--do_validate`: Enable validation functionality
- `--val_steps N`: Run validation every N training steps (default: 100)
- `--val_samples N`: Number of samples per validation run (default: 10)

**Validation Flow:**
1. Validation callback triggers at specified steps
2. Generates code on validation set and compiles/runs
3. Calculates Pass@1, compile rate, and other metrics
4. If Pass@1 improves, auto-saves to `best_tiny_lora_v.pt`

**Example Output:**
```
ğŸ” Running validation at step 100

ğŸ“Š Validation Results:
  â€¢ Average Score: 0.6500
  â€¢ Compile Rate: 80.00%
  â€¢ Pass@1: 35.00%
  â€¢ Compile Success: 8/10
  â€¢ Full Pass: 3/10

ğŸ‰ New best model!
   Previous best Pass@1: 0.00%
   Current Pass@1: 35.00%

ğŸ’¾ Best model saved to: ./output/luoguqwencoder-lora/best_tiny_lora_v.pt
```

### Standalone Validation

Can also run validation independently after training:

```bash
# Use default settings
python validate.py

# Custom number of samples
python validate.py 50
```

### Testing Evaluation

After training completes, use `test.py` for final evaluation on test set:

```bash
# Test TinyLoRA model (named args)
python test.py --checkpoint_path ./output/luoguqwencoder-lora/tiny_lora_v.pt --num_samples 50

# Test best model
python test.py --checkpoint_path ./output/luoguqwencoder-lora/best_tiny_lora_v.pt --num_samples 100

# Test base model (for comparison - shows effect of fine-tuning)
python test.py --baseline --num_samples 50

# Custom test data
python test.py --checkpoint_path ./output/luoguqwencoder-lora/tiny_lora_v.pt --num_samples 50 --test_data ./local_code_contests/code_contests_test.jsonl
```

**Baseline Mode (`--baseline`)**:
- Tests the original base model WITHOUT TinyLoRA adaptations
- Useful to see the effect of fine-tuning by comparing with baseline
- Skips checkpoint loading and TinyLoRA injection
- Direct comparison: `python test.py --baseline --num_samples 50` vs `python test.py --checkpoint_path ./output/luoguqwencoder-lora/best_tiny_lora_v.pt --num_samples 50`

**Command-line Arguments:**
- `--checkpoint_path`: Checkpoint path (default: `./output/luoguqwencoder-lora/tiny_lora_v.pt`)
- `--num_samples`: Number of test samples (default: 50)
- `--test_data`: Test dataset path
- `--baseline`: Test base model without TinyLoRA (for comparison with fine-tuned version)

**Testing Flow:**
1. **Load Checkpoint**: Read metadata from `.pt` file (`seed`, `u_value`, `rank`)
2. **Set Random Seed**: Use `torch.manual_seed(seed)` to ensure identical P matrices
3. **Load Base Model**: Load 4-bit quantized `Qwen2.5-Coder-3B-Instruct`
4. **Inject TinyLoRA**: Execute `apply_tiny_lora` with same `u_value` and `seed`
5. **Load Weights**: Load `global_v` into model
6. **Run Evaluation**: Generate code on test set and evaluate

**Example Output:**
```
ğŸ“Š Test Results:
   â€¢ Total Samples: 50
   â€¢ Average Score: 0.7200
   â€¢ Compile Rate: 86.00% (43/50)
   â€¢ Pass@1: 42.00% (21/50)
   â€¢ Partial Pass: 22/50
   â€¢ No Code Extracted: 7/50
```

### Checkpoint File Format

Training and validation save `.pt` files with the following information:

```python
{
    "global_v": tensor([...]),           # Trained shared vector
    "u_value": 16,                       # Vector dimension
    "rank": 2,                           # TinyLoRA rank
    "seed": 42,                          # Random seed (for rebuilding P matrices)
    "model_id": "qwen/Qwen2.5-Coder-3B-Instruct",  # Base model ID
    "total_replaced_layers": 252,       # Number of replaced layers
    "validation_score": 0.42,           # Validation score (best_tiny_lora_v.pt only)
    "step": 500,                         # Training step (best_tiny_lora_v.pt only)
}
```

**Important Notes:**
- Random seed `seed` is critical for reproducibility; it generates P matrices
- **v2.5 Note**: The seed must be set *immediately before* `apply_tiny_lora`, **not** before model loading (model loading consumes random state, causing P matrix mismatch)
- SVD decomposition is deterministic, so U/S/Vh are fully reproducible
- With identical `seed`, `u_value`, `rank`, the model can be completely reconstructed

---

### Core Features

- **Extreme Parameter Compression**: The entire model's trainable parameters consist of a single vector `global_v âˆˆ R^{16}` shared across all replaced linear layers.
- **TinyLoRA Tiling**: Freezes the base skeleton (`U, S, Vh`) from SVD and reconstructs low-rank increments via a shared vector `v`.
- **Real-world Code Reward**:
  - Compiles generated code with system `g++`.
  - Strips `freopen` to use standard I/O for compatibility.
  - Discrete rewards: `0` for failure, `0.5` for compilation success, `1.0` for passing all test cases.
- **VRAM Friendly**: Optimized for 16GB+ single-GPU setups using 4-bit quantization and BF16 computation.

### Quick Start

#### 1. Environment
Requires Linux, Python 3.10+, and `g++`.
```bash
pip install -r requirements.txt
```

#### 2. Model Download
`train_rl.py` auto-downloads `qwen/Qwen2.5-Coder-3B-Instruct` to `./models/` if missing.

#### 3. Data Preparation
Run the stream-download script to prepare CodeContests data:
```bash
python download_dataset.py
```

#### 4. Verification
Verify the model-to-execution pipeline:
```bash
python verify_pipeline.py
```

#### 5. Start RL Training
```bash
python train_rl.py [u] [MAX_SAMPLES]
```
- **`u`**: Shared vector dimension (default 16).
- **`MAX_SAMPLES`**: Max number of samples to train (default 2000).

Training saves `output/tiny_lora_v.pt` containing `global_v` and reconstruction metadata (seed, rank, model_id).

### Data Preparation and Format

- **Source**: DeepMind `code_contests` (https://github.com/google-deepmind/code_contests).
- **Format**: JSONL files in `./local_code_contests/`. Each entry includes `description` and `public_tests` (inputs/outputs).

### Dataset & Reward Configuration

To optimize training efficiency and focus on specific difficulty levels, `train_rl.py` implements a dual-layer filtering and rewarding mechanism based on **Source** and **Difficulty**:

1. **Problem Filtering (`DATASET_CONFIG`)**:
   - Allows selecting specific difficulty ranges for different platforms (e.g., Codeforces, AtCoder).
   - Default: Only Introductory A-B level problems (Difficulty 7, 8) from Codeforces and AtCoder are retained.

2. **Reward Scaling (`REWARD_SCALING_CONFIG`)**:
   - **First Key (Source)**: Adjusts base weights by platform.
   - **Second Key (Difficulty)**: Within a platform, harder problems receive higher reward multipliers.
   - Example: Passing a Level B problem yields 10% more reward than a Level A problem ($1.1 \times$ vs $1.0 \times$).

This allows the model to prioritize learning challenging logic while maintaining basic functional correctness.

### Technical Details: TinyLoRA Tiling

1. **SVD Integration**: 4-bit weights are dequantized to FP32 on CPU for SVD decomposition. Top components are stored as frozen buffers.
2. **Increment Construction**: $\Delta W = U S (\sum_{i=1}^{u} v_i P_i) V^H$, where $P$ is a fixed random projection cluster.
3. **Global Sharing**: Every injected layer references the same `global_v`.

### Reward Function Logic

- **Extraction**: Regex matching for code blocks or standard `#include` snippets.
- **Scoring**:
  - `0`: Compilation error or invalid format.
  - `0.5`: Successfully compiled but failed tests (partial or full).
  - `1.0`: Successfully passed all sample cases.
  This provides a clear gradient: Learn to compile first, then learn to solve.

### Resource Consumption

- **VRAM**: 16GB is the baseline recommendation.
- **Safety**: Reward function executes compiled binaries; ensure dataset trustworthiness.

### License and Citation

#### Project License
- This project is licensed under **CC BY 4.0** (Creative Commons Attribution 4.0 International license) to comply with dataset requirements.

#### Dataset Attribution
- **CodeContests (AlphaCode)** is provided under **CC BY 4.0**.
- Codeforces materials are sourced from [codeforces.com](http://codeforces.com).
- Description2Code materials are sourced from [Description2Code Dataset](https://github.com/multi30k/dataset) (MIT).
- CodeNet materials are sourced from [Project_CodeNet](https://github.com/IBM/Project_CodeNet) (Apache 2.0).

#### Citation
If you find this project useful, please cite the following papers:

```bibtex
@article{morris2026learning,
  title={Learning to Reason in 13 Parameters},
  author={Morris, John X and Mireshghallah, Niloofar and Ibrahim, Mark and Mahloujifar, Saeed},
  journal={arXiv preprint arXiv:2602.04118},
  year={2026}
}
```