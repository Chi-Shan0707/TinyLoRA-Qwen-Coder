import torch
import os
import re
import json
import subprocess
import tempfile
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# ==================== é…ç½®åŒºåŸŸ ====================
MS_MODEL_ID = "qwen/Qwen2.5-Coder-3B-Instruct"
LOCAL_MODEL_DIR = "./models/Qwen2.5-Coder-3B-Instruct"

# ã€å…³é”®ã€‘ä½¿ç”¨ CodeContests æ•°æ®ç»“æ„è¿›è¡Œæµ‹è¯•
# è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€å•çš„æ‹¬å·åŒ¹é…é—®é¢˜ä½œä¸ºæµ‹è¯•é¢˜ï¼ˆæ¥è‡ªå®é™…æ•°æ®é›†ï¼‰
TEST_DATA_JSON = {
    "description": """Problem description.
You are smart. You like maths. You are given a problem.
Input

Two integers(1<=x,y<=200)
Output:

The first line: their sum
The second line: their product




Explanation
Example is self-explanatory.""",
    "public_tests": {
        "input": ["1 2"],
        "output": ["3\n2"]
    },
    "private_tests": {
        "input": ["2 4"],
        "output": ["6\n8"]
    },
    "generated_tests": {
        "input": ["2  100"],
        "output": ["102\n200"]
    },
    "source": 0,
    "difficulty": 1
}
# =================================================

def print_step(title):
    print(f"\n{'='*10} {title} {'='*10}")

def extract_code(completion):
    """ä»å›å¤ä¸­æå–ä»£ç ï¼Œé€»è¾‘åŒ train_rl.py"""
    # ä¼˜å…ˆåŒ¹é…ä»£ç å—
    match = re.search(r"```(?:cpp|c\+\+)?\n(.*?)```", completion, re.DOTALL)
    if match:
        return match.group(1), "Code Block"
    # å…œåº•åŒ¹é… #include
    elif "#include" in completion:
        return completion, "Raw Text"
    else:
        return None, "Failed"

def compile_and_run(code, test_cases):
    """ç¼–è¯‘å¹¶è¿è¡Œï¼Œé€»è¾‘åŒ train_rl.py"""
    # ç§»é™¤ freopenï¼Œé˜²æ­¢å¡æ­»
    code = re.sub(r'freopen\s*\(.*?\);', '', code, flags=re.IGNORECASE)
    
    with tempfile.TemporaryDirectory() as temp_dir:
        src_file = os.path.join(temp_dir, "solution.cpp")
        exe_file = os.path.join(temp_dir, "solution")
        
        # å†™å…¥
        with open(src_file, 'w', encoding='utf-8') as f:
            f.write(code)
            
        print(f"   -> æ­£åœ¨ç¼–è¯‘ä¸´æ—¶æ–‡ä»¶...")
        # ç¼–è¯‘
        try:
            res = subprocess.run(
                ['g++', src_file, '-o', exe_file, '-O2'],
                capture_output=True, text=True, timeout=5
            )
            if res.returncode != 0:
                return 0.0, f"ç¼–è¯‘å¤±è´¥:\n{res.stderr}"
        except Exception as e:
            return 0.0, f"ç¼–è¯‘å¼‚å¸¸: {e}"

        # è¿è¡Œæµ‹è¯•ç”¨ä¾‹
        passed = 0
        total = len(test_cases)
        for i, case in enumerate(test_cases):
            input_data = case['input']
            expected_output = case['output'].strip()
            
            try:
                res = subprocess.run(
                    [exe_file],
                    input=input_data,
                    capture_output=True,
                    text=True,
                    timeout=2 # 2ç§’è¶…æ—¶
                )
                actual_output = res.stdout.strip()
                
                if actual_output == expected_output:
                    print(f"   -> Case {i+1}: âœ… é€šè¿‡ (è¾“å…¥: '{input_data.strip()}' | é¢„æœŸ: '{expected_output}' | å®é™…: '{actual_output}')")
                    passed += 1
                else:
                    print(f"   -> Case {i+1}: âŒ å¤±è´¥ (è¾“å…¥: '{input_data.strip()}' | é¢„æœŸ: '{expected_output}' | å®é™…: '{actual_output}')")
            except subprocess.TimeoutExpired:
                print(f"   -> Case {i+1}: âš ï¸ è¿è¡Œè¶…æ—¶ (Timeout)")
            except Exception as e:
                print(f"   -> Case {i+1}: âš ï¸ è¿è¡Œé”™è¯¯ {e}")
        
        return passed / total, "Success"

def main():
    print_step("STEP 1: åŠ è½½æ¨¡å‹ä¸Tokenizer")
    
    # æ£€æŸ¥ g++
    try:
        subprocess.run(['g++', '--version'], capture_output=True)
        print("âœ… æ£€æµ‹åˆ° g++ ç¼–è¯‘å™¨")
    except:
        print("âŒ æœªæ£€æµ‹åˆ° g++ï¼Œè¯·å…ˆå®‰è£… (sudo apt install g++)")
        return

    # åŠ è½½ Tokenizer
    model_path = LOCAL_MODEL_DIR if os.path.exists(LOCAL_MODEL_DIR) else MS_MODEL_ID
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ¨¡å‹ (4-bit)
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path} (4-bit)...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
    )
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    # ------------------------------------------------------------------
    print_step("STEP 2: éªŒè¯ Chat Template (JSON -> Qwen Prompt)")
    
    # æ¨¡æ‹Ÿ train_rl.py ä¸­çš„ apply_chat_template é€»è¾‘
    description = TEST_DATA_JSON.get('description', '')
    public_tests = TEST_DATA_JSON.get('public_tests', {})
    
    # Build public test cases section / æ„å»ºå…¬å¼€æµ‹è¯•ç”¨ä¾‹éƒ¨åˆ†
    public_tests_section = ""
    if isinstance(public_tests, dict) and 'input' in public_tests and 'output' in public_tests:
        inputs = public_tests['input'] if isinstance(public_tests['input'], list) else [public_tests['input']]
        outputs = public_tests['output'] if isinstance(public_tests['output'], list) else [public_tests['output']]
        
        if inputs and outputs:
            public_tests_section = "\nã€Casesã€‘\n"
            for i, (inp, out) in enumerate(zip(inputs, outputs), 1):
                public_tests_section += f"Test {i}:\n"
                public_tests_section += f"Input :\n{inp}\n"
                public_tests_section += f"Output:\n{out}\n"
    
    # Combine into final prompt / ç»„åˆæˆæœ€ç»ˆæç¤º
    raw_prompt = f"""You will be given a programming contest problem. Please reason step by step and provide a complete C++ implementation.
Output the solution in a code block. Do not include debugging info or extra output. Limit reasoning to 128 tokens.


ã€Problem Description ã€‘
{description}

{public_tests_section}

Please provide your C++ solution :"""
    
    messages = [
        {"role": "system", "content": "You are an expert competitive programmer. Output valid C++ code that compiles and solves the problem correctly."},
        {"role": "user", "content": raw_prompt}
    ]
    
    # åº”ç”¨æ¨¡ç‰ˆ
    final_prompt = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    print("--- æœ€ç»ˆè¾“å…¥ç»™æ¨¡å‹çš„ Prompt å¼€å¤´éƒ¨åˆ† ---")
    print(final_prompt[:300] + "...\n")
    print("--- æœ€ç»ˆè¾“å…¥ç»™æ¨¡å‹çš„ Prompt ç»“å°¾éƒ¨åˆ† ---")
    print("..." + final_prompt[-100:])
    
    # æ£€æŸ¥å…³é”®æ ‡ç­¾
    if "<|im_start|>system" in final_prompt and "<|im_start|>assistant" in final_prompt:
        print("\nâœ… æ¨¡ç‰ˆæ ¼å¼æ£€æŸ¥é€šè¿‡ (æ£€æµ‹åˆ° Qwen ChatML æ ‡ç­¾)")
    else:
        print("\nâŒ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ° ChatML æ ‡ç­¾ï¼Œè¯·æ£€æŸ¥ tokenizer_config.json")

    # ------------------------------------------------------------------
    print_step("STEP 3: æ‰§è¡Œæ¨¡å‹ç”Ÿæˆ")
    
    inputs = tokenizer([final_prompt], return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    print(f"Prompt token é•¿åº¦: {inputs.input_ids.shape[1]}")
    print("æ­£åœ¨ç”Ÿæˆ (Max 1024 tokens)...")
    
    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=1024,
            do_sample=True,     
            temperature=0.6,
            top_p=0.9,
            pad_token_id=tokenizer.pad_token_id
        )
    
    # è§£ç 
    full_response = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0]
    
    # åªè¦ç”Ÿæˆéƒ¨åˆ†
    if "<|im_start|>assistant" in full_response:
        response_only = full_response.split("<|im_start|>assistant")[-1]
    else:
        response_only = full_response
    
    print("\n--- æ¨¡å‹ç”Ÿæˆçš„ä»£ç éƒ¨åˆ† (å‰1000å­—ç¬¦) ---")
    print(response_only[:1000] + "..." if len(response_only)>500 else response_only)

    # ------------------------------------------------------------------
    print_step("STEP 4: éªŒè¯ä»£ç æå–ä¸è¯„æµ‹ (åŸºäº CodeContests æ ¼å¼)")
    
    extracted_code, method = extract_code(response_only)
    
    # Parse test cases from CodeContests format / ä» CodeContests æ ¼å¼è§£ææµ‹è¯•ç”¨ä¾‹
    test_cases = []
    for test_type in ['public_tests', 'private_tests', 'generated_tests']:
        test_data = TEST_DATA_JSON.get(test_type, {})
        if isinstance(test_data, dict) and 'input' in test_data and 'output' in test_data:
            inputs = test_data['input'] if isinstance(test_data['input'], list) else [test_data['input']]
            outputs = test_data['output'] if isinstance(test_data['output'], list) else [test_data['output']]
            for inp, out in zip(inputs, outputs):
                test_cases.append({'input': inp, 'output': out})
    
    if extracted_code:
        print(f"âœ… æˆåŠŸæå–ä»£ç  (æ–¹å¼: {method})")
        print(f"æ­£åœ¨ä½¿ç”¨ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹è¿›è¡Œè¯„æµ‹...")
        
        # å®é™…è¿è¡Œè¯„æµ‹
        score, msg = compile_and_run(extracted_code, test_cases)
        
        print(f"\nğŸ“Š æœ€ç»ˆå¾—åˆ† (Reward): {score}")
        
        if score == 1.0:
            print("ğŸ‰ ç»“è®ºï¼šPipeline å®Œç¾é€šè¿‡ï¼æ¨¡å‹æˆåŠŸè§£å‡ºäº†é¢˜ç›®ã€‚")
        elif score > 0.0:
            print("âš ï¸ ç»“è®ºï¼šPipeline é€šç•…ï¼Œä»£ç å¯è¿è¡Œï¼Œä½†éƒ¨åˆ†ç”¨ä¾‹æœªé€šè¿‡ (è¿™æ˜¯ RL è®­ç»ƒéœ€è¦è§£å†³çš„é—®é¢˜)ã€‚")
        else:
            print(f"âš ï¸ ç»“è®ºï¼šä»£ç ç¼–è¯‘å¤±è´¥æˆ–è¿è¡Œå…¨é”™ã€‚è¯¦ç»†ä¿¡æ¯: {msg}")
            print("æ³¨æ„ï¼šå¯¹äºæœªå¾®è°ƒçš„ 3B æ¨¡å‹ï¼Œç¬¬ä¸€æ¬¡åšå¯¹ç«èµ›é¢˜ç›®å¯èƒ½æœ‰æŒ‘æˆ˜ã€‚åªè¦ç¼–è¯‘è¿‡ç¨‹æ²¡æŠ¥é”™ï¼ŒPipeline å°±æ˜¯å¥½çš„ã€‚")
    else:
        print("âŒ ä»£ç æå–å¤±è´¥ï¼æ¨¡å‹å¯èƒ½æ²¡ç”Ÿæˆä»£ç å—ã€‚")

if __name__ == "__main__":
    main()