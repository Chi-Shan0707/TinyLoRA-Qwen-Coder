"""
validate.py - Validation Script for TinyLoRA Model
éªŒè¯è„šæœ¬ - ç”¨äº TinyLoRA æ¨¡å‹éªŒè¯

This script provides validation functionality during training.
æœ¬è„šæœ¬åœ¨è®­ç»ƒæœŸé—´æä¾›éªŒè¯åŠŸèƒ½ã€‚
"""

import torch
import re
from tqdm import tqdm
from utils import compile_and_run, convert_hf_tests_to_list, extract_code_from_response, apply_chat_template


def run_validation(model, tokenizer, dataset, num_samples=10, max_length=1024, temperature=0.7):
    """
    Run validation on a subset of the dataset
    åœ¨æ•°æ®é›†çš„å­é›†ä¸Šè¿è¡ŒéªŒè¯
    
    Args:
        model: The model to validate / è¦éªŒè¯çš„æ¨¡å‹
        tokenizer: Tokenizer for the model / æ¨¡å‹çš„åˆ†è¯å™¨
        dataset: Validation dataset / éªŒè¯æ•°æ®é›†
        num_samples: Number of samples to validate / è¦éªŒè¯çš„æ ·æœ¬æ•°
        max_length: Maximum generation length / æœ€å¤§ç”Ÿæˆé•¿åº¦
        temperature: Sampling temperature / é‡‡æ ·æ¸©åº¦
    
    Returns:
        dict: Validation metrics including Pass@1 / éªŒè¯æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ Pass@1
    """
    print(f"\n{'='*60}")
    print(f"ğŸ” Starting validation... / å¼€å§‹éªŒè¯...")
    print(f"ğŸ“Š Samples to validate / éªŒè¯æ ·æœ¬æ•°: {num_samples}")
    print(f"{'='*60}\n")
    
    # Ensure we don't exceed dataset size / ç¡®ä¿ä¸è¶…è¿‡æ•°æ®é›†å¤§å°
    num_samples = min(num_samples, len(dataset))
    
    # Set model to eval mode / å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    total_score = 0.0
    compile_success = 0
    partial_pass = 0
    full_pass = 0
    
    with torch.no_grad():
        for i in tqdm(range(num_samples), desc="Validating / éªŒè¯ä¸­"):
            sample = dataset[i]
            
            # Get prompt / è·å–æç¤º
            prompt = sample.get('prompt', '')
            if not prompt:
                # If prompt is not pre-generated, skip / å¦‚æœæç¤ºæœªé¢„ç”Ÿæˆï¼Œè·³è¿‡
                continue
            
            # Prepare test cases / å‡†å¤‡æµ‹è¯•ç”¨ä¾‹
            public_tests = sample.get('public_tests', {})
            private_tests = sample.get('private_tests', {})
            generated_tests = sample.get('generated_tests', {})
            
            all_test_cases = []
            all_test_cases.extend(convert_hf_tests_to_list(public_tests))
            all_test_cases.extend(convert_hf_tests_to_list(private_tests))
            all_test_cases.extend(convert_hf_tests_to_list(generated_tests))
            
            if not all_test_cases:
                continue
            
            # Tokenize and generate / åˆ†è¯å¹¶ç”Ÿæˆ
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
            # Ensure inputs are float32 for 4-bit quantized model / ç¡®ä¿è¾“å…¥ä¸º float32 ä»¥é€‚é… 4-bit é‡åŒ–æ¨¡å‹
            inputs = {k: v.to(torch.float32) if v.dtype != torch.int64 else v for k, v in inputs.items()}
            
            try:
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_length,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                )
                
                # Decode response / è§£ç å“åº”
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # Remove prompt from response / ä»å“åº”ä¸­ç§»é™¤æç¤º
                if prompt in response:
                    response = response[len(prompt):].strip()
                
                # Extract code / æå–ä»£ç 
                code = extract_code_from_response(response)
                
                if not code:
                    # No valid code found / æœªæ‰¾åˆ°æœ‰æ•ˆä»£ç 
                    continue
                
                # Compile and run / ç¼–è¯‘å¹¶è¿è¡Œ
                reward = compile_and_run(code, all_test_cases)
                
                total_score += reward
                
                # Track statistics / è·Ÿè¸ªç»Ÿè®¡ä¿¡æ¯
                if reward >= 0.5:
                    compile_success += 1
                if reward == 0.5:
                    partial_pass += 1
                elif reward == 1.0:
                    full_pass += 1
                    
            except Exception as e:
                print(f"âš ï¸ Error during generation / ç”Ÿæˆæ—¶å‡ºé”™: {e}")
                continue
    
    # Calculate metrics / è®¡ç®—æŒ‡æ ‡
    avg_score = total_score / num_samples if num_samples > 0 else 0.0
    compile_rate = compile_success / num_samples if num_samples > 0 else 0.0
    pass_at_1 = full_pass / num_samples if num_samples > 0 else 0.0
    
    results = {
        'avg_score': avg_score,
        'compile_rate': compile_rate,
        'pass_at_1': pass_at_1,
        'compile_success': compile_success,
        'partial_pass': partial_pass,
        'full_pass': full_pass,
        'total_samples': num_samples,
    }
    
    print(f"\n{'='*60}")
    print(f"âœ… Validation complete / éªŒè¯å®Œæˆ")
    print(f"{'='*60}")
    print(f"ğŸ“Š Validation Results / éªŒè¯ç»“æœ:")
    print(f"  â€¢ Average Score / å¹³å‡åˆ†æ•°: {avg_score:.4f}")
    print(f"  â€¢ Compile Rate / ç¼–è¯‘æˆåŠŸç‡: {compile_rate:.2%}")
    print(f"  â€¢ Pass@1 / é€šè¿‡ç‡: {pass_at_1:.2%}")
    print(f"  â€¢ Compile Success / ç¼–è¯‘æˆåŠŸ: {compile_success}/{num_samples}")
    print(f"  â€¢ Partial Pass / éƒ¨åˆ†é€šè¿‡: {partial_pass}/{num_samples}")
    print(f"  â€¢ Full Pass / å®Œå…¨é€šè¿‡: {full_pass}/{num_samples}")
    print(f"{'='*60}\n")
    
    # Set model back to train mode / å°†æ¨¡å‹è®¾å›è®­ç»ƒæ¨¡å¼
    model.train()
    
    return results


if __name__ == "__main__":
    """
    Standalone validation script / ç‹¬ç«‹éªŒè¯è„šæœ¬
    Usage / ç”¨æ³•: python validate.py
    """
    import sys
    from datasets import load_dataset
    from utils import get_model_and_tokenizer, TinyLoRAGlobalParams, apply_tiny_lora
    
    # Configuration / é…ç½®
    MODEL_PATH = "./models/Qwen2.5-Coder-3B-Instruct"
    CHECKPOINT_PATH = "./output/luoguqwencoder-lora/tiny_lora_v.pt"
    VAL_DATA_PATH = "./local_code_contests/code_contests_valid.jsonl"
    NUM_SAMPLES = int(sys.argv[1]) if len(sys.argv) > 1 else 10
    
    print(f"\nğŸš€ Standalone Validation Mode / ç‹¬ç«‹éªŒè¯æ¨¡å¼")
    print(f"ğŸ“¦ Model Path / æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    print(f"ğŸ’¾ Checkpoint Path / æ£€æŸ¥ç‚¹è·¯å¾„: {CHECKPOINT_PATH}")
    print(f"ğŸ“ Validation Data / éªŒè¯æ•°æ®: {VAL_DATA_PATH}")
    print(f"ğŸ”¢ Validation Samples / éªŒè¯æ ·æœ¬æ•°: {NUM_SAMPLES}\n")
    
    # Load checkpoint / åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(CHECKPOINT_PATH, map_location='cpu')
    u_value = checkpoint['u_value']
    seed = checkpoint['seed']
    
    # Set random seed / è®¾ç½®éšæœºç§å­
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    print(f"âœ… Random seed set / å·²è®¾ç½®éšæœºç§å­: {seed}")
    
    # Load model and tokenizer / åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = get_model_and_tokenizer(MODEL_PATH)
    
    # Create global params / åˆ›å»ºå…¨å±€å‚æ•°
    device = model.model.layers[0].self_attn.q_proj.weight.device
    global_params = TinyLoRAGlobalParams(u_dim=u_value, device=device, dtype=torch.bfloat16)
    
    # Inject TinyLoRA / æ³¨å…¥ TinyLoRA
    model.tiny_lora_params = global_params
    print(f"ğŸ”§ Injecting TinyLoRA... / æ­£åœ¨æ³¨å…¥ TinyLoRA...")
    total_replaced = apply_tiny_lora(model, global_params)
    print(f"âœ… TinyLoRA injected / TinyLoRA æ³¨å…¥å®Œæˆ: {total_replaced} layers / å±‚")
    
    # Load trained weights / åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    with torch.no_grad():
        global_params.global_v.copy_(checkpoint['global_v'].to(global_params.global_v.dtype).to(device))
    print(f"âœ… Loaded trained weights / å·²åŠ è½½è®­ç»ƒæƒé‡: global_v shape={global_params.global_v.shape}")
    
    # Load validation dataset / åŠ è½½éªŒè¯æ•°æ®é›†
    val_dataset = load_dataset("json", data_files=VAL_DATA_PATH, split="train")
    
    # Apply chat template if needed / å¦‚æœéœ€è¦ï¼Œåº”ç”¨èŠå¤©æ¨¡æ¿
    if 'prompt' not in val_dataset.column_names:
        print(f"ğŸ”„ Applying chat template... / æ­£åœ¨åº”ç”¨èŠå¤©æ¨¡æ¿...")
        val_dataset = val_dataset.map(lambda x: apply_chat_template(x, tokenizer))
    
    # Run validation / è¿è¡ŒéªŒè¯
    results = run_validation(model, tokenizer, val_dataset, num_samples=NUM_SAMPLES)
    
    print(f"\nâœ… Validation finished! / éªŒè¯å®Œæˆï¼")
    print(f"ğŸ“Š Final Pass@1 / æœ€ç»ˆé€šè¿‡ç‡: {results['pass_at_1']:.2%}")
