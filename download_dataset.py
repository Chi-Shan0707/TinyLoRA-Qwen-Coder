import json
import os
from datasets import load_dataset
from tqdm import tqdm

# 1. å®šä¹‰ä½ éœ€è¦ä¿ç•™çš„å­—æ®µ
KEEP_COLUMNS = [
    "description", 
    "public_tests", 
    "private_tests", 
    "generated_tests", 
    "source", 
    "difficulty"
]

# 2. å®šä¹‰è¦ä¸‹è½½çš„åˆ†åŒº
# SPLITS = ["train", "valid", "test"]
# SPLITS = ["valid","test"]
SPLITS = ["train"]  # å…ˆæµ‹è¯•ä¸€ä¸ªåˆ†åŒºï¼Œç¡®è®¤æ— è¯¯åå†å…¨éƒ¨ä¸‹è½½

# å®šä¹‰è¾“å‡ºç›®å½•
OUTPUT_DIR = "./local_code_contests"
os.makedirs(OUTPUT_DIR, exist_ok=True)

def process_and_save_split(split_name):
    print(f"ğŸš€ æ­£åœ¨å¤„ç†åˆ†åŒº: {split_name} ...")
    
    # å…³é”®ï¼šstreaming=True
    # è¿™æ„å‘³ç€æˆ‘ä»¬ä¸ä¼šä¸‹è½½å‡ ç™¾ GB çš„åŸå§‹æ–‡ä»¶ï¼Œè€Œæ˜¯åƒçœ‹è§†é¢‘ä¸€æ ·è¾¹ä¸‹è¾¹å¤„ç†
    dataset = load_dataset("deepmind/code_contests", split=split_name, streaming=True)
    
    output_file = os.path.join(OUTPUT_DIR, f"code_contests_{split_name}.jsonl")
    
    count = 0
    with open(output_file, "w", encoding="utf-8") as f:
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦ï¼ˆç”±äºæµå¼åŠ è½½ä¸çŸ¥é“æ€»é•¿åº¦ï¼Œåªæ˜¾ç¤ºå¤„ç†æ¡æ•°ï¼‰
        for sample in tqdm(dataset, desc=f"Saving {split_name}"):
            # è¿‡æ»¤ï¼šåªæ„å»ºåŒ…å«ç›®æ ‡å­—æ®µçš„å­—å…¸
            filtered_sample = {key: sample[key] for key in KEEP_COLUMNS}
            
            # å†™å…¥ JSONL
            # ensure_ascii=False ä¿è¯ä¸­æ–‡æè¿°ï¼ˆå¦‚æœæœ‰ï¼‰èƒ½æ­£å¸¸æ˜¾ç¤º
            f.write(json.dumps(filtered_sample, ensure_ascii=False) + "\n")
            count += 1
            
    print(f"âœ… åˆ†åŒº {split_name} å®Œæˆï¼å…±ä¿å­˜ {count} æ¡æ•°æ®åˆ° {output_file}\n")

if __name__ == "__main__":
    print("å¼€å§‹è½»é‡åŒ–ä¸‹è½½ deepmind/code_contests æ•°æ®é›†...")
    print(f"ä¿ç•™å­—æ®µ: {KEEP_COLUMNS}")
    
    for split in SPLITS:
        process_and_save_split(split)
        
    print("ğŸ‰ æ‰€æœ‰æ•°æ®ä¸‹è½½å¹¶æ¸…æ´—å®Œæˆï¼")
    print(f"æ•°æ®ä¿å­˜åœ¨: {OUTPUT_DIR}")